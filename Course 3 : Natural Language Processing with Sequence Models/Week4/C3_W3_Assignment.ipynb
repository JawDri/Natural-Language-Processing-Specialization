
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{mathrsfs}
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \title{C3\_W3\_Assignment}
    
    
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \hypertarget{assignment-3---named-entity-recognition-ner}{%
\section{Assignment 3 - Named Entity Recognition
(NER)}\label{assignment-3---named-entity-recognition-ner}}

Welcome to the third programming assignment of Course 3. In this
assignment, you will learn to build more complicated models with Trax.
By completing this assignment, you will be able to:

\begin{itemize}
\tightlist
\item
  Design the architecture of a neural network, train it, and test it.
\item
  Process features and represents them
\item
  Understand word padding
\item
  Implement LSTMs
\item
  Test with your own sentence
\end{itemize}

\hypertarget{outline}{%
\subsection{Outline}\label{outline}}

\begin{itemize}
\tightlist
\item
  Section \ref{0}
\item
  Section \ref{1}

  \begin{itemize}
  \tightlist
  \item
    Section \ref{11}
  \item
    Section \ref{12}

    \begin{itemize}
    \tightlist
    \item
      Section \ref{ex01}
    \end{itemize}
  \end{itemize}
\item
  Section \ref{2}

  \begin{itemize}
  \tightlist
  \item
    Section \ref{ex02}
  \end{itemize}
\item
  Section \ref{3}

  \begin{itemize}
  \tightlist
  \item
    Section \ref{ex03}
  \end{itemize}
\item
  Section \ref{4}

  \begin{itemize}
  \tightlist
  \item
    Section \ref{ex04}
  \end{itemize}
\item
  Section \ref{5}
\end{itemize}

    \# Introduction

We first start by defining named entity recognition (NER). NER is a
subtask of information extraction that locates and classifies named
entities in a text. The named entities could be organizations, persons,
locations, times, etc.

For example:

Is labeled as follows:

\begin{itemize}
\tightlist
\item
  French: geopolitical entity
\item
  Morocco: geographic entity
\item
  Christmas: time indicator
\end{itemize}

Everything else that is labeled with an \texttt{O} is not considered to
be a named entity. In this assignment, you will train a named entity
recognition system that could be trained in a few seconds (on a GPU) and
will get around 75\% accuracy. Then, you will load in the exact version
of your model, which was trained for a longer period of time. You could
then evaluate the trained version of your model to get 96\% accuracy!
Finally, you will be able to test your named entity recognition system
with your own sentence.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{c+ch}{\PYZsh{}!pip \PYZhy{}q install trax==1.3.1}
        
        \PY{k+kn}{import} \PY{n+nn}{trax} 
        \PY{k+kn}{from} \PY{n+nn}{trax} \PY{k}{import} \PY{n}{layers} \PY{k}{as} \PY{n}{tl}
        \PY{k+kn}{import} \PY{n+nn}{os} 
        \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
        \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
        
        
        \PY{k+kn}{from} \PY{n+nn}{utils} \PY{k}{import} \PY{n}{get\PYZus{}params}\PY{p}{,} \PY{n}{get\PYZus{}vocab}
        \PY{k+kn}{import} \PY{n+nn}{random} \PY{k}{as} \PY{n+nn}{rnd}
        
        \PY{c+c1}{\PYZsh{} set random seeds to make this notebook easier to replicate}
        \PY{n}{trax}\PY{o}{.}\PY{n}{supervised}\PY{o}{.}\PY{n}{trainer\PYZus{}lib}\PY{o}{.}\PY{n}{init\PYZus{}random\PYZus{}number\PYZus{}generators}\PY{p}{(}\PY{l+m+mi}{33}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
INFO:tensorflow:tokens\_length=568 inputs\_length=512 targets\_length=114 noise\_density=0.15 mean\_noise\_span\_length=3.0 

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}1}]:} DeviceArray([ 0, 33], dtype=uint32)
\end{Verbatim}
            
    \# Part 1: Exploring the data

We will be using a dataset from Kaggle, which we will preprocess for
you. The original data consists of four columns, the sentence number,
the word, the part of speech of the word, and the tags. A few tags you
might expect to see are:

\begin{itemize}
\tightlist
\item
  geo: geographical entity
\item
  org: organization
\item
  per: person
\item
  gpe: geopolitical entity
\item
  tim: time indicator
\item
  art: artifact
\item
  eve: event
\item
  nat: natural phenomenon
\item
  O: filler word
\end{itemize}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{c+c1}{\PYZsh{} display original kaggle data}
        \PY{n}{data} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ner\PYZus{}dataset.csv}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{encoding} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ISO\PYZhy{}8859\PYZhy{}1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)} 
        \PY{n}{train\PYZus{}sents} \PY{o}{=} \PY{n+nb}{open}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{data/small/train/sentences.txt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{r}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{readline}\PY{p}{(}\PY{p}{)}
        \PY{n}{train\PYZus{}labels} \PY{o}{=} \PY{n+nb}{open}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{data/small/train/labels.txt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{r}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{readline}\PY{p}{(}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SENTENCE:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{train\PYZus{}sents}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SENTENCE LABEL:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{train\PYZus{}labels}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ORIGINAL DATA:}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{data}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{)}
        \PY{k}{del}\PY{p}{(}\PY{n}{data}\PY{p}{,} \PY{n}{train\PYZus{}sents}\PY{p}{,} \PY{n}{train\PYZus{}labels}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
SENTENCE: Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .

SENTENCE LABEL: O O O O O O B-geo O O O O O B-geo O O O O O B-gpe O O O O O

ORIGINAL DATA:
     Sentence \#           Word  POS Tag
0  Sentence: 1      Thousands  NNS   O
1          NaN             of   IN   O
2          NaN  demonstrators  NNS   O
3          NaN           have  VBP   O
4          NaN        marched  VBN   O

    \end{Verbatim}

    \#\# 1.1 Importing the Data

In this part, we will import the preprocessed data and explore it.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{n}{vocab}\PY{p}{,} \PY{n}{tag\PYZus{}map} \PY{o}{=} \PY{n}{get\PYZus{}vocab}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{data/large/words.txt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{data/large/tags.txt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{t\PYZus{}sentences}\PY{p}{,} \PY{n}{t\PYZus{}labels}\PY{p}{,} \PY{n}{t\PYZus{}size} \PY{o}{=} \PY{n}{get\PYZus{}params}\PY{p}{(}\PY{n}{vocab}\PY{p}{,} \PY{n}{tag\PYZus{}map}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{data/large/train/sentences.txt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{data/large/train/labels.txt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{v\PYZus{}sentences}\PY{p}{,} \PY{n}{v\PYZus{}labels}\PY{p}{,} \PY{n}{v\PYZus{}size} \PY{o}{=} \PY{n}{get\PYZus{}params}\PY{p}{(}\PY{n}{vocab}\PY{p}{,} \PY{n}{tag\PYZus{}map}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{data/large/val/sentences.txt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{data/large/val/labels.txt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{test\PYZus{}sentences}\PY{p}{,} \PY{n}{test\PYZus{}labels}\PY{p}{,} \PY{n}{test\PYZus{}size} \PY{o}{=} \PY{n}{get\PYZus{}params}\PY{p}{(}\PY{n}{vocab}\PY{p}{,} \PY{n}{tag\PYZus{}map}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{data/large/test/sentences.txt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{data/large/test/labels.txt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

    \texttt{vocab} is a dictionary that translates a word string to a unique
number. Given a sentence, you can represent it as an array of numbers
translating with this dictionary. The dictionary contains a
\texttt{\textless{}PAD\textgreater{}} token.

When training an LSTM using batches, all your input sentences must be
the same size. To accomplish this, you set the length of your sentences
to a certain number and add the generic
\texttt{\textless{}PAD\textgreater{}} token to fill all the empty
spaces.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{c+c1}{\PYZsh{} vocab translates from a word to a unique number}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{vocab[}\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{the}\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{]:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{vocab}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{the}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} Pad token}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{padded token:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{vocab}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZlt{}PAD\PYZgt{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
vocab["the"]: 9
padded token: 35180

    \end{Verbatim}

    The tag\_map corresponds to one of the possible tags a word can have.
Run the cell below to see the possible classes you will be predicting.
The prepositions in the tags mean: * I: Token is inside an entity. * B:
Token begins an entity.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{n+nb}{print}\PY{p}{(}\PY{n}{tag\PYZus{}map}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
\{'O': 0, 'B-geo': 1, 'B-gpe': 2, 'B-per': 3, 'I-geo': 4, 'B-org': 5, 'I-org': 6, 'B-tim': 7, 'B-art': 8, 'I-art': 9, 'I-per': 10, 'I-gpe': 11, 'I-tim': 12, 'B-nat': 13, 'B-eve': 14, 'I-eve': 15, 'I-nat': 16\}

    \end{Verbatim}

    So the coding scheme that tags the entities is a minimal one where B-
indicates the first token in a multi-token entity, and I- indicates one
in the middle of a multi-token entity. If you had the sentence

\textbf{``Sharon flew to Miami on Friday''}

the outputs would look like:

\begin{verbatim}
Sharon B-per
flew   O
to     O
Miami  B-geo
on     O
Friday B-tim
\end{verbatim}

your tags would reflect three tokens beginning with B-, since there are
no multi-token entities in the sequence. But if you added Sharon's last
name to the sentence:

\textbf{``Sharon Floyd flew to Miami on Friday''}

\begin{verbatim}
Sharon B-per
Floyd  I-per
flew   O
to     O
Miami  B-geo
on     O
Friday B-tim
\end{verbatim}

then your tags would change to show first ``Sharon'' as B-per, and
``Floyd'' as I-per, where I- indicates an inner token in a multi-token
sequence.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{c+c1}{\PYZsh{} Exploring information about the data}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{The number of outputs is tag\PYZus{}map}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{tag\PYZus{}map}\PY{p}{)}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} The number of vocabulary tokens (including \PYZlt{}PAD\PYZgt{})}
        \PY{n}{g\PYZus{}vocab\PYZus{}size} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{vocab}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Num of vocabulary words: }\PY{l+s+si}{\PYZob{}g\PYZus{}vocab\PYZus{}size\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{The vocab size is}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{vocab}\PY{p}{)}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{The training size is}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{t\PYZus{}size}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{The validation size is}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{v\PYZus{}size}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{An example of the first sentence is}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{t\PYZus{}sentences}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{An example of its corresponding label is}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{t\PYZus{}labels}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
The number of outputs is tag\_map 17
Num of vocabulary words: 35181
The vocab size is 35181
The training size is 33570
The validation size is 7194
An example of the first sentence is [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 9, 15, 1, 16, 17, 18, 19, 20, 21]
An example of its corresponding label is [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0]

    \end{Verbatim}

    So you can see that we have already encoded each sentence into a tensor
by converting it into a number. We also have 16 possible classes, as
shown in the tag map.

\#\# 1.2 Data generator

In python, a generator is a function that behaves like an iterator. It
will return the next item. Here is a
\href{https://wiki.python.org/moin/Generators}{link} to review python
generators.

In many AI applications it is very useful to have a data generator. You
will now implement a data generator for our NER application.

\#\#\# Exercise 01

\textbf{Instructions:} Implement a data generator function that takes in
\texttt{batch\_size,\ x,\ y,\ pad,\ shuffle} where x is a large list of
sentences, and y is a list of the tags associated with those sentences
and pad is a pad value. Return a subset of those inputs in a tuple of
two arrays \texttt{(X,Y)}. Each is an array of dimension
(\texttt{batch\_size,\ max\_len}), where \texttt{max\_len} is the length
of the longest sentence \emph{in that batch}. You will pad the X and Y
examples with the pad argument. If \texttt{shuffle=True}, the data will
be traversed in a random form.

\textbf{Details:}

This code as an outer loop

\begin{verbatim}
while True:  
...  
yield((X,Y))  
\end{verbatim}

Which runs continuously in the fashion of generators, pausing when
yielding the next values. We will generate a batch\_size output on each
pass of this loop.

It has two inner loops. 1. The first stores in temporal lists the data
samples to be included in the next batch, and finds the maximum length
of the sentences contained in it. By adjusting the length to include
only the size of the longest sentence in each batch, overall computation
is reduced.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  The second loop moves those inputs from the temporal list into NumPy
  arrays pre-filled with pad values.
\end{enumerate}

There are three slightly out of the ordinary features. 1. The first is
the use of the NumPy \texttt{full} function to fill the NumPy arrays
with a pad value. See
\href{https://numpy.org/doc/1.18/reference/generated/numpy.full.html}{full
function documentation}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  The second is tracking the current location in the incoming lists of
  sentences. Generators variables hold their values between invocations,
  so we create an \texttt{index} variable, initialize to zero, and
  increment by one for each sample included in a batch. However, we do
  not use the \texttt{index} to access the positions of the list of
  sentences directly. Instead, we use it to select one index from a list
  of indexes. In this way, we can change the order in which we traverse
  our original list, keeping untouched our original list.
\item
  The third also relates to wrapping. Because \texttt{batch\_size} and
  the length of the input lists are not aligned, gathering a batch\_size
  group of inputs may involve wrapping back to the beginning of the
  input loop. In our approach, it is just enough to reset the
  \texttt{index} to 0. We can re-shuffle the list of indexes to produce
  different batches each time.
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{c+c1}{\PYZsh{} UNQ\PYZus{}C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)}
        \PY{c+c1}{\PYZsh{} GRADED FUNCTION: data\PYZus{}generator}
        \PY{k}{def} \PY{n+nf}{data\PYZus{}generator}\PY{p}{(}\PY{n}{batch\PYZus{}size}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{pad}\PY{p}{,} \PY{n}{shuffle}\PY{o}{=}\PY{k+kc}{False}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}}
        \PY{l+s+sd}{      Input: }
        \PY{l+s+sd}{        batch\PYZus{}size \PYZhy{} integer describing the batch size}
        \PY{l+s+sd}{        x \PYZhy{} list containing sentences where words are represented as integers}
        \PY{l+s+sd}{        y \PYZhy{} list containing tags associated with the sentences}
        \PY{l+s+sd}{        shuffle \PYZhy{} Shuffle the data order}
        \PY{l+s+sd}{        pad \PYZhy{} an integer representing a pad character}
        \PY{l+s+sd}{        verbose \PYZhy{} Print information during runtime}
        \PY{l+s+sd}{      Output:}
        \PY{l+s+sd}{        a tuple containing 2 elements:}
        \PY{l+s+sd}{        X \PYZhy{} np.ndarray of dim (batch\PYZus{}size, max\PYZus{}len) of padded sentences}
        \PY{l+s+sd}{        Y \PYZhy{} np.ndarray of dim (batch\PYZus{}size, max\PYZus{}len) of tags associated with the sentences in X}
        \PY{l+s+sd}{    \PYZsq{}\PYZsq{}\PYZsq{}}
            
            \PY{c+c1}{\PYZsh{} count the number of lines in data\PYZus{}lines}
            \PY{n}{num\PYZus{}lines} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{x}\PY{p}{)}
            
            \PY{c+c1}{\PYZsh{} create an array with the indexes of data\PYZus{}lines that can be shuffled}
            \PY{n}{lines\PYZus{}index} \PY{o}{=} \PY{p}{[}\PY{o}{*}\PY{n+nb}{range}\PY{p}{(}\PY{n}{num\PYZus{}lines}\PY{p}{)}\PY{p}{]}
            
            \PY{c+c1}{\PYZsh{} shuffle the indexes if shuffle is set to True}
            \PY{k}{if} \PY{n}{shuffle}\PY{p}{:}
                \PY{n}{rnd}\PY{o}{.}\PY{n}{shuffle}\PY{p}{(}\PY{n}{lines\PYZus{}index}\PY{p}{)}
            
            \PY{n}{index} \PY{o}{=} \PY{l+m+mi}{0} \PY{c+c1}{\PYZsh{} tracks current location in x, y}
            \PY{k}{while} \PY{k+kc}{True}\PY{p}{:}
                \PY{n}{buffer\PYZus{}x} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{*} \PY{n}{batch\PYZus{}size} \PY{c+c1}{\PYZsh{} Temporal array to store the raw x data for this batch}
                \PY{n}{buffer\PYZus{}y} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{*} \PY{n}{batch\PYZus{}size} \PY{c+c1}{\PYZsh{} Temporal array to store the raw y data for this batch}
                        
          \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} START CODE HERE (Replace instances of \PYZsq{}None\PYZsq{} with your code) \PYZsh{}\PYZsh{}\PYZsh{}}
                
                \PY{c+c1}{\PYZsh{} Copy into the temporal buffers the sentences in x[index : index + batch\PYZus{}size] }
                \PY{c+c1}{\PYZsh{} along with their corresponding labels y[index : index + batch\PYZus{}size]}
                \PY{c+c1}{\PYZsh{} Find maximum length of sentences in x[index : index + batch\PYZus{}size] for this batch. }
                \PY{c+c1}{\PYZsh{} Reset the index if we reach the end of the data set, and shuffle the indexes if needed.}
                \PY{n}{max\PYZus{}len} \PY{o}{=} \PY{l+m+mi}{0}
                \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{batch\PYZus{}size}\PY{p}{)}\PY{p}{:}
                     \PY{c+c1}{\PYZsh{} if the index is greater than or equal to the number of lines in x}
                    \PY{k}{if} \PY{n}{index} \PY{o}{\PYZgt{}}\PY{o}{=} \PY{n}{num\PYZus{}lines}\PY{p}{:}
                        \PY{c+c1}{\PYZsh{} then reset the index to 0}
                        \PY{n}{index} \PY{o}{=} \PY{l+m+mi}{0}
                        \PY{c+c1}{\PYZsh{} re\PYZhy{}shuffle the indexes if shuffle is set to True}
                        \PY{k}{if} \PY{n}{shuffle}\PY{p}{:}
                            \PY{n}{rnd}\PY{o}{.}\PY{n}{shuffle}\PY{p}{(}\PY{n}{lines\PYZus{}index}\PY{p}{)}
                    
                    \PY{c+c1}{\PYZsh{} The current position is obtained using `lines\PYZus{}index[index]`}
                    \PY{c+c1}{\PYZsh{} Store the x value at the current position into the buffer\PYZus{}x}
                    \PY{n}{buffer\PYZus{}x}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{=} \PY{n}{x}\PY{p}{[}\PY{n}{lines\PYZus{}index}\PY{p}{[}\PY{n}{index}\PY{p}{]}\PY{p}{]}
                    
                    \PY{c+c1}{\PYZsh{} Store the y value at the current position into the buffer\PYZus{}y}
                    \PY{n}{buffer\PYZus{}y}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{=} \PY{n}{y}\PY{p}{[}\PY{n}{lines\PYZus{}index}\PY{p}{[}\PY{n}{index}\PY{p}{]}\PY{p}{]}
                    
                    \PY{n}{lenx} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{x}\PY{p}{[}\PY{n}{lines\PYZus{}index}\PY{p}{[}\PY{n}{index}\PY{p}{]}\PY{p}{]}\PY{p}{)}    \PY{c+c1}{\PYZsh{}length of current x[]}
                    \PY{k}{if} \PY{n}{lenx} \PY{o}{\PYZgt{}} \PY{n}{max\PYZus{}len}\PY{p}{:}
                        \PY{n}{max\PYZus{}len} \PY{o}{=} \PY{n}{lenx}                   \PY{c+c1}{\PYZsh{}max\PYZus{}len tracks longest x[]}
                    
                    \PY{c+c1}{\PYZsh{} increment index by one}
                    \PY{n}{index} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
        
        
                \PY{c+c1}{\PYZsh{} create X,Y, NumPy arrays of size (batch\PYZus{}size, max\PYZus{}len) \PYZsq{}full\PYZsq{} of pad value}
                \PY{n}{X} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{full}\PY{p}{(}\PY{p}{(}\PY{n}{batch\PYZus{}size}\PY{p}{,} \PY{n}{max\PYZus{}len}\PY{p}{)}\PY{p}{,} \PY{n}{pad}\PY{p}{)}
                \PY{n}{Y} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{full}\PY{p}{(}\PY{p}{(}\PY{n}{batch\PYZus{}size}\PY{p}{,} \PY{n}{max\PYZus{}len}\PY{p}{)}\PY{p}{,} \PY{n}{pad}\PY{p}{)}
        
                \PY{c+c1}{\PYZsh{} copy values from lists to NumPy arrays. Use the buffered values}
                \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{batch\PYZus{}size}\PY{p}{)}\PY{p}{:}
                    \PY{c+c1}{\PYZsh{} get the example (sentence as a tensor)}
                    \PY{c+c1}{\PYZsh{} in `buffer\PYZus{}x` at the `i` index}
                    \PY{n}{x\PYZus{}i} \PY{o}{=} \PY{n}{buffer\PYZus{}x}\PY{p}{[}\PY{n}{i}\PY{p}{]}
                    
                    \PY{c+c1}{\PYZsh{} similarly, get the example\PYZsq{}s labels}
                    \PY{c+c1}{\PYZsh{} in `buffer\PYZus{}y` at the `i` index}
                    \PY{n}{y\PYZus{}i} \PY{o}{=} \PY{n}{buffer\PYZus{}y}\PY{p}{[}\PY{n}{i}\PY{p}{]}
                    
                    \PY{c+c1}{\PYZsh{} Walk through each word in x\PYZus{}i}
                    \PY{k}{for} \PY{n}{j} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{x\PYZus{}i}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                        \PY{c+c1}{\PYZsh{} store the word in x\PYZus{}i at position j into X}
                        \PY{n}{X}\PY{p}{[}\PY{n}{i}\PY{p}{,} \PY{n}{j}\PY{p}{]} \PY{o}{=} \PY{n}{x\PYZus{}i}\PY{p}{[}\PY{n}{j}\PY{p}{]}
                        
                        \PY{c+c1}{\PYZsh{} store the label in y\PYZus{}i at position j into Y}
                        \PY{n}{Y}\PY{p}{[}\PY{n}{i}\PY{p}{,} \PY{n}{j}\PY{p}{]} \PY{o}{=} \PY{n}{y\PYZus{}i}\PY{p}{[}\PY{n}{j}\PY{p}{]}
        
            \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} END CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}
                \PY{k}{if} \PY{n}{verbose}\PY{p}{:} \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{index=}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{index}\PY{p}{)}
                \PY{k}{yield}\PY{p}{(}\PY{p}{(}\PY{n}{X}\PY{p}{,}\PY{n}{Y}\PY{p}{)}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} \PY{n}{batch\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{5}
        \PY{n}{mini\PYZus{}sentences} \PY{o}{=} \PY{n}{t\PYZus{}sentences}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{:} \PY{l+m+mi}{8}\PY{p}{]}
        \PY{n}{mini\PYZus{}labels} \PY{o}{=} \PY{n}{t\PYZus{}labels}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{:} \PY{l+m+mi}{8}\PY{p}{]}
        \PY{n}{dg} \PY{o}{=} \PY{n}{data\PYZus{}generator}\PY{p}{(}\PY{n}{batch\PYZus{}size}\PY{p}{,} \PY{n}{mini\PYZus{}sentences}\PY{p}{,} \PY{n}{mini\PYZus{}labels}\PY{p}{,} \PY{n}{vocab}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZlt{}PAD\PYZgt{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{,} \PY{n}{shuffle}\PY{o}{=}\PY{k+kc}{False}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
        \PY{n}{X1}\PY{p}{,} \PY{n}{Y1} \PY{o}{=} \PY{n+nb}{next}\PY{p}{(}\PY{n}{dg}\PY{p}{)}
        \PY{n}{X2}\PY{p}{,} \PY{n}{Y2} \PY{o}{=} \PY{n+nb}{next}\PY{p}{(}\PY{n}{dg}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{Y1}\PY{o}{.}\PY{n}{shape}\PY{p}{,} \PY{n}{X1}\PY{o}{.}\PY{n}{shape}\PY{p}{,} \PY{n}{Y2}\PY{o}{.}\PY{n}{shape}\PY{p}{,} \PY{n}{X2}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{X1}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{p}{:}\PY{p}{]}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{Y1}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{p}{:}\PY{p}{]}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
index= 5
index= 2
(5, 30) (5, 30) (5, 30) (5, 30)
[    0     1     2     3     4     5     6     7     8     9    10    11
    12    13    14     9    15     1    16    17    18    19    20    21
 35180 35180 35180 35180 35180 35180] 
 [    0     0     0     0     0     0     1     0     0     0     0     0
     1     0     0     0     0     0     2     0     0     0     0     0
 35180 35180 35180 35180 35180 35180]

    \end{Verbatim}

    \textbf{Expected output:}

\begin{verbatim}
index= 5
index= 2
(5, 30) (5, 30) (5, 30) (5, 30)
[    0     1     2     3     4     5     6     7     8     9    10    11
    12    13    14     9    15     1    16    17    18    19    20    21
 35180 35180 35180 35180 35180 35180] 
 [    0     0     0     0     0     0     1     0     0     0     0     0
     1     0     0     0     0     0     2     0     0     0     0     0
 35180 35180 35180 35180 35180 35180]  
\end{verbatim}

    \# Part 2: Building the model

You will now implement the model. You will be using Google's TensorFlow.
Your model will be able to distinguish the following:

The model architecture will be as follows:

Concretely:

\begin{itemize}
\tightlist
\item
  Use the input tensors you built in your data generator
\item
  Feed it into an Embedding layer, to produce more semantic entries
\item
  Feed it into an LSTM layer
\item
  Run the output through a linear layer
\item
  Run the result through a log softmax layer to get the predicted class
  for each word.
\end{itemize}

Good news! We won't make you implement the LSTM unit drawn above.
However, we will ask you to build the model.

\#\#\# Exercise 02

\textbf{Instructions:} Implement the initialization step and the forward
function of your Named Entity Recognition system.\\
Please utilize help function e.g.~\texttt{help(tl.Dense)} for more
information on a layer

\begin{itemize}
\tightlist
\item
  \href{https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/combinators.py\#L26}{tl.Serial}:
  Combinator that applies layers serially (by function composition).

  \begin{itemize}
  \tightlist
  \item
    You can pass in the layers as arguments to \texttt{Serial},
    separated by commas.
  \item
    For example:
    \texttt{tl.Serial(tl.Embeddings(...),\ tl.Mean(...),\ tl.Dense(...),\ tl.LogSoftmax(...))}
  \end{itemize}
\item
  \href{https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/core.py\#L113}{tl.Embedding}:
  Initializes the embedding. In this case it is the dimension of the
  model by the size of the vocabulary.

  \begin{itemize}
  \tightlist
  \item
    \texttt{tl.Embedding(vocab\_size,\ d\_feature)}.
  \item
    \texttt{vocab\_size} is the number of unique words in the given
    vocabulary.
  \item
    \texttt{d\_feature} is the number of elements in the word embedding
    (some choices for a word embedding size range from 150 to 300, for
    example).
  \end{itemize}
\item
  \href{https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/rnn.py\#L87}{tl.LSTM}:\texttt{Trax}
  LSTM layer of size d\_model.

  \begin{itemize}
  \tightlist
  \item
    \texttt{LSTM(n\_units)} Builds an LSTM layer of n\_cells.
  \end{itemize}
\item
  \href{https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/core.py\#L28}{tl.Dense}:
  A dense layer.

  \begin{itemize}
  \tightlist
  \item
    \texttt{tl.Dense(n\_units)}: The parameter \texttt{n\_units} is the
    number of units chosen for this dense layer.
  \end{itemize}
\item
  \href{https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/core.py\#L242}{tl.LogSoftmax}:
  Log of the output probabilities.

  \begin{itemize}
  \tightlist
  \item
    Here, you don't need to set any parameters for
    \texttt{LogSoftMax()}.
  \end{itemize}
\end{itemize}

\textbf{Online documentation}

\begin{itemize}
\item
  \href{https://trax-ml.readthedocs.io/en/latest/trax.layers.html\#module-trax.layers.combinators}{tl.Serial}
\item
  \href{https://trax-ml.readthedocs.io/en/latest/trax.layers.html\#trax.layers.core.Embedding}{tl.Embedding}
\item
  \href{https://trax-ml.readthedocs.io/en/latest/trax.layers.html\#trax.layers.rnn.LSTM}{tl.LSTM}
\item
  \href{https://trax-ml.readthedocs.io/en/latest/trax.layers.html\#trax.layers.core.Dense}{tl.Dense}
\item
  \href{https://trax-ml.readthedocs.io/en/latest/trax.layers.html\#trax.layers.core.LogSoftmax}{tl.LogSoftmax}
\end{itemize}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}9}]:} \PY{c+c1}{\PYZsh{} UNQ\PYZus{}C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)}
        \PY{c+c1}{\PYZsh{} GRADED FUNCTION: NER}
        \PY{k}{def} \PY{n+nf}{NER}\PY{p}{(}\PY{n}{vocab\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{35181}\PY{p}{,} \PY{n}{d\PYZus{}model}\PY{o}{=}\PY{l+m+mi}{50}\PY{p}{,} \PY{n}{tags}\PY{o}{=}\PY{n}{tag\PYZus{}map}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}}
        \PY{l+s+sd}{      Input: }
        \PY{l+s+sd}{        vocab\PYZus{}size \PYZhy{} integer containing the size of the vocabulary}
        \PY{l+s+sd}{        d\PYZus{}model \PYZhy{} integer describing the embedding size}
        \PY{l+s+sd}{      Output:}
        \PY{l+s+sd}{        model \PYZhy{} a trax serial model}
        \PY{l+s+sd}{    \PYZsq{}\PYZsq{}\PYZsq{}}
            \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} START CODE HERE (Replace instances of \PYZsq{}None\PYZsq{} with your code) \PYZsh{}\PYZsh{}\PYZsh{}}
            \PY{n}{model} \PY{o}{=} \PY{n}{tl}\PY{o}{.}\PY{n}{Serial}\PY{p}{(}
              \PY{n}{tl}\PY{o}{.}\PY{n}{Embedding}\PY{p}{(}\PY{n}{vocab\PYZus{}size}\PY{p}{,} \PY{n}{d\PYZus{}model}\PY{p}{)}\PY{p}{,} \PY{c+c1}{\PYZsh{} Embedding layer}
              \PY{n}{tl}\PY{o}{.}\PY{n}{LSTM}\PY{p}{(}\PY{n}{d\PYZus{}model}\PY{p}{)}\PY{p}{,} \PY{c+c1}{\PYZsh{} LSTM layer}
              \PY{n}{tl}\PY{o}{.}\PY{n}{Dense}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{tags}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{c+c1}{\PYZsh{} Dense layer with len(tags) units}
              \PY{n}{tl}\PY{o}{.}\PY{n}{LogSoftmax}\PY{p}{(}\PY{p}{)}  \PY{c+c1}{\PYZsh{} LogSoftmax layer}
              \PY{p}{)}
              \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} END CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}
            \PY{k}{return} \PY{n}{model}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{c+c1}{\PYZsh{} initializing your model}
         \PY{n}{model} \PY{o}{=} \PY{n}{NER}\PY{p}{(}\PY{p}{)}
         \PY{c+c1}{\PYZsh{} display your model}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{model}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Serial[
  Embedding\_35181\_50
  LSTM\_50
  Dense\_17
  LogSoftmax
]

    \end{Verbatim}

    \textbf{Expected output:}

\begin{verbatim}
Serial[
  Embedding_35181_50
  LSTM_50
  Dense_17
  LogSoftmax
]
\end{verbatim}

    \# Part 3: Train the Model

This section will train your model.

Before you start, you need to create the data generators for training
and validation data. It is important that you mask padding in the loss
weights of your data, which can be done using the \texttt{id\_to\_mask}
argument of \texttt{trax.supervised.inputs.add\_loss\_weights}.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} \PY{k+kn}{from} \PY{n+nn}{trax}\PY{n+nn}{.}\PY{n+nn}{supervised} \PY{k}{import} \PY{n}{training}
         
         \PY{n}{rnd}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{33}\PY{p}{)}
         
         \PY{n}{batch\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{64}
         
         \PY{c+c1}{\PYZsh{} Create training data, mask pad id=35180 for training.}
         \PY{n}{train\PYZus{}generator} \PY{o}{=} \PY{n}{trax}\PY{o}{.}\PY{n}{supervised}\PY{o}{.}\PY{n}{inputs}\PY{o}{.}\PY{n}{add\PYZus{}loss\PYZus{}weights}\PY{p}{(}
             \PY{n}{data\PYZus{}generator}\PY{p}{(}\PY{n}{batch\PYZus{}size}\PY{p}{,} \PY{n}{t\PYZus{}sentences}\PY{p}{,} \PY{n}{t\PYZus{}labels}\PY{p}{,} \PY{n}{vocab}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZlt{}PAD\PYZgt{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{k+kc}{True}\PY{p}{)}\PY{p}{,}
             \PY{n}{id\PYZus{}to\PYZus{}mask}\PY{o}{=}\PY{n}{vocab}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZlt{}PAD\PYZgt{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Create validation data, mask pad id=35180 for training.}
         \PY{n}{eval\PYZus{}generator} \PY{o}{=} \PY{n}{trax}\PY{o}{.}\PY{n}{supervised}\PY{o}{.}\PY{n}{inputs}\PY{o}{.}\PY{n}{add\PYZus{}loss\PYZus{}weights}\PY{p}{(}
             \PY{n}{data\PYZus{}generator}\PY{p}{(}\PY{n}{batch\PYZus{}size}\PY{p}{,} \PY{n}{v\PYZus{}sentences}\PY{p}{,} \PY{n}{v\PYZus{}labels}\PY{p}{,} \PY{n}{vocab}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZlt{}PAD\PYZgt{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{k+kc}{True}\PY{p}{)}\PY{p}{,}
             \PY{n}{id\PYZus{}to\PYZus{}mask}\PY{o}{=}\PY{n}{vocab}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZlt{}PAD\PYZgt{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}

    \#\#\# 3.1 Training the model

You will now write a function that takes in your model and trains it.

As you've seen in the previous assignments, you will first create the
\href{https://trax-ml.readthedocs.io/en/stable/trax.supervised.html\#trax.supervised.training.TrainTask}{TrainTask}
and
\href{https://trax-ml.readthedocs.io/en/stable/trax.supervised.html\#trax.supervised.training.EvalTask}{EvalTask}
using your data generator. Then you will use the \texttt{training.Loop}
to train your model.

\#\#\# Exercise 03

\textbf{Instructions:} Implement the \texttt{train\_model} program below
to train the neural network above. Here is a list of things you should
do: - Create the trainer object by calling
\href{https://trax-ml.readthedocs.io/en/latest/trax.supervised.html\#trax.supervised.training.Loop}{\texttt{trax.supervised.training.Loop}}
and pass in the following:

\begin{verbatim}
- model = [NER](#ex02)
- [training task](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.TrainTask) that uses the train data generator defined in the cell above
    - loss_layer = [tl.CrossEntropyLoss()](https://github.com/google/trax/blob/22765bb18608d376d8cd660f9865760e4ff489cd/trax/layers/metrics.py#L71)
    - optimizer = [trax.optimizers.Adam(0.01)](https://github.com/google/trax/blob/03cb32995e83fc1455b0c8d1c81a14e894d0b7e3/trax/optimizers/adam.py#L23)
- [evaluation task](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.EvalTask) that uses the validation data generator defined in the cell above
    - metrics for `EvalTask`: `tl.CrossEntropyLoss()` and `tl.Accuracy()`
    - in `EvalTask` set `n_eval_batches=10` for better evaluation accuracy
- output_dir = output_dir
\end{verbatim}

You'll be using a
\href{https://trax-ml.readthedocs.io/en/latest/trax.layers.html\#trax.layers.metrics.CrossEntropyLoss}{cross
entropy loss}, with an
\href{https://trax-ml.readthedocs.io/en/latest/trax.optimizers.html\#trax.optimizers.adam.Adam}{Adam
optimizer}. Please read the
\href{https://trax-ml.readthedocs.io/en/latest/trax.html}{trax}
documentation to get a full understanding. The
\href{https://github.com/google/trax}{trax GitHub} also contains some
useful information and a link to a colab notebook.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}12}]:} \PY{c+c1}{\PYZsh{} UNQ\PYZus{}C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)}
         \PY{c+c1}{\PYZsh{} GRADED FUNCTION: train\PYZus{}model}
         \PY{k}{def} \PY{n+nf}{train\PYZus{}model}\PY{p}{(}\PY{n}{NER}\PY{p}{,} \PY{n}{train\PYZus{}generator}\PY{p}{,} \PY{n}{eval\PYZus{}generator}\PY{p}{,} \PY{n}{train\PYZus{}steps}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{output\PYZus{}dir}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{model}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}}
         \PY{l+s+sd}{    Input: }
         \PY{l+s+sd}{        NER \PYZhy{} the model you are building}
         \PY{l+s+sd}{        train\PYZus{}generator \PYZhy{} The data generator for training examples}
         \PY{l+s+sd}{        eval\PYZus{}generator \PYZhy{} The data generator for validation examples,}
         \PY{l+s+sd}{        train\PYZus{}steps \PYZhy{} number of training steps}
         \PY{l+s+sd}{        output\PYZus{}dir \PYZhy{} folder to save your model}
         \PY{l+s+sd}{    Output:}
         \PY{l+s+sd}{        training\PYZus{}loop \PYZhy{} a trax supervised training Loop}
         \PY{l+s+sd}{    \PYZsq{}\PYZsq{}\PYZsq{}}
             \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} START CODE HERE (Replace instances of \PYZsq{}None\PYZsq{} with your code) \PYZsh{}\PYZsh{}\PYZsh{}}
             \PY{n}{train\PYZus{}task} \PY{o}{=} \PY{n}{training}\PY{o}{.}\PY{n}{TrainTask}\PY{p}{(}
               \PY{n}{train\PYZus{}generator}\PY{p}{,} \PY{c+c1}{\PYZsh{} A train data generator}
               \PY{n}{loss\PYZus{}layer} \PY{o}{=} \PY{n}{tl}\PY{o}{.}\PY{n}{CrossEntropyLoss}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{c+c1}{\PYZsh{} A cross\PYZhy{}entropy loss function}
               \PY{n}{optimizer} \PY{o}{=} \PY{n}{trax}\PY{o}{.}\PY{n}{optimizers}\PY{o}{.}\PY{n}{Adam}\PY{p}{(}\PY{l+m+mf}{0.01}\PY{p}{)}\PY{p}{,}  \PY{c+c1}{\PYZsh{} The adam optimizer}
             \PY{p}{)}
         
             \PY{n}{eval\PYZus{}task} \PY{o}{=} \PY{n}{training}\PY{o}{.}\PY{n}{EvalTask}\PY{p}{(}
               \PY{n}{labeled\PYZus{}data} \PY{o}{=} \PY{n}{eval\PYZus{}generator}\PY{p}{,} \PY{c+c1}{\PYZsh{} A labeled data generator}
               \PY{n}{metrics} \PY{o}{=} \PY{p}{[}\PY{n}{tl}\PY{o}{.}\PY{n}{CrossEntropyLoss}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{tl}\PY{o}{.}\PY{n}{Accuracy}\PY{p}{(}\PY{p}{)}\PY{p}{]}\PY{p}{,} \PY{c+c1}{\PYZsh{} Evaluate with cross\PYZhy{}entropy loss and accuracy}
               \PY{n}{n\PYZus{}eval\PYZus{}batches} \PY{o}{=} \PY{l+m+mi}{10}  \PY{c+c1}{\PYZsh{} Number of batches to use on each evaluation}
             \PY{p}{)}
         
             \PY{n}{training\PYZus{}loop} \PY{o}{=} \PY{n}{training}\PY{o}{.}\PY{n}{Loop}\PY{p}{(}
                 \PY{n}{NER}\PY{p}{,} \PY{c+c1}{\PYZsh{} A model to train}
                 \PY{n}{train\PYZus{}task}\PY{p}{,} \PY{c+c1}{\PYZsh{} A train task}
                 \PY{n}{eval\PYZus{}task} \PY{o}{=} \PY{n}{eval\PYZus{}task}\PY{p}{,} \PY{c+c1}{\PYZsh{} The evaluation task}
                 \PY{n}{output\PYZus{}dir} \PY{o}{=} \PY{n}{output\PYZus{}dir}\PY{p}{)} \PY{c+c1}{\PYZsh{} The output directory}
         
             \PY{c+c1}{\PYZsh{} Train with train\PYZus{}steps}
             \PY{n}{training\PYZus{}loop}\PY{o}{.}\PY{n}{run}\PY{p}{(}\PY{n}{n\PYZus{}steps} \PY{o}{=} \PY{n}{train\PYZus{}steps}\PY{p}{)}
             \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} END CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}
             \PY{k}{return} \PY{n}{training\PYZus{}loop}
\end{Verbatim}

    On your local machine, you can run this training for 1000 train\_steps
and get your own model. This training takes about 5 to 10 minutes to
run.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}13}]:} \PY{n}{train\PYZus{}steps} \PY{o}{=} \PY{l+m+mi}{100}            \PY{c+c1}{\PYZsh{} In coursera we can only train 100 steps}
         \PY{o}{!}rm \PYZhy{}f \PY{l+s+s1}{\PYZsq{}model/model.pkl.gz\PYZsq{}}  \PYZsh{} Remove old model.pkl \PY{k}{if} it exists
         
         \PY{c+c1}{\PYZsh{} Train the model}
         \PY{n}{training\PYZus{}loop} \PY{o}{=} \PY{n}{train\PYZus{}model}\PY{p}{(}\PY{n}{NER}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{train\PYZus{}generator}\PY{p}{,} \PY{n}{eval\PYZus{}generator}\PY{p}{,} \PY{n}{train\PYZus{}steps}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Step      1: train CrossEntropyLoss |  3.29933977
Step      1: eval  CrossEntropyLoss |  2.27930465
Step      1: eval          Accuracy |  0.22279498
Step    100: train CrossEntropyLoss |  0.61237383
Step    100: eval  CrossEntropyLoss |  0.37608672
Step    100: eval          Accuracy |  0.90983244

    \end{Verbatim}

    \textbf{Expected output (Approximately)}

\begin{verbatim}
...
Step      1: train CrossEntropyLoss |  2.94375849
Step      1: eval  CrossEntropyLoss |  1.93172036
Step      1: eval          Accuracy |  0.78727312
Step    100: train CrossEntropyLoss |  0.57727730
Step    100: eval  CrossEntropyLoss |  0.36356260
Step    100: eval          Accuracy |  0.90943187
...
\end{verbatim}

This value may change between executions, but it must be around 90\% of
accuracy on train and validations sets, after 100 training steps.

    We have trained the model longer, and we give you such a trained model.
In that way, we ensure you can continue with the rest of the assignment
even if you had some troubles up to here, and also we are sure that
everybody will get the same outputs for the last example. However, you
are free to try your model, as well.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} \PY{c+c1}{\PYZsh{} loading in a pretrained model..}
         \PY{n}{model} \PY{o}{=} \PY{n}{NER}\PY{p}{(}\PY{p}{)}
         \PY{n}{model}\PY{o}{.}\PY{n}{init}\PY{p}{(}\PY{n}{trax}\PY{o}{.}\PY{n}{shapes}\PY{o}{.}\PY{n}{ShapeDtype}\PY{p}{(}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{int32}\PY{p}{)}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Load the pretrained model}
         \PY{n}{model}\PY{o}{.}\PY{n}{init\PYZus{}from\PYZus{}file}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{model.pkl.gz}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{weights\PYZus{}only}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\end{Verbatim}

    \# Part 4: Compute Accuracy

You will now evaluate in the test set. Previously, you have seen the
accuracy on the training set and the validation (noted as eval) set. You
will now evaluate on your test set. To get a good evaluation, you will
need to create a mask to avoid counting the padding tokens when
computing the accuracy.

\#\#\# Exercise 04

\textbf{Instructions:} Write a program that takes in your model and uses
it to evaluate on the test set. You should be able to get an accuracy of
95\%.

    More Detailed Instructions

\begin{itemize}
\item
  \emph{Step 1}: model(sentences) will give you the predicted output.
\item
  \emph{Step 2}: Prediction will produce an output with an added
  dimension. For each sentence, for each word, there will be a vector of
  probabilities for each tag type. For each sentence,word, you need to
  pick the maximum valued tag. This will require \texttt{np.argmax} and
  careful use of the \texttt{axis} argument.
\item
  \emph{Step 3}: Create a mask to prevent counting pad characters. It
  has the same dimension as output. An example below on matrix
  comparison provides a hint.
\item
  \emph{Step 4}: Compute the accuracy metric by comparing your outputs
  against your test labels. Take the sum of that and divide by the total
  number of \textbf{unpadded} tokens. Use your mask value to mask the
  padded tokens. Return the accuracy. 
\end{itemize}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}15}]:} \PY{c+c1}{\PYZsh{}Example of a comparision on a matrix }
         \PY{n}{a} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{]}\PY{p}{)}
         \PY{n}{a} \PY{o}{==} \PY{l+m+mi}{2}
\end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}15}]:} array([False,  True, False, False])
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}16}]:} \PY{c+c1}{\PYZsh{} create the evaluation inputs}
         \PY{n}{x}\PY{p}{,} \PY{n}{y} \PY{o}{=} \PY{n+nb}{next}\PY{p}{(}\PY{n}{data\PYZus{}generator}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{test\PYZus{}sentences}\PY{p}{)}\PY{p}{,} \PY{n}{test\PYZus{}sentences}\PY{p}{,} \PY{n}{test\PYZus{}labels}\PY{p}{,} \PY{n}{vocab}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZlt{}PAD\PYZgt{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{input shapes}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{x}\PY{o}{.}\PY{n}{shape}\PY{p}{,} \PY{n}{y}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
input shapes (7194, 70) (7194, 70)

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}17}]:} \PY{c+c1}{\PYZsh{} sample prediction}
         \PY{n}{tmp\PYZus{}pred} \PY{o}{=} \PY{n}{model}\PY{p}{(}\PY{n}{x}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n+nb}{type}\PY{p}{(}\PY{n}{tmp\PYZus{}pred}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{tmp\PYZus{}pred has shape: }\PY{l+s+si}{\PYZob{}tmp\PYZus{}pred.shape\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
<class 'jax.interpreters.xla.DeviceArray'>
tmp\_pred has shape: (7194, 70, 17)

    \end{Verbatim}

    Note that the model's prediction has 3 axes: - the number of examples -
the number of words in each example (padded to be as long as the longest
sentence in the batch) - the number of possible targets (the 17 named
entity tags).

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}18}]:} \PY{c+c1}{\PYZsh{} UNQ\PYZus{}C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)}
         \PY{c+c1}{\PYZsh{} GRADED FUNCTION: evaluate\PYZus{}prediction}
         \PY{k}{def} \PY{n+nf}{evaluate\PYZus{}prediction}\PY{p}{(}\PY{n}{pred}\PY{p}{,} \PY{n}{labels}\PY{p}{,} \PY{n}{pad}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{    Inputs:}
         \PY{l+s+sd}{        pred: prediction array with shape }
         \PY{l+s+sd}{            (num examples, max sentence length in batch, num of classes)}
         \PY{l+s+sd}{        labels: array of size (batch\PYZus{}size, seq\PYZus{}len)}
         \PY{l+s+sd}{        pad: integer representing pad character}
         \PY{l+s+sd}{    Outputs:}
         \PY{l+s+sd}{        accuracy: float}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} START CODE HERE (Replace instances of \PYZsq{}None\PYZsq{} with your code) \PYZsh{}\PYZsh{}\PYZsh{}}
         \PY{c+c1}{\PYZsh{}\PYZsh{} step 1 \PYZsh{}\PYZsh{}}
             \PY{n}{outputs} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{pred}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{outputs shape:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{outputs}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{}\PYZsh{} step 2 \PYZsh{}\PYZsh{}}
             \PY{n}{mask} \PY{o}{=} \PY{n}{labels} \PY{o}{!=} \PY{n}{pad}
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{mask shape:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{mask}\PY{o}{.}\PY{n}{shape}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{mask[0][20:30]:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{mask}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{20}\PY{p}{:}\PY{l+m+mi}{30}\PY{p}{]}\PY{p}{)}
         \PY{c+c1}{\PYZsh{}\PYZsh{} step 3 \PYZsh{}\PYZsh{}}
             \PY{n}{accuracy} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{outputs} \PY{o}{==} \PY{n}{labels}\PY{p}{)} \PY{o}{/} \PY{n+nb}{float}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{mask}\PY{p}{)}\PY{p}{)}
             \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} END CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}
             \PY{k}{return} \PY{n}{accuracy}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}19}]:} \PY{n}{accuracy} \PY{o}{=} \PY{n}{evaluate\PYZus{}prediction}\PY{p}{(}\PY{n}{model}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{vocab}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZlt{}PAD\PYZgt{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{accuracy: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{accuracy}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
outputs shape: (7194, 70)
mask shape: (7194, 70) mask[0][20:30]: [ True  True  True False False False False False False False]
accuracy:  0.9543761281155191

    \end{Verbatim}

    \textbf{Expected output (Approximately)}

\begin{verbatim}
outputs shape: (7194, 70)
mask shape: (7194, 70) mask[0][20:30]: [ True  True  True False False False False False False False]
accuracy:  0.9543761281155191
\end{verbatim}

    \# Part 5: Testing with your own sentence

    Below, you can test it out with your own sentence!

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}20}]:} \PY{c+c1}{\PYZsh{} This is the function you will be using to test your own sentence.}
         \PY{k}{def} \PY{n+nf}{predict}\PY{p}{(}\PY{n}{sentence}\PY{p}{,} \PY{n}{model}\PY{p}{,} \PY{n}{vocab}\PY{p}{,} \PY{n}{tag\PYZus{}map}\PY{p}{)}\PY{p}{:}
             \PY{n}{s} \PY{o}{=} \PY{p}{[}\PY{n}{vocab}\PY{p}{[}\PY{n}{token}\PY{p}{]} \PY{k}{if} \PY{n}{token} \PY{o+ow}{in} \PY{n}{vocab} \PY{k}{else} \PY{n}{vocab}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{UNK}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{k}{for} \PY{n}{token} \PY{o+ow}{in} \PY{n}{sentence}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ }\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{]}
             \PY{n}{batch\PYZus{}data} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{ones}\PY{p}{(}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{s}\PY{p}{)}\PY{p}{)}\PY{p}{)}
             \PY{n}{batch\PYZus{}data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{p}{:}\PY{p}{]} \PY{o}{=} \PY{n}{s}
             \PY{n}{sentence} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{batch\PYZus{}data}\PY{p}{)}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{n+nb}{int}\PY{p}{)}
             \PY{n}{output} \PY{o}{=} \PY{n}{model}\PY{p}{(}\PY{n}{sentence}\PY{p}{)}
             \PY{n}{outputs} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{output}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}
             \PY{n}{labels} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n}{tag\PYZus{}map}\PY{o}{.}\PY{n}{keys}\PY{p}{(}\PY{p}{)}\PY{p}{)}
             \PY{n}{pred} \PY{o}{=} \PY{p}{[}\PY{p}{]}
             \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{outputs}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                 \PY{n}{idx} \PY{o}{=} \PY{n}{outputs}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{n}{i}\PY{p}{]} 
                 \PY{n}{pred\PYZus{}label} \PY{o}{=} \PY{n}{labels}\PY{p}{[}\PY{n}{idx}\PY{p}{]}
                 \PY{n}{pred}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{pred\PYZus{}label}\PY{p}{)}
             \PY{k}{return} \PY{n}{pred}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}21}]:} \PY{c+c1}{\PYZsh{} Try the output for the introduction example}
         \PY{c+c1}{\PYZsh{}sentence = \PYZdq{}Many French citizens are goin to visit Morocco for summer\PYZdq{}}
         \PY{c+c1}{\PYZsh{}sentence = \PYZdq{}Sharon Floyd flew to Miami last Friday\PYZdq{}}
         
         \PY{c+c1}{\PYZsh{} New york times news:}
         \PY{n}{sentence} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Peter Navarro, the White House director of trade and manufacturing policy of U.S, said in an interview on Sunday morning that the White House was working to prepare for the possibility of a second wave of the coronavirus in the fall, though he said it wouldn’t necessarily come}\PY{l+s+s2}{\PYZdq{}}
         \PY{n}{s} \PY{o}{=} \PY{p}{[}\PY{n}{vocab}\PY{p}{[}\PY{n}{token}\PY{p}{]} \PY{k}{if} \PY{n}{token} \PY{o+ow}{in} \PY{n}{vocab} \PY{k}{else} \PY{n}{vocab}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{UNK}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{k}{for} \PY{n}{token} \PY{o+ow}{in} \PY{n}{sentence}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ }\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{]}
         \PY{n}{predictions} \PY{o}{=} \PY{n}{predict}\PY{p}{(}\PY{n}{sentence}\PY{p}{,} \PY{n}{model}\PY{p}{,} \PY{n}{vocab}\PY{p}{,} \PY{n}{tag\PYZus{}map}\PY{p}{)}
         \PY{k}{for} \PY{n}{x}\PY{p}{,}\PY{n}{y} \PY{o+ow}{in} \PY{n+nb}{zip}\PY{p}{(}\PY{n}{sentence}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ }\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,} \PY{n}{predictions}\PY{p}{)}\PY{p}{:}
             \PY{k}{if} \PY{n}{y} \PY{o}{!=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{O}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}
                 \PY{n+nb}{print}\PY{p}{(}\PY{n}{x}\PY{p}{,}\PY{n}{y}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Peter B-per
Navarro, I-per
White B-org
House I-org
Sunday B-tim
morning I-tim
White B-org
House I-org
coronavirus B-tim
fall, B-tim

    \end{Verbatim}

    ** Expected Results **

\begin{verbatim}
Peter B-per
Navarro, I-per
White B-org
House I-org
Sunday B-tim
morning I-tim
White B-org
House I-org
coronavirus B-tim
fall, B-tim
\end{verbatim}


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
