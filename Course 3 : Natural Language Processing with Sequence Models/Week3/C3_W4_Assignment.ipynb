
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{mathrsfs}
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \title{C3\_W4\_Assignment}
    
    
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \hypertarget{assignment-4-question-duplicates}{%
\section{Assignment 4: Question
duplicates}\label{assignment-4-question-duplicates}}

Welcome to the fourth assignment of course 3. In this assignment you
will explore Siamese networks applied to natural language processing.
You will further explore the fundamentals of Trax and you will be able
to implement a more complicated structure using it. By completing this
assignment, you will learn how to implement models with different
architectures.

\hypertarget{outline}{%
\subsection{Outline}\label{outline}}

\begin{itemize}
\tightlist
\item
  Section \ref{0}
\item
  Section \ref{1}

  \begin{itemize}
  \tightlist
  \item
    Section \ref{11}
  \item
    Section \ref{12}
  \item
    Section \ref{13}

    \begin{itemize}
    \tightlist
    \item
      Section \ref{ex01}
    \end{itemize}
  \end{itemize}
\item
  Section \ref{2}

  \begin{itemize}
  \tightlist
  \item
    Section \ref{21}

    \begin{itemize}
    \tightlist
    \item
      Section \ref{ex02}
    \end{itemize}
  \item
    Section \ref{22}

    \begin{itemize}
    \tightlist
    \item
      Section \ref{ex03}
    \end{itemize}
  \end{itemize}
\item
  Section \ref{3}

  \begin{itemize}
  \tightlist
  \item
    Section \ref{31}

    \begin{itemize}
    \tightlist
    \item
      Section \ref{ex04}
    \end{itemize}
  \end{itemize}
\item
  Section \ref{4}

  \begin{itemize}
  \tightlist
  \item
    Section \ref{41}
  \item
    Section \ref{42}

    \begin{itemize}
    \tightlist
    \item
      Section \ref{ex05}
    \end{itemize}
  \end{itemize}
\item
  Section \ref{5}

  \begin{itemize}
  \tightlist
  \item
    Section \ref{ex06}
  \end{itemize}
\item
  Section \ref{6}
\end{itemize}

\#\#\# Overview In this assignment, concretely you will:

\begin{itemize}
\tightlist
\item
  Learn about Siamese networks
\item
  Understand how the triplet loss works
\item
  Understand how to evaluate accuracy
\item
  Use cosine similarity between the model's outputted vectors
\item
  Use the data generator to get batches of questions
\item
  Predict using your own model
\end{itemize}

By now, you are familiar with trax and know how to make use of classes
to define your model. We will start this homework by asking you to
preprocess the data the same way you did in the previous assignments.
After processing the data you will build a classifier that will allow
you to identify whether to questions are the same or not.

You will process the data first and then pad in a similar way you have
done in the previous assignment. Your model will take in the two
question embeddings, run them through an LSTM, and then compare the
outputs of the two sub networks using cosine similarity. Before taking a
deep dive into the model, start by importing the data set.

    \# Part 1: Importing the Data \#\#\# 1.1 Loading in the data

You will be using the Quora question answer dataset to build a model
that could identify similar questions. This is a useful task because you
don't want to have several versions of the same question posted. Several
times when teaching I end up responding to similar questions on piazza,
or on other community forums. This data set has been labeled for you.
Run the cell below to import some of the packages you will be using.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{k+kn}{import} \PY{n+nn}{os}
        \PY{k+kn}{import} \PY{n+nn}{nltk}
        \PY{k+kn}{import} \PY{n+nn}{trax}
        \PY{k+kn}{from} \PY{n+nn}{trax} \PY{k}{import} \PY{n}{layers} \PY{k}{as} \PY{n}{tl}
        \PY{k+kn}{from} \PY{n+nn}{trax}\PY{n+nn}{.}\PY{n+nn}{supervised} \PY{k}{import} \PY{n}{training}
        \PY{k+kn}{from} \PY{n+nn}{trax}\PY{n+nn}{.}\PY{n+nn}{fastmath} \PY{k}{import} \PY{n}{numpy} \PY{k}{as} \PY{n}{fastnp}
        \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
        \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
        \PY{k+kn}{import} \PY{n+nn}{random} \PY{k}{as} \PY{n+nn}{rnd}
        
        \PY{c+c1}{\PYZsh{} set random seeds}
        \PY{n}{trax}\PY{o}{.}\PY{n}{supervised}\PY{o}{.}\PY{n}{trainer\PYZus{}lib}\PY{o}{.}\PY{n}{init\PYZus{}random\PYZus{}number\PYZus{}generators}\PY{p}{(}\PY{l+m+mi}{34}\PY{p}{)}
        \PY{n}{rnd}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{34}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
INFO:tensorflow:tokens\_length=568 inputs\_length=512 targets\_length=114 noise\_density=0.15 mean\_noise\_span\_length=3.0 

    \end{Verbatim}

    \textbf{Notice that for this assignment Trax's numpy is referred to as
\texttt{fastnp}, while regular numpy is referred to as \texttt{np}.}

You will now load in the data set. We have done some preprocessing for
you. If you have taken the deeplearning specialization, this is a
slightly different training method than the one you have seen there. If
you have not, then don't worry about it, we will explain everything.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{n}{data} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{questions.csv}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n}{N}\PY{o}{=}\PY{n+nb}{len}\PY{p}{(}\PY{n}{data}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Number of question pairs: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{N}\PY{p}{)}
        \PY{n}{data}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Number of question pairs:  404351

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}2}]:}    id  qid1  qid2                                          question1  \textbackslash{}
        0   0     1     2  What is the step by step guide to invest in sh{\ldots}   
        1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia{\ldots}   
        2   2     5     6  How can I increase the speed of my internet co{\ldots}   
        3   3     7     8  Why am I mentally very lonely? How can I solve{\ldots}   
        4   4     9    10  Which one dissolve in water quikly sugar, salt{\ldots}   
        
                                                   question2  is\_duplicate  
        0  What is the step by step guide to invest in sh{\ldots}             0  
        1  What would happen if the Indian government sto{\ldots}             0  
        2  How can Internet speed be increased by hacking{\ldots}             0  
        3  Find the remainder when [math]23\^{}\{24\}[/math] i{\ldots}             0  
        4            Which fish would survive in salt water?             0  
\end{Verbatim}
            
    We first split the data into a train and test set. The test set will be
used later to evaluate our model.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{n}{N\PYZus{}train} \PY{o}{=} \PY{l+m+mi}{300000}
        \PY{n}{N\PYZus{}test}  \PY{o}{=} \PY{l+m+mi}{10}\PY{o}{*}\PY{l+m+mi}{1024}
        \PY{n}{data\PYZus{}train} \PY{o}{=} \PY{n}{data}\PY{p}{[}\PY{p}{:}\PY{n}{N\PYZus{}train}\PY{p}{]}
        \PY{n}{data\PYZus{}test}  \PY{o}{=} \PY{n}{data}\PY{p}{[}\PY{n}{N\PYZus{}train}\PY{p}{:}\PY{n}{N\PYZus{}train}\PY{o}{+}\PY{n}{N\PYZus{}test}\PY{p}{]}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Train set:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{data\PYZus{}train}\PY{p}{)}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test set:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{data\PYZus{}test}\PY{p}{)}\PY{p}{)}
        \PY{k}{del}\PY{p}{(}\PY{n}{data}\PY{p}{)} \PY{c+c1}{\PYZsh{} remove to free memory}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Train set: 300000 Test set: 10240

    \end{Verbatim}

    As explained in the lectures, we select only the question pairs that are
duplicate to train the model. We build two batches as input for the
Siamese network and we assume that question \(q1_i\) (question \(i\) in
the first batch) is a duplicate of \(q2_i\) (question \(i\) in the
second batch), but all other questions in the second batch are not
duplicates of \(q1_i\).\\
The test set uses the original pairs of questions and the status
describing if the questions are duplicates.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{n}{td\PYZus{}index} \PY{o}{=} \PY{p}{(}\PY{n}{data\PYZus{}train}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{is\PYZus{}duplicate}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{==} \PY{l+m+mi}{1}\PY{p}{)}\PY{o}{.}\PY{n}{to\PYZus{}numpy}\PY{p}{(}\PY{p}{)}
        \PY{n}{td\PYZus{}index} \PY{o}{=} \PY{p}{[}\PY{n}{i} \PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{n}{x} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{td\PYZus{}index}\PY{p}{)} \PY{k}{if} \PY{n}{x}\PY{p}{]} 
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{number of duplicate questions: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{td\PYZus{}index}\PY{p}{)}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{indexes of first ten duplicate questions:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{td\PYZus{}index}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{10}\PY{p}{]}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
number of duplicate questions:  111486
indexes of first ten duplicate questions: [5, 7, 11, 12, 13, 15, 16, 18, 20, 29]

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{n+nb}{print}\PY{p}{(}\PY{n}{data\PYZus{}train}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{question1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{5}\PY{p}{]}\PY{p}{)}  \PY{c+c1}{\PYZsh{}  Example of question duplicates (first one in data)}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{data\PYZus{}train}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{question2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{5}\PY{p}{]}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{is\PYZus{}duplicate: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{data\PYZus{}train}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{is\PYZus{}duplicate}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{5}\PY{p}{]}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Astrology: I am a Capricorn Sun Cap moon and cap rising{\ldots}what does that say about me?
I'm a triple Capricorn (Sun, Moon and ascendant in Capricorn) What does this say about me?
is\_duplicate:  1

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{n}{Q1\PYZus{}train\PYZus{}words} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{data\PYZus{}train}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{question1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{[}\PY{n}{td\PYZus{}index}\PY{p}{]}\PY{p}{)}
        \PY{n}{Q2\PYZus{}train\PYZus{}words} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{data\PYZus{}train}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{question2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{[}\PY{n}{td\PYZus{}index}\PY{p}{]}\PY{p}{)}
        
        \PY{n}{Q1\PYZus{}test\PYZus{}words} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{data\PYZus{}test}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{question1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
        \PY{n}{Q2\PYZus{}test\PYZus{}words} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{data\PYZus{}test}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{question2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
        \PY{n}{y\PYZus{}test}  \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{data\PYZus{}test}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{is\PYZus{}duplicate}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}

    Above, you have seen that you only took the duplicated questions for
training our model. You did so on purpose, because the data generator
will produce batches \(([q1_1, q1_2, q1_3, ...]\),
\([q2_1, q2_2,q2_3, ...])\) where \(q1_i\) and \(q2_k\) are duplicate if
and only if \(i = k\).

Let's print to see what your data looks like.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{TRAINING QUESTIONS:}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Question 1: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{Q1\PYZus{}train\PYZus{}words}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Question 2: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{Q2\PYZus{}train\PYZus{}words}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Question 1: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{Q1\PYZus{}train\PYZus{}words}\PY{p}{[}\PY{l+m+mi}{5}\PY{p}{]}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Question 2: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{Q2\PYZus{}train\PYZus{}words}\PY{p}{[}\PY{l+m+mi}{5}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{TESTING QUESTIONS:}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Question 1: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{Q1\PYZus{}test\PYZus{}words}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Question 2: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{Q2\PYZus{}test\PYZus{}words}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{is\PYZus{}duplicate =}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
TRAINING QUESTIONS:

Question 1:  Astrology: I am a Capricorn Sun Cap moon and cap rising{\ldots}what does that say about me?
Question 2:  I'm a triple Capricorn (Sun, Moon and ascendant in Capricorn) What does this say about me? 

Question 1:  What would a Trump presidency mean for current international master’s students on an F1 visa?
Question 2:  How will a Trump presidency affect the students presently in US or planning to study in US? 

TESTING QUESTIONS:

Question 1:  How do I prepare for interviews for cse?
Question 2:  What is the best way to prepare for cse? 

is\_duplicate = 0 


    \end{Verbatim}

    You will now encode each word of the selected duplicate pairs with an
index. Given a question, you can then just encode it as a list of
numbers.

First you tokenize the questions using \texttt{nltk.word\_tokenize}. You
need a python default dictionary which later, during inference, assigns
the values \(0\) to all Out Of Vocabulary (OOV) words. Then you encode
each word of the selected duplicate pairs with an index. Given a
question, you can then just encode it as a list of numbers.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} \PY{c+c1}{\PYZsh{}create arrays}
        \PY{n}{Q1\PYZus{}train} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{empty\PYZus{}like}\PY{p}{(}\PY{n}{Q1\PYZus{}train\PYZus{}words}\PY{p}{)}
        \PY{n}{Q2\PYZus{}train} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{empty\PYZus{}like}\PY{p}{(}\PY{n}{Q2\PYZus{}train\PYZus{}words}\PY{p}{)}
        
        \PY{n}{Q1\PYZus{}test} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{empty\PYZus{}like}\PY{p}{(}\PY{n}{Q1\PYZus{}test\PYZus{}words}\PY{p}{)}
        \PY{n}{Q2\PYZus{}test} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{empty\PYZus{}like}\PY{p}{(}\PY{n}{Q2\PYZus{}test\PYZus{}words}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}9}]:} \PY{c+c1}{\PYZsh{} Building the vocabulary with the train set         (this might take a minute)}
        \PY{k+kn}{from} \PY{n+nn}{collections} \PY{k}{import} \PY{n}{defaultdict}
        
        \PY{n}{vocab} \PY{o}{=} \PY{n}{defaultdict}\PY{p}{(}\PY{k}{lambda}\PY{p}{:} \PY{l+m+mi}{0}\PY{p}{)}
        \PY{n}{vocab}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZlt{}PAD\PYZgt{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{1}
        
        \PY{k}{for} \PY{n}{idx} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{Q1\PYZus{}train\PYZus{}words}\PY{p}{)}\PY{p}{)}\PY{p}{:}
            \PY{n}{Q1\PYZus{}train}\PY{p}{[}\PY{n}{idx}\PY{p}{]} \PY{o}{=} \PY{n}{nltk}\PY{o}{.}\PY{n}{word\PYZus{}tokenize}\PY{p}{(}\PY{n}{Q1\PYZus{}train\PYZus{}words}\PY{p}{[}\PY{n}{idx}\PY{p}{]}\PY{p}{)}
            \PY{n}{Q2\PYZus{}train}\PY{p}{[}\PY{n}{idx}\PY{p}{]} \PY{o}{=} \PY{n}{nltk}\PY{o}{.}\PY{n}{word\PYZus{}tokenize}\PY{p}{(}\PY{n}{Q2\PYZus{}train\PYZus{}words}\PY{p}{[}\PY{n}{idx}\PY{p}{]}\PY{p}{)}
            \PY{n}{q} \PY{o}{=} \PY{n}{Q1\PYZus{}train}\PY{p}{[}\PY{n}{idx}\PY{p}{]} \PY{o}{+} \PY{n}{Q2\PYZus{}train}\PY{p}{[}\PY{n}{idx}\PY{p}{]}
            \PY{k}{for} \PY{n}{word} \PY{o+ow}{in} \PY{n}{q}\PY{p}{:}
                \PY{k}{if} \PY{n}{word} \PY{o+ow}{not} \PY{o+ow}{in} \PY{n}{vocab}\PY{p}{:}
                    \PY{n}{vocab}\PY{p}{[}\PY{n}{word}\PY{p}{]} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{vocab}\PY{p}{)} \PY{o}{+} \PY{l+m+mi}{1}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{The length of the vocabulary is: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{vocab}\PY{p}{)}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
The length of the vocabulary is:  36268

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{n+nb}{print}\PY{p}{(}\PY{n}{vocab}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZlt{}PAD\PYZgt{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{vocab}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Astrology}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{vocab}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Astronomy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}  \PY{c+c1}{\PYZsh{}not in vocabulary, returns 0}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
1
2
0

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} \PY{k}{for} \PY{n}{idx} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{Q1\PYZus{}test\PYZus{}words}\PY{p}{)}\PY{p}{)}\PY{p}{:} 
             \PY{n}{Q1\PYZus{}test}\PY{p}{[}\PY{n}{idx}\PY{p}{]} \PY{o}{=} \PY{n}{nltk}\PY{o}{.}\PY{n}{word\PYZus{}tokenize}\PY{p}{(}\PY{n}{Q1\PYZus{}test\PYZus{}words}\PY{p}{[}\PY{n}{idx}\PY{p}{]}\PY{p}{)}
             \PY{n}{Q2\PYZus{}test}\PY{p}{[}\PY{n}{idx}\PY{p}{]} \PY{o}{=} \PY{n}{nltk}\PY{o}{.}\PY{n}{word\PYZus{}tokenize}\PY{p}{(}\PY{n}{Q2\PYZus{}test\PYZus{}words}\PY{p}{[}\PY{n}{idx}\PY{p}{]}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}12}]:} \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Train set has reduced to: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{Q1\PYZus{}train}\PY{p}{)} \PY{p}{)} 
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Test set length: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{Q1\PYZus{}test}\PY{p}{)} \PY{p}{)} 
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Train set has reduced to:  111486
Test set length:  10240

    \end{Verbatim}

    \#\#\# 1.2 Converting a question to a tensor

You will now convert every question to a tensor, or an array of numbers,
using your vocabulary built above.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}13}]:} \PY{c+c1}{\PYZsh{} Converting questions to array of integers}
         \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{Q1\PYZus{}train}\PY{p}{)}\PY{p}{)}\PY{p}{:}
             \PY{n}{Q1\PYZus{}train}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{=} \PY{p}{[}\PY{n}{vocab}\PY{p}{[}\PY{n}{word}\PY{p}{]} \PY{k}{for} \PY{n}{word} \PY{o+ow}{in} \PY{n}{Q1\PYZus{}train}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{]}
             \PY{n}{Q2\PYZus{}train}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{=} \PY{p}{[}\PY{n}{vocab}\PY{p}{[}\PY{n}{word}\PY{p}{]} \PY{k}{for} \PY{n}{word} \PY{o+ow}{in} \PY{n}{Q2\PYZus{}train}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{]}
         
                 
         \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{Q1\PYZus{}test}\PY{p}{)}\PY{p}{)}\PY{p}{:}
             \PY{n}{Q1\PYZus{}test}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{=} \PY{p}{[}\PY{n}{vocab}\PY{p}{[}\PY{n}{word}\PY{p}{]} \PY{k}{for} \PY{n}{word} \PY{o+ow}{in} \PY{n}{Q1\PYZus{}test}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{]}
             \PY{n}{Q2\PYZus{}test}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{=} \PY{p}{[}\PY{n}{vocab}\PY{p}{[}\PY{n}{word}\PY{p}{]} \PY{k}{for} \PY{n}{word} \PY{o+ow}{in} \PY{n}{Q2\PYZus{}test}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{]}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{first question in the train set:}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{Q1\PYZus{}train\PYZus{}words}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} 
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{encoded version:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{Q1\PYZus{}train}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{first question in the test set:}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{Q1\PYZus{}test\PYZus{}words}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{encoded version:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{Q1\PYZus{}test}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)} 
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
first question in the train set:

Astrology: I am a Capricorn Sun Cap moon and cap rising{\ldots}what does that say about me? 

encoded version:
[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21] 

first question in the test set:

How do I prepare for interviews for cse? 

encoded version:
[32, 38, 4, 107, 65, 1015, 65, 11509, 21]

    \end{Verbatim}

    You will now split your train set into a training/validation set so that
you can use it to train and evaluate your Siamese model.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}15}]:} \PY{c+c1}{\PYZsh{} Splitting the data}
         \PY{n}{cut\PYZus{}off} \PY{o}{=} \PY{n+nb}{int}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{Q1\PYZus{}train}\PY{p}{)}\PY{o}{*}\PY{o}{.}\PY{l+m+mi}{8}\PY{p}{)}
         \PY{n}{train\PYZus{}Q1}\PY{p}{,} \PY{n}{train\PYZus{}Q2} \PY{o}{=} \PY{n}{Q1\PYZus{}train}\PY{p}{[}\PY{p}{:}\PY{n}{cut\PYZus{}off}\PY{p}{]}\PY{p}{,} \PY{n}{Q2\PYZus{}train}\PY{p}{[}\PY{p}{:}\PY{n}{cut\PYZus{}off}\PY{p}{]}
         \PY{n}{val\PYZus{}Q1}\PY{p}{,} \PY{n}{val\PYZus{}Q2} \PY{o}{=} \PY{n}{Q1\PYZus{}train}\PY{p}{[}\PY{n}{cut\PYZus{}off}\PY{p}{:} \PY{p}{]}\PY{p}{,} \PY{n}{Q2\PYZus{}train}\PY{p}{[}\PY{n}{cut\PYZus{}off}\PY{p}{:}\PY{p}{]}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Number of duplicate questions: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{Q1\PYZus{}train}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{The length of the training set is:  }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{train\PYZus{}Q1}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{The length of the validation set is: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{val\PYZus{}Q1}\PY{p}{)}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Number of duplicate questions:  111486
The length of the training set is:   89188
The length of the validation set is:  22298

    \end{Verbatim}

    \#\#\# 1.3 Understanding the iterator

Most of the time in Natural Language Processing, and AI in general we
use batches when training our data sets. If you were to use stochastic
gradient descent with one example at a time, it will take you forever to
build a model. In this example, we show you how you can build a data
generator that takes in \(Q1\) and \(Q2\) and returns a batch of size
\texttt{batch\_size} in the following format
\(([q1_1, q1_2, q1_3, ...]\), \([q2_1, q2_2,q2_3, ...])\). The tuple
consists of two arrays and each array has \texttt{batch\_size}
questions. Again, \(q1_i\) and \(q2_i\) are duplicates, but they are not
duplicates with any other elements in the batch.

The command \texttt{next(data\_generator)}returns the next batch. This
iterator returns the data in a format that you could directly use in
your model when computing the feed-forward of your algorithm. This
iterator returns a pair of arrays of questions.

\#\#\# Exercise 01

\textbf{Instructions:}\\
Implement the data generator below. Here are some things you will need.

\begin{itemize}
\tightlist
\item
  While true loop.
\item
  if \texttt{index\ \textgreater{}=\ len\_Q1}, set the \texttt{idx} to
  \(0\).
\item
  The generator should return shuffled batches of data. To achieve this
  without modifying the actual question lists, a list containing the
  indexes of the questions is created. This list can be shuffled and
  used to get random batches everytime the index is reset.
\item
  Append elements of \(Q1\) and \(Q2\) to \texttt{input1} and
  \texttt{input2} respectively.
\item
  if \texttt{len(input1)\ ==\ batch\_size}, determine \texttt{max\_len}
  as the longest question in \texttt{input1} and \texttt{input2}. Ceil
  \texttt{max\_len} to a power of \(2\) (for computation purposes) using
  the following command:
  \texttt{max\_len\ =\ 2**int(np.ceil(np.log2(max\_len)))}.
\item
  Pad every question by
  \texttt{vocab{[}\textquotesingle{}\textless{}PAD\textgreater{}\textquotesingle{}{]}}
  until you get the length \texttt{max\_len}.
\item
  Use yield to return \texttt{input1,\ input2}.
\item
  Don't forget to reset \texttt{input1,\ input2} to empty arrays at the
  end (data generator resumes from where it last left).
\end{itemize}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}16}]:} \PY{c+c1}{\PYZsh{} UNQ\PYZus{}C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)}
         \PY{c+c1}{\PYZsh{} GRADED FUNCTION: data\PYZus{}generator}
         \PY{k}{def} \PY{n+nf}{data\PYZus{}generator}\PY{p}{(}\PY{n}{Q1}\PY{p}{,} \PY{n}{Q2}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{p}{,} \PY{n}{pad}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{shuffle}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Generator function that yields batches of data}
         
         \PY{l+s+sd}{    Args:}
         \PY{l+s+sd}{        Q1 (list): List of transformed (to tensor) questions.}
         \PY{l+s+sd}{        Q2 (list): List of transformed (to tensor) questions.}
         \PY{l+s+sd}{        batch\PYZus{}size (int): Number of elements per batch.}
         \PY{l+s+sd}{        pad (int, optional): Pad character from the vocab. Defaults to 1.}
         \PY{l+s+sd}{        shuffle (bool, optional): If the batches should be randomnized or not. Defaults to True.}
         \PY{l+s+sd}{    Yields:}
         \PY{l+s+sd}{        tuple: Of the form (input1, input2) with types (numpy.ndarray, numpy.ndarray)}
         \PY{l+s+sd}{        NOTE: input1: inputs to your model [q1a, q2a, q3a, ...] i.e. (q1a,q1b) are duplicates}
         \PY{l+s+sd}{              input2: targets to your model [q1b, q2b,q3b, ...] i.e. (q1a,q2i) i!=a are not duplicates}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
         
             \PY{n}{input1} \PY{o}{=} \PY{p}{[}\PY{p}{]}
             \PY{n}{input2} \PY{o}{=} \PY{p}{[}\PY{p}{]}
             \PY{n}{idx} \PY{o}{=} \PY{l+m+mi}{0}
             \PY{n}{len\PYZus{}q} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{Q1}\PY{p}{)}
             \PY{n}{question\PYZus{}indexes} \PY{o}{=} \PY{p}{[}\PY{o}{*}\PY{n+nb}{range}\PY{p}{(}\PY{n}{len\PYZus{}q}\PY{p}{)}\PY{p}{]}
             
             \PY{k}{if} \PY{n}{shuffle}\PY{p}{:}
                 \PY{n}{rnd}\PY{o}{.}\PY{n}{shuffle}\PY{p}{(}\PY{n}{question\PYZus{}indexes}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} START CODE HERE (Replace instances of \PYZsq{}None\PYZsq{} with your code) \PYZsh{}\PYZsh{}\PYZsh{}}
             \PY{k}{while} \PY{k+kc}{True}\PY{p}{:}
                 \PY{k}{if} \PY{n}{idx} \PY{o}{\PYZgt{}}\PY{o}{=} \PY{n}{len\PYZus{}q}\PY{p}{:}
                     \PY{c+c1}{\PYZsh{} if idx is greater than or equal to len\PYZus{}q, set idx accordingly }
                     \PY{c+c1}{\PYZsh{} (Hint: look at the instructions above)}
                     \PY{n}{idx} \PY{o}{=} \PY{n}{len\PYZus{}q}
                     \PY{c+c1}{\PYZsh{} shuffle to get random batches if shuffle is set to True}
                     \PY{k}{if} \PY{n}{shuffle}\PY{p}{:}
                         \PY{n}{rnd}\PY{o}{.}\PY{n}{shuffle}\PY{p}{(}\PY{n}{question\PYZus{}indexes}\PY{p}{)}
                 
                 \PY{c+c1}{\PYZsh{} get questions at the `question\PYZus{}indexes[idx]` position in Q1 and Q2}
                 \PY{n}{q1} \PY{o}{=} \PY{n}{Q1}\PY{p}{[}\PY{n}{question\PYZus{}indexes}\PY{p}{[}\PY{n}{idx}\PY{p}{]}\PY{p}{]}
                 \PY{n}{q2} \PY{o}{=} \PY{n}{Q2}\PY{p}{[}\PY{n}{question\PYZus{}indexes}\PY{p}{[}\PY{n}{idx}\PY{p}{]}\PY{p}{]}
                 
                 \PY{c+c1}{\PYZsh{} increment idx by 1}
                 \PY{n}{idx} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
                 \PY{c+c1}{\PYZsh{} append q1}
                 \PY{n}{input1}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{q1}\PY{p}{)}
                 \PY{c+c1}{\PYZsh{} append q2}
                 \PY{n}{input2}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{q2}\PY{p}{)}
                 \PY{k}{if} \PY{n+nb}{len}\PY{p}{(}\PY{n}{input1}\PY{p}{)} \PY{o}{==} \PY{n}{batch\PYZus{}size}\PY{p}{:}
                     \PY{c+c1}{\PYZsh{} determine max\PYZus{}len as the longest question in input1 \PYZam{} input 2}
                     \PY{c+c1}{\PYZsh{} Hint: use the `max` function. }
                     \PY{c+c1}{\PYZsh{} take max of input1 \PYZam{} input2 and then max out of the two of them.}
                     \PY{n}{max\PYZus{}len} \PY{o}{=} \PY{n+nb}{max}\PY{p}{(}\PY{n+nb}{max}\PY{p}{(}\PY{p}{[}\PY{n+nb}{len}\PY{p}{(}\PY{n}{q}\PY{p}{)} \PY{k}{for} \PY{n}{q} \PY{o+ow}{in} \PY{n}{input1}\PY{p}{]}\PY{p}{)}\PY{p}{,}\PY{n+nb}{max}\PY{p}{(}\PY{p}{[}\PY{n+nb}{len}\PY{p}{(}\PY{n}{q}\PY{p}{)} \PY{k}{for} \PY{n}{q} \PY{o+ow}{in} \PY{n}{input2}\PY{p}{]}\PY{p}{)}\PY{p}{)}
                     \PY{c+c1}{\PYZsh{} pad to power\PYZhy{}of\PYZhy{}2 (Hint: look at the instructions above)}
                     \PY{n}{max\PYZus{}len} \PY{o}{=} \PY{l+m+mi}{2}\PY{o}{*}\PY{o}{*}\PY{n+nb}{int}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{ceil}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{log2}\PY{p}{(}\PY{n}{max\PYZus{}len}\PY{p}{)}\PY{p}{)}\PY{p}{)}
                     \PY{n}{b1} \PY{o}{=} \PY{p}{[}\PY{p}{]}
                     \PY{n}{b2} \PY{o}{=} \PY{p}{[}\PY{p}{]}
                     \PY{k}{for} \PY{n}{q1}\PY{p}{,} \PY{n}{q2} \PY{o+ow}{in} \PY{n+nb}{zip}\PY{p}{(}\PY{n}{input1}\PY{p}{,} \PY{n}{input2}\PY{p}{)}\PY{p}{:}
                         \PY{c+c1}{\PYZsh{} add [pad] to q1 until it reaches max\PYZus{}len}
                         \PY{n}{q1} \PY{o}{=} \PY{n}{q1} \PY{o}{+} \PY{p}{[}\PY{n}{pad}\PY{p}{]} \PY{o}{*} \PY{p}{(}\PY{n}{max\PYZus{}len} \PY{o}{\PYZhy{}} \PY{n+nb}{len}\PY{p}{(}\PY{n}{q1}\PY{p}{)}\PY{p}{)}
                         \PY{c+c1}{\PYZsh{} add [pad] to q2 until it reaches max\PYZus{}len}
                         \PY{n}{q2} \PY{o}{=} \PY{n}{q2} \PY{o}{+} \PY{p}{[}\PY{n}{pad}\PY{p}{]} \PY{o}{*} \PY{p}{(}\PY{n}{max\PYZus{}len} \PY{o}{\PYZhy{}} \PY{n+nb}{len}\PY{p}{(}\PY{n}{q2}\PY{p}{)}\PY{p}{)}
                         \PY{c+c1}{\PYZsh{} append q1}
                         \PY{n}{b1}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{q1}\PY{p}{)}
                         \PY{c+c1}{\PYZsh{} append q2}
                         \PY{n}{b2}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{q2}\PY{p}{)}
                     \PY{c+c1}{\PYZsh{} use b1 and b2}
                     \PY{k}{yield} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{b1}\PY{p}{)}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{b2}\PY{p}{)}
             \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} END CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}
                     \PY{c+c1}{\PYZsh{} reset the batches}
                     \PY{n}{input1}\PY{p}{,} \PY{n}{input2} \PY{o}{=} \PY{p}{[}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{p}{]}  \PY{c+c1}{\PYZsh{} reset the batches}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}17}]:} \PY{n}{batch\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{2}
         \PY{n}{res1}\PY{p}{,} \PY{n}{res2} \PY{o}{=} \PY{n+nb}{next}\PY{p}{(}\PY{n}{data\PYZus{}generator}\PY{p}{(}\PY{n}{train\PYZus{}Q1}\PY{p}{,} \PY{n}{train\PYZus{}Q2}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{First questions  : }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{res1}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Second questions : }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{res2}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
First questions  :  
 [[  30   87   78  134 2132 1981   28   78  594   21    1    1    1    1
     1    1]
 [  30   55   78 3541 1460   28   56  253   21    1    1    1    1    1
     1    1]] 

Second questions :  
 [[  30  156   78  134 2132 9508   21    1    1    1    1    1    1    1
     1    1]
 [  30  156   78 3541 1460  131   56  253   21    1    1    1    1    1
     1    1]]

    \end{Verbatim}

    \textbf{Note}: The following expected output is valid only if you run
the above test cell \textbf{\emph{once}} (first time). The output will
change on each execution.

If you think your implementation is correct and it is not matching the
output, make sure to restart the kernel and run all the cells from the
top again.

\textbf{Expected Output:}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{First questions  :  }
\NormalTok{ [[}\AttributeTok{  }\DecValTok{30}\AttributeTok{   }\DecValTok{87}\AttributeTok{   }\DecValTok{78}\AttributeTok{  }\DecValTok{134}\AttributeTok{ }\DecValTok{2132}\AttributeTok{ }\DecValTok{1981}\AttributeTok{   }\DecValTok{28}\AttributeTok{   }\DecValTok{78}\AttributeTok{  }\DecValTok{594}\AttributeTok{   }\DecValTok{21}\AttributeTok{    }\DecValTok{1}\AttributeTok{    }\DecValTok{1}\AttributeTok{    }\DecValTok{1}\AttributeTok{    }\DecValTok{1}
     \DecValTok{1}    \DecValTok{1}\NormalTok{]}
\NormalTok{ [  }\DecValTok{30}   \DecValTok{55}   \DecValTok{78} \DecValTok{3541} \DecValTok{1460}   \DecValTok{28}   \DecValTok{56}  \DecValTok{253}   \DecValTok{21}    \DecValTok{1}    \DecValTok{1}    \DecValTok{1}    \DecValTok{1}    \DecValTok{1}
     \DecValTok{1}    \DecValTok{1}\NormalTok{]] }

\NormalTok{Second questions :  }
\NormalTok{ [[}\AttributeTok{  }\DecValTok{30}\AttributeTok{  }\DecValTok{156}\AttributeTok{   }\DecValTok{78}\AttributeTok{  }\DecValTok{134}\AttributeTok{ }\DecValTok{2132}\AttributeTok{ }\DecValTok{9508}\AttributeTok{   }\DecValTok{21}\AttributeTok{    }\DecValTok{1}\AttributeTok{    }\DecValTok{1}\AttributeTok{    }\DecValTok{1}\AttributeTok{    }\DecValTok{1}\AttributeTok{    }\DecValTok{1}\AttributeTok{    }\DecValTok{1}\AttributeTok{    }\DecValTok{1}
     \DecValTok{1}    \DecValTok{1}\NormalTok{]}
\NormalTok{ [  }\DecValTok{30}  \DecValTok{156}   \DecValTok{78} \DecValTok{3541} \DecValTok{1460}  \DecValTok{131}   \DecValTok{56}  \DecValTok{253}   \DecValTok{21}    \DecValTok{1}    \DecValTok{1}    \DecValTok{1}    \DecValTok{1}    \DecValTok{1}
     \DecValTok{1}    \DecValTok{1}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

Now that you have your generator, you can just call it and it will
return tensors which correspond to your questions in the Quora data
set.Now you can go ahead and start building your neural network.

    \# Part 2: Defining the Siamese model

\hypertarget{understanding-siamese-network}{%
\subsubsection{2.1 Understanding Siamese
Network}\label{understanding-siamese-network}}

A Siamese network is a neural network which uses the same weights while
working in tandem on two different input vectors to compute comparable
output vectors.The Siamese network you are about to implement looks like
this:

You get the question embedding, run it through an LSTM layer, normalize
\(v_1\) and \(v_2\), and finally use a triplet loss (explained below) to
get the corresponding cosine similarity for each pair of questions. As
usual, you will start by importing the data set. The triplet loss makes
use of a baseline (anchor) input that is compared to a positive (truthy)
input and a negative (falsy) input. The distance from the baseline
(anchor) input to the positive (truthy) input is minimized, and the
distance from the baseline (anchor) input to the negative (falsy) input
is maximized. In math equations, you are trying to maximize the
following.

\[\mathcal{L}(A, P, N)=\max \left(\|\mathrm{f}(A)-\mathrm{f}(P)\|^{2}-\|\mathrm{f}(A)-\mathrm{f}(N)\|^{2}+\alpha, 0\right)\]

\(A\) is the anchor input, for example \(q1_1\), \(P\) the duplicate
input, for example, \(q2_1\), and \(N\) the negative input (the non
duplicate question), for example \(q2_2\). \(\alpha\) is a margin; you
can think about it as a safety net, or by how much you want to push the
duplicates from the non duplicates.

\#\#\# Exercise 02

\textbf{Instructions:} Implement the \texttt{Siamese} function below.
You should be using all the objects explained below.

To implement this model, you will be using \texttt{trax}. Concretely,
you will be using the following functions.

\begin{itemize}
\tightlist
\item
  \texttt{tl.Serial}: Combinator that applies layers serially (by
  function composition) allows you set up the overall structure of the
  feedforward.
  \href{https://trax-ml.readthedocs.io/en/latest/trax.layers.html\#trax.layers.combinators.Serial}{docs}
  /
  \href{https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/combinators.py\#L26}{source
  code}

  \begin{itemize}
  \tightlist
  \item
    You can pass in the layers as arguments to \texttt{Serial},
    separated by commas.
  \item
    For example:
    \texttt{tl.Serial(tl.Embeddings(...),\ tl.Mean(...),\ tl.Dense(...),\ tl.LogSoftmax(...))}
  \end{itemize}
\item
  \texttt{tl.Embedding}: Maps discrete tokens to vectors. It will have
  shape (vocabulary length X dimension of output vectors). The dimension
  of output vectors (also called d\_feature) is the number of elements
  in the word embedding.
  \href{https://trax-ml.readthedocs.io/en/latest/trax.layers.html\#trax.layers.core.Embedding}{docs}
  /
  \href{https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/core.py\#L113}{source
  code}

  \begin{itemize}
  \tightlist
  \item
    \texttt{tl.Embedding(vocab\_size,\ d\_feature)}.
  \item
    \texttt{vocab\_size} is the number of unique words in the given
    vocabulary.
  \item
    \texttt{d\_feature} is the number of elements in the word embedding
    (some choices for a word embedding size range from 150 to 300, for
    example).
  \end{itemize}
\item
  \texttt{tl.LSTM} The LSTM layer. It leverages another Trax layer
  called
  \href{https://trax-ml.readthedocs.io/en/latest/trax.layers.html\#trax.layers.rnn.LSTMCell}{\texttt{LSTMCell}}.
  The number of units should be specified and should match the number of
  elements in the word embedding.
  \href{https://trax-ml.readthedocs.io/en/latest/trax.layers.html\#trax.layers.rnn.LSTM}{docs}
  /
  \href{https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/rnn.py\#L87}{source
  code}

  \begin{itemize}
  \tightlist
  \item
    \texttt{tl.LSTM(n\_units)} Builds an LSTM layer of n\_units.
  \end{itemize}
\item
  \texttt{tl.Mean}: Computes the mean across a desired axis. Mean uses
  one tensor axis to form groups of values and replaces each group with
  the mean value of that group.
  \href{https://trax-ml.readthedocs.io/en/latest/trax.layers.html\#trax.layers.core.Mean}{docs}
  /
  \href{https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/core.py\#L276}{source
  code}

  \begin{itemize}
  \tightlist
  \item
    \texttt{tl.Mean(axis=1)} mean over columns.
  \end{itemize}
\item
  \texttt{tl.Fn} Layer with no weights that applies the function f,
  which should be specified using a lambda syntax.
  \href{https://trax-ml.readthedocs.io/en/latest/trax.layers.html\#trax.layers.base.Fn}{docs}
  /
  \href{https://github.com/google/trax/blob/70f5364dcaf6ec11aabbd918e5f5e4b0f5bfb995/trax/layers/base.py\#L576}{source
  doce}

  \begin{itemize}
  \tightlist
  \item
    \(x\) -\textgreater{} This is used for cosine similarity.
  \item
    \texttt{tl.Fn(\textquotesingle{}Normalize\textquotesingle{},\ lambda\ x:\ normalize(x))}
    Returns a layer with no weights that applies the function \texttt{f}
  \end{itemize}
\item
  \texttt{tl.parallel}: It is a combinator layer (like \texttt{Serial})
  that applies a list of layers in parallel to its inputs.
  \href{https://trax-ml.readthedocs.io/en/latest/trax.layers.html\#trax.layers.combinators.Parallel}{docs}
  /
  \href{https://github.com/google/trax/blob/37aba571a89a8ad86be76a569d0ec4a46bdd8642/trax/layers/combinators.py\#L152}{source
  code}
\end{itemize}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}18}]:} \PY{c+c1}{\PYZsh{} UNQ\PYZus{}C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)}
         \PY{c+c1}{\PYZsh{} GRADED FUNCTION: Siamese}
         \PY{k}{def} \PY{n+nf}{Siamese}\PY{p}{(}\PY{n}{vocab\PYZus{}size}\PY{o}{=}\PY{n+nb}{len}\PY{p}{(}\PY{n}{vocab}\PY{p}{)}\PY{p}{,} \PY{n}{d\PYZus{}model}\PY{o}{=}\PY{l+m+mi}{128}\PY{p}{,} \PY{n}{mode}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Returns a Siamese model.}
         
         \PY{l+s+sd}{    Args:}
         \PY{l+s+sd}{        vocab\PYZus{}size (int, optional): Length of the vocabulary. Defaults to len(vocab).}
         \PY{l+s+sd}{        d\PYZus{}model (int, optional): Depth of the model. Defaults to 128.}
         \PY{l+s+sd}{        mode (str, optional): \PYZsq{}train\PYZsq{}, \PYZsq{}eval\PYZsq{} or \PYZsq{}predict\PYZsq{}, predict mode is for fast inference. Defaults to \PYZsq{}train\PYZsq{}.}
         
         \PY{l+s+sd}{    Returns:}
         \PY{l+s+sd}{        trax.layers.combinators.Parallel: A Siamese model. }
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
         
             \PY{k}{def} \PY{n+nf}{normalize}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{:}  \PY{c+c1}{\PYZsh{} normalizes the vectors to have L2 norm 1}
                 \PY{k}{return} \PY{n}{x} \PY{o}{/} \PY{n}{fastnp}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{fastnp}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{x} \PY{o}{*} \PY{n}{x}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{keepdims}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} START CODE HERE (Replace instances of \PYZsq{}None\PYZsq{} with your code) \PYZsh{}\PYZsh{}\PYZsh{}}
             \PY{n}{q\PYZus{}processor} \PY{o}{=} \PY{n}{tl}\PY{o}{.}\PY{n}{Serial}\PY{p}{(}  \PY{c+c1}{\PYZsh{} Processor will run on Q1 and Q2.}
                 \PY{n}{tl}\PY{o}{.}\PY{n}{Embedding}\PY{p}{(}\PY{n}{vocab\PYZus{}size}\PY{p}{,} \PY{n}{d\PYZus{}model}\PY{p}{)}\PY{p}{,} \PY{c+c1}{\PYZsh{} Embedding layer}
                 \PY{n}{tl}\PY{o}{.}\PY{n}{LSTM}\PY{p}{(}\PY{n}{d\PYZus{}model}\PY{p}{)}\PY{p}{,} \PY{c+c1}{\PYZsh{} LSTM layer}
                 \PY{n}{tl}\PY{o}{.}\PY{n}{Mean}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{c+c1}{\PYZsh{} Mean over columns}
                 \PY{n}{tl}\PY{o}{.}\PY{n}{Fn}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Normalize}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{n}{normalize}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Apply normalize function}
             \PY{p}{)}  \PY{c+c1}{\PYZsh{} Returns one vector of shape [batch\PYZus{}size, d\PYZus{}model].}
             
             \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} END CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}
             
             \PY{c+c1}{\PYZsh{} Run on Q1 and Q2 in parallel.}
             \PY{n}{model} \PY{o}{=} \PY{n}{tl}\PY{o}{.}\PY{n}{Parallel}\PY{p}{(}\PY{n}{q\PYZus{}processor}\PY{p}{,} \PY{n}{q\PYZus{}processor}\PY{p}{)}
             \PY{k}{return} \PY{n}{model}
\end{Verbatim}

    Setup the Siamese network model

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}19}]:} \PY{c+c1}{\PYZsh{} check your model}
         \PY{n}{model} \PY{o}{=} \PY{n}{Siamese}\PY{p}{(}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{model}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Parallel\_in2\_out2[
  Serial[
    Embedding\_41699\_128
    LSTM\_128
    Mean
    Normalize
  ]
  Serial[
    Embedding\_41699\_128
    LSTM\_128
    Mean
    Normalize
  ]
]

    \end{Verbatim}

    \textbf{Expected output:}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Parallel_in2_out2[}
\NormalTok{  Serial[}
\NormalTok{    Embeddin}\VariableTok{g_41699_128}
\NormalTok{    LSTM_128}
\NormalTok{    Mean}
\NormalTok{    Normalize}
\NormalTok{  ]}
\NormalTok{  Serial[}
\NormalTok{    Embeddin}\VariableTok{g_41699_128}
\NormalTok{    LSTM_128}
\NormalTok{    Mean}
\NormalTok{    Normalize}
\NormalTok{  ]}
\NormalTok{]}
\end{Highlighting}
\end{Shaded}

    \hypertarget{hard-negative-mining}{%
\subsubsection{2.2 Hard Negative Mining}\label{hard-negative-mining}}

You will now implement the \texttt{TripletLoss}. As explained in the
lecture, loss is composed of two terms. One term utilizes the mean of
all the non duplicates, the second utilizes the \emph{closest negative}.
Our loss expression is then:

\begin{align}
 \mathcal{Loss_1(A,P,N)} &=\max \left( -cos(A,P)  + mean_{neg} +\alpha, 0\right) \\
 \mathcal{Loss_2(A,P,N)} &=\max \left( -cos(A,P)  + closest_{neg} +\alpha, 0\right) \\
\mathcal{Loss(A,P,N)} &= mean(Loss_1 + Loss_2) \\
\end{align}

Further, two sets of instructions are provided. The first set provides a
brief description of the task. If that set proves insufficient, a more
detailed set can be displayed.

\#\#\# Exercise 03

\textbf{Instructions (Brief):} Here is a list of things you should do:

\begin{itemize}
\tightlist
\item
  As this will be run inside trax, use \texttt{fastnp.xyz} when using
  any \texttt{xyz} numpy function
\item
  Use \texttt{fastnp.dot} to calculate the similarity matrix
  \(v_1v_2^T\) of dimension \texttt{batch\_size} x \texttt{batch\_size}
\item
  Take the score of the duplicates on the diagonal
  \texttt{fastnp.diagonal}
\item
  Use the \texttt{trax} functions \texttt{fastnp.eye} and
  \texttt{fastnp.maximum} for the identity matrix and the maximum.
\end{itemize}

    More Detailed Instructions We'll describe the algorithm using a detailed
example. Below, V1, V2 are the output of the normalization blocks in our
model. Here we will use a batch\_size of 4 and a d\_model of 3. As
explained in lecture, the inputs, Q1, Q2 are arranged so that
corresponding inputs are duplicates while non-corresponding entries are
not. The outputs will have the same pattern. This testcase arranges the
outputs, v1,v2, to highlight different scenarios. Here, the first two
outputs V1{[}0{]}, V2{[}0{]} match exactly - so the model is generating
the same vector for Q1{[}0{]} and Q2{[}0{]} inputs. The second outputs
differ, circled in orange, we set, V2{[}1{]} is set to match
V2{[}\textbf{2}{]}, simulating a model which is generating very poor
results. V1{[}3{]} and V2{[}3{]} match exactly again while V1{[}4{]} and
V2{[}4{]} are set to be exactly wrong - 180 degrees from each other,
circled in blue.

The first step is to compute the cosine similarity matrix or
\texttt{score} in the code. As explained in lecture, this is
\[V_1 V_2^T\] This is generated with \texttt{fastnp.dot}. The clever
arrangement of inputs creates the data needed for positive \emph{and}
negative examples without having to run all pair-wise combinations.
Because Q1{[}n{]} is a duplicate of only Q2{[}n{]}, other combinations
are explicitly created negative examples or \emph{Hard Negative}
examples. The matrix multiplication efficiently produces the cosine
similarity of all positive/negative combinations as shown above on the
left side of the diagram. `Positive' are the results of duplicate
examples and `negative' are the results of explicitly created negative
examples. The results for our test case are as expected,
V1{[}0{]}V2{[}0{]} match producing `1' while our other `positive' cases
(in green) don't match well, as was arranged. The V2{[}2{]} was set to
match V1{[}3{]} producing a poor match at \texttt{score{[}2,2{]}} and an
undesired `negative' case of a `1' shown in grey.

With the similarity matrix (\texttt{score}) we can begin to implement
the loss equations. First, we can extract \[cos(A,P)\] by utilizing
\texttt{fastnp.diagonal}. The goal is to grab all the green entries in
the diagram above. This is \texttt{positive} in the code.

Next, we will create the \emph{closest\_negative}. This is the
nonduplicate entry in V2 that is closest (has largest cosine similarity)
to an entry in V1. Each row, n, of \texttt{score} represents all
comparisons of the results of Q1{[}n{]} vs Q2{[}x{]} within a batch. A
specific example in our testcase is row \texttt{score{[}2,:{]}}. It has
the cosine similarity of V1{[}2{]} and V2{[}x{]}. The
\emph{closest\_negative}, as was arranged, is V2{[}2{]} which has a
score of 1. This is the maximum value of the `negative' entries (blue
entries in the diagram).

To implement this, we need to pick the maximum entry on a row of
\texttt{score}, ignoring the `positive'/green entries. To avoid
selecting the `positive'/green entries, we can make them larger negative
numbers. Multiply \texttt{fastnp.eye(batch\_size)} with 2.0 and subtract
it out of \texttt{scores}. The result is
\texttt{negative\_without\_positive}. Now we can use
\texttt{fastnp.max}, row by row (axis=1), to select the maximum which is
\texttt{closest\_negative}.

Next, we'll create \emph{mean\_negative}. As the name suggests, this is
the mean of all the `negative'/blue values in \texttt{score} on a row by
row basis. We can use \texttt{fastnp.eye(batch\_size)} and a constant,
this time to create a mask with zeros on the diagonal. Element-wise
multiply this with \texttt{score} to get just the 'negative values. This
is \texttt{negative\_zero\_on\_duplicate} in the code. Compute the mean
by using \texttt{fastnp.sum} on \texttt{negative\_zero\_on\_duplicate}
for \texttt{axis=1} and divide it by \texttt{(batch\_size\ -\ 1)} . This
is \texttt{mean\_negative}.

Now, we can compute loss using the two equations above and
\texttt{fastnp.maximum}. This will form \texttt{triplet\_loss1} and
\texttt{triplet\_loss2}.

\texttt{triple\_loss} is the \texttt{fastnp.mean} of the sum of the two
individual losses.

Once you have this code matching the expected results, you can clip out
the section between \#\#\# START CODE HERE and \#\#\# END CODE HERE it
out and insert it into TripletLoss below.

\textless{}\details\textgreater{}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}20}]:} \PY{c+c1}{\PYZsh{} UNQ\PYZus{}C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)}
         \PY{c+c1}{\PYZsh{} GRADED FUNCTION: TripletLossFn}
         \PY{k}{def} \PY{n+nf}{TripletLossFn}\PY{p}{(}\PY{n}{v1}\PY{p}{,} \PY{n}{v2}\PY{p}{,} \PY{n}{margin}\PY{o}{=}\PY{l+m+mf}{0.25}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Custom Loss function.}
         
         \PY{l+s+sd}{    Args:}
         \PY{l+s+sd}{        v1 (numpy.ndarray): Array with dimension (batch\PYZus{}size, model\PYZus{}dimension) associated to Q1.}
         \PY{l+s+sd}{        v2 (numpy.ndarray): Array with dimension (batch\PYZus{}size, model\PYZus{}dimension) associated to Q2.}
         \PY{l+s+sd}{        margin (float, optional): Desired margin. Defaults to 0.25.}
         
         \PY{l+s+sd}{    Returns:}
         \PY{l+s+sd}{        jax.interpreters.xla.DeviceArray: Triplet Loss.}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} START CODE HERE (Replace instances of \PYZsq{}None\PYZsq{} with your code) \PYZsh{}\PYZsh{}\PYZsh{}}
             
             \PY{c+c1}{\PYZsh{} use fastnp to take the dot product of the two batches (don\PYZsq{}t forget to transpose the second argument)}
             \PY{n}{scores} \PY{o}{=} \PY{n}{fastnp}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{v1}\PY{p}{,} \PY{n}{v2}\PY{o}{.}\PY{n}{T}\PY{p}{)}  \PY{c+c1}{\PYZsh{} pairwise cosine sim}
             \PY{c+c1}{\PYZsh{} calculate new batch size}
             \PY{n}{batch\PYZus{}size} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{scores}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} use fastnp to grab all postive `diagonal` entries in `scores`}
             \PY{n}{positive} \PY{o}{=} \PY{n}{fastnp}\PY{o}{.}\PY{n}{diagonal}\PY{p}{(}\PY{n}{scores}\PY{p}{)}  \PY{c+c1}{\PYZsh{} the positive ones (duplicates)}
             \PY{c+c1}{\PYZsh{} multiply `fastnp.eye(batch\PYZus{}size)` with 2.0 and subtract it out of `scores`}
             \PY{n}{negative\PYZus{}without\PYZus{}positive} \PY{o}{=} \PY{n}{scores} \PY{o}{\PYZhy{}} \PY{l+m+mf}{2.0} \PY{o}{*} \PY{n}{fastnp}\PY{o}{.}\PY{n}{eye}\PY{p}{(}\PY{n}{batch\PYZus{}size}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} take the row by row `max` of `negative\PYZus{}without\PYZus{}positive`. }
             \PY{c+c1}{\PYZsh{} Hint: negative\PYZus{}without\PYZus{}positive.max(axis = [?])  }
             \PY{n}{closest\PYZus{}negative} \PY{o}{=} \PY{n}{negative\PYZus{}without\PYZus{}positive}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} subtract `fastnp.eye(batch\PYZus{}size)` out of 1.0 and do element\PYZhy{}wise multiplication with `scores`}
             \PY{n}{negative\PYZus{}zero\PYZus{}on\PYZus{}duplicate} \PY{o}{=} \PY{n}{scores} \PY{o}{*} \PY{p}{(}\PY{l+m+mf}{1.0} \PY{o}{\PYZhy{}} \PY{n}{fastnp}\PY{o}{.}\PY{n}{eye}\PY{p}{(}\PY{n}{batch\PYZus{}size}\PY{p}{)}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} use `fastnp.sum` on `negative\PYZus{}zero\PYZus{}on\PYZus{}duplicate` for `axis=1` and divide it by `(batch\PYZus{}size \PYZhy{} 1)` }
             \PY{n}{mean\PYZus{}negative} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{negative\PYZus{}zero\PYZus{}on\PYZus{}duplicate}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)} \PY{o}{/} \PY{p}{(}\PY{n}{batch\PYZus{}size}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} compute `fastnp.maximum` among 0.0 and `A`}
             \PY{c+c1}{\PYZsh{} A = subtract `positive` from `margin` and add `closest\PYZus{}negative` }
             \PY{n}{triplet\PYZus{}loss1} \PY{o}{=} \PY{n}{fastnp}\PY{o}{.}\PY{n}{maximum}\PY{p}{(}\PY{l+m+mf}{0.0}\PY{p}{,} \PY{n}{margin} \PY{o}{\PYZhy{}} \PY{n}{positive} \PY{o}{+} \PY{n}{closest\PYZus{}negative}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} compute `fastnp.maximum` among 0.0 and `B`}
             \PY{c+c1}{\PYZsh{} B = subtract `positive` from `margin` and add `mean\PYZus{}negative`}
             \PY{n}{triplet\PYZus{}loss2} \PY{o}{=} \PY{n}{fastnp}\PY{o}{.}\PY{n}{maximum}\PY{p}{(}\PY{l+m+mf}{0.0}\PY{p}{,} \PY{n}{margin} \PY{o}{\PYZhy{}} \PY{n}{positive} \PY{o}{+} \PY{n}{mean\PYZus{}negative}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} add the two losses together and take the `fastnp.mean` of it}
             \PY{n}{triplet\PYZus{}loss} \PY{o}{=} \PY{n}{fastnp}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{triplet\PYZus{}loss1} \PY{o}{+} \PY{n}{triplet\PYZus{}loss2}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} END CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}
             
             \PY{k}{return} \PY{n}{triplet\PYZus{}loss}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}21}]:} \PY{n}{v1} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{p}{[}\PY{l+m+mf}{0.26726124}\PY{p}{,} \PY{l+m+mf}{0.53452248}\PY{p}{,} \PY{l+m+mf}{0.80178373}\PY{p}{]}\PY{p}{,}\PY{p}{[}\PY{l+m+mf}{0.5178918} \PY{p}{,} \PY{l+m+mf}{0.57543534}\PY{p}{,} \PY{l+m+mf}{0.63297887}\PY{p}{]}\PY{p}{]}\PY{p}{)}
         \PY{n}{v2} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{p}{[} \PY{l+m+mf}{0.26726124}\PY{p}{,}  \PY{l+m+mf}{0.53452248}\PY{p}{,}  \PY{l+m+mf}{0.80178373}\PY{p}{]}\PY{p}{,}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mf}{0.5178918} \PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mf}{0.57543534}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mf}{0.63297887}\PY{p}{]}\PY{p}{]}\PY{p}{)}
         \PY{n}{TripletLossFn}\PY{p}{(}\PY{n}{v2}\PY{p}{,}\PY{n}{v1}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Triplet Loss:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{TripletLossFn}\PY{p}{(}\PY{n}{v2}\PY{p}{,}\PY{n}{v1}\PY{p}{)}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Triplet Loss: 0.5

    \end{Verbatim}

    \textbf{Expected Output:}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Triplet Loss: }\FloatTok{0.5}
\end{Highlighting}
\end{Shaded}

    To make a layer out of a function with no trainable variables, use
\texttt{tl.Fn}.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}22}]:} \PY{k+kn}{from} \PY{n+nn}{functools} \PY{k}{import} \PY{n}{partial}
         \PY{k}{def} \PY{n+nf}{TripletLoss}\PY{p}{(}\PY{n}{margin}\PY{o}{=}\PY{l+m+mf}{0.25}\PY{p}{)}\PY{p}{:}
             \PY{n}{triplet\PYZus{}loss\PYZus{}fn} \PY{o}{=} \PY{n}{partial}\PY{p}{(}\PY{n}{TripletLossFn}\PY{p}{,} \PY{n}{margin}\PY{o}{=}\PY{n}{margin}\PY{p}{)}
             \PY{k}{return} \PY{n}{tl}\PY{o}{.}\PY{n}{Fn}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{TripletLoss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{triplet\PYZus{}loss\PYZus{}fn}\PY{p}{)}
\end{Verbatim}

    \hypertarget{part-3-training}{%
\section{Part 3: Training}\label{part-3-training}}

Now you are going to train your model. As usual, you have to define the
cost function and the optimizer. You also have to feed in the built
model. Before, going into the training, we will use a special data set
up. We will define the inputs using the data generator we built above.
The lambda function acts as a seed to remember the last batch that was
given. Run the cell below to get the question pairs inputs.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}23}]:} \PY{n}{batch\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{256}
         \PY{n}{train\PYZus{}generator} \PY{o}{=} \PY{n}{data\PYZus{}generator}\PY{p}{(}\PY{n}{train\PYZus{}Q1}\PY{p}{,} \PY{n}{train\PYZus{}Q2}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{p}{,} \PY{n}{vocab}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZlt{}PAD\PYZgt{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
         \PY{n}{val\PYZus{}generator} \PY{o}{=} \PY{n}{data\PYZus{}generator}\PY{p}{(}\PY{n}{val\PYZus{}Q1}\PY{p}{,} \PY{n}{val\PYZus{}Q2}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{p}{,} \PY{n}{vocab}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZlt{}PAD\PYZgt{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train\PYZus{}Q1.shape }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{train\PYZus{}Q1}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{val\PYZus{}Q1.shape   }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{val\PYZus{}Q1}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
train\_Q1.shape  (89188,)
val\_Q1.shape    (22298,)

    \end{Verbatim}

    \hypertarget{training-the-model}{%
\subsubsection{3.1 Training the model}\label{training-the-model}}

You will now write a function that takes in your model and trains it. To
train your model you have to decide how many times you want to iterate
over the entire data set; each iteration is defined as an
\texttt{epoch}. For each epoch, you have to go over all the data, using
your training iterator.

\#\#\# Exercise 04

\textbf{Instructions:} Implement the \texttt{train\_model} below to
train the neural network above. Here is a list of things you should do,
as already shown in lecture 7:

\begin{itemize}
\tightlist
\item
  Create \texttt{TrainTask} and \texttt{EvalTask}
\item
  Create the training loop \texttt{trax.supervised.training.Loop}
\item
  Pass in the following depending on the context (train\_task or
  eval\_task):

  \begin{itemize}
  \tightlist
  \item
    \texttt{labeled\_data=generator}
  \item
    \texttt{metrics={[}TripletLoss(){]}},
  \item
    \texttt{loss\_layer=TripletLoss()}
  \item
    \texttt{optimizer=trax.optimizers.Adam} with learning rate of 0.01
  \item
    \texttt{lr\_schedule=lr\_schedule},
  \item
    \texttt{output\_dir=output\_dir}
  \end{itemize}
\end{itemize}

You will be using your triplet loss function with Adam optimizer. Please
read the
\href{https://trax-ml.readthedocs.io/en/latest/trax.optimizers.html?highlight=adam\#trax.optimizers.adam.Adam}{trax}
documentation to get a full understanding.

This function should return a \texttt{training.Loop} object. To read
more about this check the
\href{https://trax-ml.readthedocs.io/en/latest/trax.supervised.html?highlight=loop\#trax.supervised.training.Loop}{docs}.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}24}]:} \PY{n}{lr\PYZus{}schedule} \PY{o}{=} \PY{n}{trax}\PY{o}{.}\PY{n}{lr}\PY{o}{.}\PY{n}{warmup\PYZus{}and\PYZus{}rsqrt\PYZus{}decay}\PY{p}{(}\PY{l+m+mi}{400}\PY{p}{,} \PY{l+m+mf}{0.01}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} UNQ\PYZus{}C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)}
         \PY{c+c1}{\PYZsh{} GRADED FUNCTION: train\PYZus{}model}
         \PY{k}{def} \PY{n+nf}{train\PYZus{}model}\PY{p}{(}\PY{n}{Siamese}\PY{p}{,} \PY{n}{TripletLoss}\PY{p}{,} \PY{n}{lr\PYZus{}schedule}\PY{p}{,} \PY{n}{train\PYZus{}generator}\PY{o}{=}\PY{n}{train\PYZus{}generator}\PY{p}{,} \PY{n}{val\PYZus{}generator}\PY{o}{=}\PY{n}{val\PYZus{}generator}\PY{p}{,} \PY{n}{output\PYZus{}dir}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{model/}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Training the Siamese Model}
         
         \PY{l+s+sd}{    Args:}
         \PY{l+s+sd}{        Siamese (function): Function that returns the Siamese model.}
         \PY{l+s+sd}{        TripletLoss (function): Function that defines the TripletLoss loss function.}
         \PY{l+s+sd}{        lr\PYZus{}schedule (function): Trax multifactor schedule function.}
         \PY{l+s+sd}{        train\PYZus{}generator (generator, optional): Training generator. Defaults to train\PYZus{}generator.}
         \PY{l+s+sd}{        val\PYZus{}generator (generator, optional): Validation generator. Defaults to val\PYZus{}generator.}
         \PY{l+s+sd}{        output\PYZus{}dir (str, optional): Path to save model to. Defaults to \PYZsq{}model/\PYZsq{}.}
         
         \PY{l+s+sd}{    Returns:}
         \PY{l+s+sd}{        trax.supervised.training.Loop: Training loop for the model.}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{n}{output\PYZus{}dir} \PY{o}{=} \PY{n}{os}\PY{o}{.}\PY{n}{path}\PY{o}{.}\PY{n}{expanduser}\PY{p}{(}\PY{n}{output\PYZus{}dir}\PY{p}{)}
         
             \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} START CODE HERE (Replace instances of \PYZsq{}None\PYZsq{} with your code) \PYZsh{}\PYZsh{}\PYZsh{}}
         
             \PY{n}{train\PYZus{}task} \PY{o}{=} \PY{n}{training}\PY{o}{.}\PY{n}{TrainTask}\PY{p}{(}
                 \PY{n}{labeled\PYZus{}data}\PY{o}{=}\PY{n}{train\PYZus{}generator}\PY{p}{,}            \PY{c+c1}{\PYZsh{} Use generator (train)}
                 \PY{n}{loss\PYZus{}layer}\PY{o}{=}\PY{n}{TripletLoss}\PY{p}{(}\PY{p}{)}\PY{p}{,}                \PY{c+c1}{\PYZsh{} Use triplet loss. Don\PYZsq{}t forget to instantiate this object}
                 \PY{n}{optimizer}\PY{o}{=}\PY{n}{trax}\PY{o}{.}\PY{n}{optimizers}\PY{o}{.}\PY{n}{Adam}\PY{p}{(}\PY{l+m+mf}{0.01}\PY{p}{)}\PY{p}{,}    \PY{c+c1}{\PYZsh{} Don\PYZsq{}t forget to add the learning rate parameter}
                 \PY{n}{lr\PYZus{}schedule}\PY{o}{=}\PY{n}{lr\PYZus{}schedule}\PY{p}{,}                  \PY{c+c1}{\PYZsh{} Use Trax multifactor schedule function}
             \PY{p}{)}
         
             \PY{n}{eval\PYZus{}task} \PY{o}{=} \PY{n}{training}\PY{o}{.}\PY{n}{EvalTask}\PY{p}{(}
                 \PY{n}{labeled\PYZus{}data}\PY{o}{=}\PY{n}{val\PYZus{}generator}\PY{p}{,}       \PY{c+c1}{\PYZsh{} Use generator (val)}
                 \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{n}{TripletLoss}\PY{p}{(}\PY{p}{)}\PY{p}{]}\PY{p}{,}          \PY{c+c1}{\PYZsh{} Use triplet loss. Don\PYZsq{}t forget to instantiate this object}
             \PY{p}{)}
             
             \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} END CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}
         
             \PY{n}{training\PYZus{}loop} \PY{o}{=} \PY{n}{training}\PY{o}{.}\PY{n}{Loop}\PY{p}{(}\PY{n}{Siamese}\PY{p}{(}\PY{p}{)}\PY{p}{,}
                                           \PY{n}{train\PYZus{}task}\PY{p}{,}
                                           \PY{n}{eval\PYZus{}task}\PY{o}{=}\PY{n}{eval\PYZus{}task}\PY{p}{,}
                                           \PY{n}{output\PYZus{}dir}\PY{o}{=}\PY{n}{output\PYZus{}dir}\PY{p}{)}
         
             \PY{k}{return} \PY{n}{training\PYZus{}loop}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}25}]:} \PY{n}{train\PYZus{}steps} \PY{o}{=} \PY{l+m+mi}{5}
         \PY{n}{training\PYZus{}loop} \PY{o}{=} \PY{n}{train\PYZus{}model}\PY{p}{(}\PY{n}{Siamese}\PY{p}{,} \PY{n}{TripletLoss}\PY{p}{,} \PY{n}{lr\PYZus{}schedule}\PY{p}{)}
         \PY{n}{training\PYZus{}loop}\PY{o}{.}\PY{n}{run}\PY{p}{(}\PY{n}{train\PYZus{}steps}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Step      1: train TripletLoss |  0.49954823
Step      1: eval  TripletLoss |  0.49950948

    \end{Verbatim}

    The model was only trained for 5 steps due to the constraints of this
environment. For the rest of the assignment you will be using a
pretrained model but now you should understand how the training can be
done using Trax.

    \hypertarget{part-4-evaluation}{%
\section{Part 4: Evaluation}\label{part-4-evaluation}}

\hypertarget{evaluating-your-siamese-network}{%
\subsubsection{4.1 Evaluating your siamese
network}\label{evaluating-your-siamese-network}}

In this section you will learn how to evaluate a Siamese network. You
will first start by loading a pretrained model and then you will use it
to predict.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}26}]:} \PY{c+c1}{\PYZsh{} Loading in the saved model}
         \PY{n}{model} \PY{o}{=} \PY{n}{Siamese}\PY{p}{(}\PY{p}{)}
         \PY{n}{model}\PY{o}{.}\PY{n}{init\PYZus{}from\PYZus{}file}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{model.pkl.gz}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

    \#\#\# 4.2 Classify To determine the accuracy of the model, we will
utilize the test set that was configured earlier. While in training we
used only positive examples, the test data, Q1\_test, Q2\_test and
y\_test, is setup as pairs of questions, some of which are duplicates
some are not. This routine will run all the test question pairs through
the model, compute the cosine simlarity of each pair, threshold it and
compare the result to y\_test - the correct response from the data set.
The results are accumulated to produce an accuracy.

\#\#\# Exercise 05

\textbf{Instructions}\\
- Loop through the incoming data in batch\_size chunks - Use the data
generator to load q1, q2 a batch at a time. \textbf{Don't forget to set
shuffle=False!} - copy a batch\_size chunk of y into y\_test - compute
v1, v2 using the model - for each element of the batch - compute the cos
similarity of each pair of entries, v1{[}j{]},v2{[}j{]} - determine if d
\textgreater{} threshold - increment accuracy if that result matches the
expected results (y\_test{[}j{]}) - compute the final accuracy and
return

Due to some limitations of this environment, running classify multiple
times may result in the kernel failing. If that happens \emph{Restart
Kernal \& clear output} and then run from the top. During development,
consider using a smaller set of data to reduce the number of calls to
model().

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}27}]:} \PY{c+c1}{\PYZsh{} UNQ\PYZus{}C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)}
         \PY{c+c1}{\PYZsh{} GRADED FUNCTION: classify}
         \PY{k}{def} \PY{n+nf}{classify}\PY{p}{(}\PY{n}{test\PYZus{}Q1}\PY{p}{,} \PY{n}{test\PYZus{}Q2}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{threshold}\PY{p}{,} \PY{n}{model}\PY{p}{,} \PY{n}{vocab}\PY{p}{,} \PY{n}{data\PYZus{}generator}\PY{o}{=}\PY{n}{data\PYZus{}generator}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{64}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Function to test the accuracy of the model.}
         
         \PY{l+s+sd}{    Args:}
         \PY{l+s+sd}{        test\PYZus{}Q1 (numpy.ndarray): Array of Q1 questions.}
         \PY{l+s+sd}{        test\PYZus{}Q2 (numpy.ndarray): Array of Q2 questions.}
         \PY{l+s+sd}{        y (numpy.ndarray): Array of actual target.}
         \PY{l+s+sd}{        threshold (float): Desired threshold.}
         \PY{l+s+sd}{        model (trax.layers.combinators.Parallel): The Siamese model.}
         \PY{l+s+sd}{        vocab (collections.defaultdict): The vocabulary used.}
         \PY{l+s+sd}{        data\PYZus{}generator (function): Data generator function. Defaults to data\PYZus{}generator.}
         \PY{l+s+sd}{        batch\PYZus{}size (int, optional): Size of the batches. Defaults to 64.}
         
         \PY{l+s+sd}{    Returns:}
         \PY{l+s+sd}{        float: Accuracy of the model.}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{n}{accuracy} \PY{o}{=} \PY{l+m+mi}{0}
             \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} START CODE HERE (Replace instances of \PYZsq{}None\PYZsq{} with your code) \PYZsh{}\PYZsh{}\PYZsh{}}
             \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{test\PYZus{}Q1}\PY{p}{)}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{p}{)}\PY{p}{:}
                 \PY{c+c1}{\PYZsh{} Call the data generator (built in Ex 01) with shuffle=False using next()}
                 \PY{c+c1}{\PYZsh{} use batch size chuncks of questions as Q1 \PYZam{} Q2 arguments of the data generator. e.g x[i:i + batch\PYZus{}size]}
                 \PY{c+c1}{\PYZsh{} Hint: use `vocab[\PYZsq{}\PYZlt{}PAD\PYZgt{}\PYZsq{}]` for the `pad` argument of the data generator}
                 \PY{n}{q1}\PY{p}{,} \PY{n}{q2} \PY{o}{=} \PY{n+nb}{next}\PY{p}{(}\PY{n}{data\PYZus{}generator}\PY{p}{(}\PY{n}{test\PYZus{}Q1}\PY{p}{[}\PY{n}{i}\PY{p}{:}\PY{n}{i} \PY{o}{+} \PY{n}{batch\PYZus{}size}\PY{p}{]}\PY{p}{,} \PY{n}{test\PYZus{}Q2}\PY{p}{[}\PY{n}{i}\PY{p}{:}\PY{n}{i} \PY{o}{+} \PY{n}{batch\PYZus{}size}\PY{p}{]}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{p}{,} \PY{n}{vocab}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZlt{}PAD\PYZgt{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{shuffle}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{p}{)}
                 \PY{c+c1}{\PYZsh{} use batch size chuncks of actual output targets (same syntax as example above)}
                 \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{y}\PY{p}{[}\PY{n}{i}\PY{p}{:}\PY{n}{i} \PY{o}{+} \PY{n}{batch\PYZus{}size}\PY{p}{]}
                 \PY{c+c1}{\PYZsh{} Call the model}
                 \PY{n}{v1}\PY{p}{,} \PY{n}{v2} \PY{o}{=} \PY{n}{model}\PY{p}{(}\PY{p}{(}\PY{n}{q1}\PY{p}{,} \PY{n}{q2}\PY{p}{)}\PY{p}{)}
         
                 \PY{k}{for} \PY{n}{j} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{batch\PYZus{}size}\PY{p}{)}\PY{p}{:}
                     \PY{c+c1}{\PYZsh{} take dot product to compute cos similarity of each pair of entries, v1[j], v2[j]}
                     \PY{c+c1}{\PYZsh{} don\PYZsq{}t forget to transpose the second argument}
                     \PY{n}{d} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{v1}\PY{p}{[}\PY{n}{j}\PY{p}{]}\PY{p}{,} \PY{n}{v2}\PY{p}{[}\PY{n}{j}\PY{p}{]}\PY{o}{.}\PY{n}{T}\PY{p}{)}
                     \PY{c+c1}{\PYZsh{} is d greater than the threshold?}
                     \PY{n}{res} \PY{o}{=} \PY{n}{d} \PY{o}{\PYZgt{}} \PY{n}{threshold}
                     \PY{c+c1}{\PYZsh{} increment accurancy if y\PYZus{}test is equal `res`}
                     \PY{n}{accuracy} \PY{o}{+}\PY{o}{=} \PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{[}\PY{n}{j}\PY{p}{]} \PY{o}{==} \PY{n}{res}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} compute accuracy using accuracy and total length of test questions}
             \PY{n}{accuracy} \PY{o}{=} \PY{n}{accuracy} \PY{o}{/} \PY{n+nb}{len}\PY{p}{(}\PY{n}{test\PYZus{}Q1}\PY{p}{)}
             \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} END CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}
             
             \PY{k}{return} \PY{n}{accuracy}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}28}]:} \PY{c+c1}{\PYZsh{} this takes around 1 minute}
         \PY{n}{accuracy} \PY{o}{=} \PY{n}{classify}\PY{p}{(}\PY{n}{Q1\PYZus{}test}\PY{p}{,}\PY{n}{Q2\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{,} \PY{l+m+mf}{0.7}\PY{p}{,} \PY{n}{model}\PY{p}{,} \PY{n}{vocab}\PY{p}{,} \PY{n}{batch\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{512}\PY{p}{)} 
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Accuracy}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{accuracy}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Accuracy 0.69091796875

    \end{Verbatim}

    \textbf{Expected Result}\\
Accuracy \textasciitilde{}0.69

    \hypertarget{part-5-testing-with-your-own-questions}{%
\section{Part 5: Testing with your own
questions}\label{part-5-testing-with-your-own-questions}}

In this section you will test the model with your own questions. You
will write a function \texttt{predict} which takes two questions as
input and returns \(1\) or \(0\) depending on whether the question pair
is a duplicate or not.

But first, we build a reverse vocabulary that allows to map encoded
questions back to words:

    Write a function \texttt{predict}that takes in two questions, the model,
and the vocabulary and returns whether the questions are duplicates
(\(1\)) or not duplicates (\(0\)) given a similarity threshold.

\#\#\# Exercise 06

\textbf{Instructions:} - Tokenize your question using
\texttt{nltk.word\_tokenize} - Create Q1,Q2 by encoding your questions
as a list of numbers using vocab - pad Q1,Q2 with
next(data\_generator({[}Q1{]}, {[}Q2{]},1,vocab{[}`'{]})) - use model()
to create v1, v2 - compute the cosine similarity (dot product) of v1, v2
- compute res by comparing d to the threshold

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}29}]:} \PY{c+c1}{\PYZsh{} UNQ\PYZus{}C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)}
         \PY{c+c1}{\PYZsh{} GRADED FUNCTION: predict}
         \PY{k}{def} \PY{n+nf}{predict}\PY{p}{(}\PY{n}{question1}\PY{p}{,} \PY{n}{question2}\PY{p}{,} \PY{n}{threshold}\PY{p}{,} \PY{n}{model}\PY{p}{,} \PY{n}{vocab}\PY{p}{,} \PY{n}{data\PYZus{}generator}\PY{o}{=}\PY{n}{data\PYZus{}generator}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Function for predicting if two questions are duplicates.}
         
         \PY{l+s+sd}{    Args:}
         \PY{l+s+sd}{        question1 (str): First question.}
         \PY{l+s+sd}{        question2 (str): Second question.}
         \PY{l+s+sd}{        threshold (float): Desired threshold.}
         \PY{l+s+sd}{        model (trax.layers.combinators.Parallel): The Siamese model.}
         \PY{l+s+sd}{        vocab (collections.defaultdict): The vocabulary used.}
         \PY{l+s+sd}{        data\PYZus{}generator (function): Data generator function. Defaults to data\PYZus{}generator.}
         \PY{l+s+sd}{        verbose (bool, optional): If the results should be printed out. Defaults to False.}
         
         \PY{l+s+sd}{    Returns:}
         \PY{l+s+sd}{        bool: True if the questions are duplicates, False otherwise.}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} START CODE HERE (Replace instances of \PYZsq{}None\PYZsq{} with your code) \PYZsh{}\PYZsh{}\PYZsh{}}
             \PY{c+c1}{\PYZsh{} use `nltk` word tokenize function to tokenize}
             \PY{n}{q1} \PY{o}{=} \PY{n}{nltk}\PY{o}{.}\PY{n}{word\PYZus{}tokenize}\PY{p}{(}\PY{n}{question1}\PY{p}{)}  \PY{c+c1}{\PYZsh{} tokenize}
             \PY{n}{q2} \PY{o}{=} \PY{n}{nltk}\PY{o}{.}\PY{n}{word\PYZus{}tokenize}\PY{p}{(}\PY{n}{question2}\PY{p}{)}  \PY{c+c1}{\PYZsh{} tokenize}
             \PY{n}{Q1}\PY{p}{,} \PY{n}{Q2} \PY{o}{=} \PY{p}{[}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{p}{]}
             \PY{k}{for} \PY{n}{word} \PY{o+ow}{in} \PY{n}{q1}\PY{p}{:}  \PY{c+c1}{\PYZsh{} encode q1}
                 \PY{c+c1}{\PYZsh{} increment by checking the \PYZsq{}word\PYZsq{} index in `vocab`}
                 \PY{n}{Q1} \PY{o}{+}\PY{o}{=} \PY{p}{[}\PY{n}{vocab}\PY{p}{[}\PY{n}{word}\PY{p}{]}\PY{p}{]}
             \PY{k}{for} \PY{n}{word} \PY{o+ow}{in} \PY{n}{q2}\PY{p}{:}  \PY{c+c1}{\PYZsh{} encode q2}
                 \PY{c+c1}{\PYZsh{} increment by checking the \PYZsq{}word\PYZsq{} index in `vocab`}
                 \PY{n}{Q2} \PY{o}{+}\PY{o}{=} \PY{p}{[}\PY{n}{vocab}\PY{p}{[}\PY{n}{word}\PY{p}{]}\PY{p}{]}
                 
             \PY{c+c1}{\PYZsh{} Call the data generator (built in Ex 01) using next()}
             \PY{c+c1}{\PYZsh{} pass [Q1] \PYZam{} [Q2] as Q1 \PYZam{} Q2 arguments of the data generator. Set batch size as 1}
             \PY{c+c1}{\PYZsh{} Hint: use `vocab[\PYZsq{}\PYZlt{}PAD\PYZgt{}\PYZsq{}]` for the `pad` argument of the data generator}
             \PY{n}{Q1}\PY{p}{,} \PY{n}{Q2} \PY{o}{=} \PY{n+nb}{next}\PY{p}{(}\PY{n}{data\PYZus{}generator}\PY{p}{(}\PY{p}{[}\PY{n}{Q1}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{n}{Q2}\PY{p}{]}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{vocab}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZlt{}PAD\PYZgt{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} Call the model}
             \PY{n}{v1}\PY{p}{,} \PY{n}{v2} \PY{o}{=} \PY{n}{model}\PY{p}{(}\PY{p}{(}\PY{n}{Q1}\PY{p}{,} \PY{n}{Q2}\PY{p}{)}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} take dot product to compute cos similarity of each pair of entries, v1, v2}
             \PY{c+c1}{\PYZsh{} don\PYZsq{}t forget to transpose the second argument}
             \PY{n}{d} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{v1}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{v2}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{T}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} is d greater than the threshold?}
             \PY{n}{res} \PY{o}{=} \PY{n}{d} \PY{o}{\PYZgt{}} \PY{n}{threshold}
             
             \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} END CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}
             
             \PY{k}{if}\PY{p}{(}\PY{n}{verbose}\PY{p}{)}\PY{p}{:}
                 \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Q1  = }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{Q1}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{Q2  = }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{Q2}\PY{p}{)}
                 \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{d   = }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{d}\PY{p}{)}
                 \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{res = }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{res}\PY{p}{)}
         
             \PY{k}{return} \PY{n}{res}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}30}]:} \PY{c+c1}{\PYZsh{} Feel free to try with your own questions}
         \PY{n}{question1} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{When will I see you?}\PY{l+s+s2}{\PYZdq{}}
         \PY{n}{question2} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{When can I see you again?}\PY{l+s+s2}{\PYZdq{}}
         \PY{c+c1}{\PYZsh{} 1 means it is duplicated, 0 otherwise}
         \PY{n}{predict}\PY{p}{(}\PY{n}{question1} \PY{p}{,} \PY{n}{question2}\PY{p}{,} \PY{l+m+mf}{0.7}\PY{p}{,} \PY{n}{model}\PY{p}{,} \PY{n}{vocab}\PY{p}{,} \PY{n}{verbose} \PY{o}{=} \PY{k+kc}{True}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Q1  =  [[585  76   4  46  53  21   1   1]] 
Q2  =  [[ 585   33    4   46   53 7280   21    1]]
d   =  0.88113236
res =  True

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}30}]:} True
\end{Verbatim}
            
    \hypertarget{expected-output}{%
\subparagraph{Expected Output}\label{expected-output}}

If input is:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{question1 = }\StringTok{"When will I see you?"}
\NormalTok{question2 = }\StringTok{"When can I see you again?"}
\end{Highlighting}
\end{Shaded}

Output is (d may vary a bit):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Q1  =  [[}\DecValTok{585}\AttributeTok{  }\DecValTok{76}\AttributeTok{   }\DecValTok{4}\AttributeTok{  }\DecValTok{46}\AttributeTok{  }\DecValTok{53}\AttributeTok{  }\DecValTok{21}\AttributeTok{   }\DecValTok{1}\AttributeTok{   }\DecValTok{1}\NormalTok{]] }
\NormalTok{Q2  =  [[}\AttributeTok{ }\DecValTok{585}\AttributeTok{   }\DecValTok{33}\AttributeTok{    }\DecValTok{4}\AttributeTok{   }\DecValTok{46}\AttributeTok{   }\DecValTok{53}\AttributeTok{ }\DecValTok{7280}\AttributeTok{   }\DecValTok{21}\AttributeTok{    }\DecValTok{1}\NormalTok{]]}
\NormalTok{d   =  }\FloatTok{0.88113236}
\NormalTok{res =  True}
\NormalTok{True}
\end{Highlighting}
\end{Shaded}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}31}]:} \PY{c+c1}{\PYZsh{} Feel free to try with your own questions}
         \PY{n}{question1} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Do they enjoy eating the dessert?}\PY{l+s+s2}{\PYZdq{}}
         \PY{n}{question2} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Do they like hiking in the desert?}\PY{l+s+s2}{\PYZdq{}}
         \PY{c+c1}{\PYZsh{} 1 means it is duplicated, 0 otherwise}
         \PY{n}{predict}\PY{p}{(}\PY{n}{question1} \PY{p}{,} \PY{n}{question2}\PY{p}{,} \PY{l+m+mf}{0.7}\PY{p}{,} \PY{n}{model}\PY{p}{,} \PY{n}{vocab}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Q1  =  [[  443  1145  3159  1169    78 29017    21     1]] 
Q2  =  [[  443  1145    60 15302    28    78  7431    21]]
d   =  0.477536
res =  False

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}31}]:} False
\end{Verbatim}
            
    \hypertarget{expected-output}{%
\subparagraph{Expected output}\label{expected-output}}

If input is:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{question1 = }\StringTok{"Do they enjoy eating the dessert?"}
\NormalTok{question2 = }\StringTok{"Do they like hiking in the desert?"}
\end{Highlighting}
\end{Shaded}

Output (d may vary a bit):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Q1  =  [[}\AttributeTok{  }\DecValTok{443}\AttributeTok{  }\DecValTok{1145}\AttributeTok{  }\DecValTok{3159}\AttributeTok{  }\DecValTok{1169}\AttributeTok{    }\DecValTok{78}\AttributeTok{ }\DecValTok{29017}\AttributeTok{    }\DecValTok{21}\AttributeTok{     }\DecValTok{1}\NormalTok{]] }
\NormalTok{Q2  =  [[}\AttributeTok{  }\DecValTok{443}\AttributeTok{  }\DecValTok{1145}\AttributeTok{    }\DecValTok{60}\AttributeTok{ }\DecValTok{15302}\AttributeTok{    }\DecValTok{28}\AttributeTok{    }\DecValTok{78}\AttributeTok{  }\DecValTok{7431}\AttributeTok{    }\DecValTok{21}\NormalTok{]]}
\NormalTok{d   =  }\FloatTok{0.477536}
\NormalTok{res =  False}
\NormalTok{False}
\end{Highlighting}
\end{Shaded}

    You can see that the Siamese network is capable of catching complicated
structures. Concretely it can identify question duplicates although the
questions do not have many words in common.

    \hypertarget{on-siamese-networks}{%
\subsubsection{\texorpdfstring{{ On Siamese networks
}}{ On Siamese networks }}\label{on-siamese-networks}}

Siamese networks are important and useful. Many times there are several
questions that are already asked in quora, or other platforms and you
can use Siamese networks to avoid question duplicates.

Congratulations, you have now built a powerful system that can recognize
question duplicates. In the next course we will use transformers for
machine translation, summarization, question answering, and chatbots.


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
