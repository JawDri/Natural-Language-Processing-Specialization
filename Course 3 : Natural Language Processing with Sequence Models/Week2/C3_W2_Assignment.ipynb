
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{mathrsfs}
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \title{C3\_W2\_Assignment}
    
    
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \hypertarget{assignment-2-deep-n-grams}{%
\section{Assignment 2: Deep N-grams}\label{assignment-2-deep-n-grams}}

Welcome to the second assignment of course 3. In this assignment you
will explore Recurrent Neural Networks \texttt{RNN}. - You will be using
the fundamentals of google's \href{https://github.com/google/trax}{trax}
package to implement any kind of deeplearning model.

By completing this assignment, you will learn how to implement models
from scratch: - How to convert a line of text into a tensor - Create an
iterator to feed data to the model - Define a GRU model using
\texttt{trax} - Train the model using \texttt{trax} - Compute the
accuracy of your model using the perplexity - Predict using your own
model

    \hypertarget{outline}{%
\subsection{Outline}\label{outline}}

\begin{itemize}
\tightlist
\item
  Section \ref{0}
\item
  Section \ref{1}

  \begin{itemize}
  \tightlist
  \item
    Section \ref{11}
  \item
    Section \ref{12}

    \begin{itemize}
    \tightlist
    \item
      Section \ref{ex01}
    \end{itemize}
  \item
    Section \ref{13}

    \begin{itemize}
    \tightlist
    \item
      Section \ref{ex02}
    \end{itemize}
  \item
    Section \ref{14}\\
  \end{itemize}
\item
  Section \ref{2}

  \begin{itemize}
  \tightlist
  \item
    Section \ref{ex03}
  \end{itemize}
\item
  Section \ref{3}

  \begin{itemize}
  \tightlist
  \item
    Section \ref{31}

    \begin{itemize}
    \tightlist
    \item
      Section \ref{ex04}
    \end{itemize}
  \end{itemize}
\item
  Section \ref{4}

  \begin{itemize}
  \tightlist
  \item
    Section \ref{41}

    \begin{itemize}
    \tightlist
    \item
      Section \ref{ex05}
    \end{itemize}
  \end{itemize}
\item
  Section \ref{5}\\
\item
  Section \ref{6}
\end{itemize}

    \#\#\# Overview

Your task will be to predict the next set of characters using the
previous characters. - Although this task sounds simple, it is pretty
useful. - You will start by converting a line of text into a tensor -
Then you will create a generator to feed data into the model - You will
train a neural network in order to predict the new set of characters of
defined length. - You will use embeddings for each character and feed
them as inputs to your model. - Many natural language tasks rely on
using embeddings for predictions. - Your model will convert each
character to its embedding, run the embeddings through a Gated Recurrent
Unit \texttt{GRU}, and run it through a linear layer to predict the next
set of characters.

The figure above gives you a summary of what you are about to implement.
- You will get the embeddings; - Stack the embeddings on top of each
other; - Run them through two layers with a relu activation in the
middle; - Finally, you will compute the softmax.

To predict the next character: - Use the softmax output and identify the
word with the highest probability. - The word with the highest
probability is the prediction for the next word.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{k+kn}{import} \PY{n+nn}{os}
        \PY{k+kn}{import} \PY{n+nn}{trax}
        \PY{k+kn}{import} \PY{n+nn}{trax}\PY{n+nn}{.}\PY{n+nn}{fastmath}\PY{n+nn}{.}\PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
        \PY{k+kn}{import} \PY{n+nn}{pickle}
        \PY{k+kn}{import} \PY{n+nn}{numpy}
        \PY{k+kn}{import} \PY{n+nn}{random} \PY{k}{as} \PY{n+nn}{rnd}
        \PY{k+kn}{from} \PY{n+nn}{trax} \PY{k}{import} \PY{n}{fastmath}
        \PY{k+kn}{from} \PY{n+nn}{trax} \PY{k}{import} \PY{n}{layers} \PY{k}{as} \PY{n}{tl}
        
        \PY{c+c1}{\PYZsh{} set random seed}
        \PY{n}{trax}\PY{o}{.}\PY{n}{supervised}\PY{o}{.}\PY{n}{trainer\PYZus{}lib}\PY{o}{.}\PY{n}{init\PYZus{}random\PYZus{}number\PYZus{}generators}\PY{p}{(}\PY{l+m+mi}{32}\PY{p}{)}
        \PY{n}{rnd}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{32}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
INFO:tensorflow:tokens\_length=568 inputs\_length=512 targets\_length=114 noise\_density=0.15 mean\_noise\_span\_length=3.0 

    \end{Verbatim}

    \# Part 1: Importing the Data

\#\#\# 1.1 Loading in the data

Now import the dataset and do some processing. - The dataset has one
sentence per line. - You will be doing character generation, so you have
to process each sentence by converting each \textbf{character} (and not
word) to a number. - You will use the \texttt{ord} function to convert a
unique character to a unique integer ID. - Store each line in a list. -
Create a data generator that takes in the \texttt{batch\_size} and the
\texttt{max\_length}. - The \texttt{max\_length} corresponds to the
maximum length of the sentence.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{n}{dirname} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{data/}\PY{l+s+s1}{\PYZsq{}}
        \PY{n}{lines} \PY{o}{=} \PY{p}{[}\PY{p}{]} \PY{c+c1}{\PYZsh{} storing all the lines in a variable. }
        \PY{k}{for} \PY{n}{filename} \PY{o+ow}{in} \PY{n}{os}\PY{o}{.}\PY{n}{listdir}\PY{p}{(}\PY{n}{dirname}\PY{p}{)}\PY{p}{:}
            \PY{k}{with} \PY{n+nb}{open}\PY{p}{(}\PY{n}{os}\PY{o}{.}\PY{n}{path}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n}{dirname}\PY{p}{,} \PY{n}{filename}\PY{p}{)}\PY{p}{)} \PY{k}{as} \PY{n}{files}\PY{p}{:}
                \PY{k}{for} \PY{n}{line} \PY{o+ow}{in} \PY{n}{files}\PY{p}{:}
                    \PY{c+c1}{\PYZsh{} remove leading and trailing whitespace}
                    \PY{n}{pure\PYZus{}line} \PY{o}{=} \PY{n}{line}\PY{o}{.}\PY{n}{strip}\PY{p}{(}\PY{p}{)}
                    
                    \PY{c+c1}{\PYZsh{} if pure\PYZus{}line is not the empty string,}
                    \PY{k}{if} \PY{n}{pure\PYZus{}line}\PY{p}{:}
                        \PY{c+c1}{\PYZsh{} append it to the list}
                        \PY{n}{lines}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{pure\PYZus{}line}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{n}{n\PYZus{}lines} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{lines}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Number of lines: }\PY{l+s+si}{\PYZob{}n\PYZus{}lines\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Sample line at position 0 }\PY{l+s+si}{\PYZob{}lines[0]\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Sample line at position 999 }\PY{l+s+si}{\PYZob{}lines[999]\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Number of lines: 125097
Sample line at position 0 A LOVER'S COMPLAINT
Sample line at position 999 With this night's revels and expire the term

    \end{Verbatim}

    Notice that the letters are both uppercase and lowercase. In order to
reduce the complexity of the task, we will convert all characters to
lowercase. This way, the model only needs to predict the likelihood that
a letter is `a' and not decide between uppercase `A' and lowercase `a'.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{c+c1}{\PYZsh{} go through each line}
        \PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{n}{line} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{lines}\PY{p}{)}\PY{p}{:}
            \PY{c+c1}{\PYZsh{} convert to all lowercase}
            \PY{n}{lines}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{=} \PY{n}{line}\PY{o}{.}\PY{n}{lower}\PY{p}{(}\PY{p}{)}
        
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Number of lines: }\PY{l+s+si}{\PYZob{}n\PYZus{}lines\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Sample line at position 0 }\PY{l+s+si}{\PYZob{}lines[0]\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Sample line at position 999 }\PY{l+s+si}{\PYZob{}lines[999]\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Number of lines: 125097
Sample line at position 0 a lover's complaint
Sample line at position 999 with this night's revels and expire the term

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{n}{eval\PYZus{}lines} \PY{o}{=} \PY{n}{lines}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1000}\PY{p}{:}\PY{p}{]} \PY{c+c1}{\PYZsh{} Create a holdout validation set}
        \PY{n}{lines} \PY{o}{=} \PY{n}{lines}\PY{p}{[}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1000}\PY{p}{]} \PY{c+c1}{\PYZsh{} Leave the rest for training}
        
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Number of lines for training: }\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{len(lines)\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Number of lines for validation: }\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{len(eval\PYZus{}lines)\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Number of lines for training: 124097
Number of lines for validation: 1000

    \end{Verbatim}

    \#\#\# 1.2 Convert a line to tensor

Now that you have your list of lines, you will convert each character in
that list to a number. You can use Python's \texttt{ord} function to do
it.

Given a string representing of one Unicode character, the \texttt{ord}
function return an integer representing the Unicode code point of that
character.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{c+c1}{\PYZsh{} View the unique unicode integer associated with each character}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ord(}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{a}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{): }\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{ord(}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{a}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{)\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ord(}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{b}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{): }\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{ord(}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{b}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{)\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ord(}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{c}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{): }\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{ord(}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{c}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{)\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ord(}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{ }\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{): }\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{ord(}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{ }\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{)\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ord(}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{x}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{): }\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{ord(}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{x}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{)\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ord(}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{y}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{): }\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{ord(}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{y}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{)\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ord(}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{z}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{): }\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{ord(}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{z}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{)\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ord(}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{1}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{): }\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{ord(}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{1}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{)\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ord(}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{2}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{): }\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{ord(}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{2}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{)\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ord(}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{3}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{): }\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{ord(}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{3}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{)\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
ord('a'): 97
ord('b'): 98
ord('c'): 99
ord(' '): 32
ord('x'): 120
ord('y'): 121
ord('z'): 122
ord('1'): 49
ord('2'): 50
ord('3'): 51

    \end{Verbatim}

    \#\#\# Exercise 01

\textbf{Instructions:} Write a function that takes in a single line and
transforms each character into its unicode integer. This returns a list
of integers, which we'll refer to as a tensor. - Use a special integer
to represent the end of the sentence (the end of the line). - This will
be the EOS\_int (end of sentence integer) parameter of the function. -
Include the EOS\_int as the last integer of the - For this exercise, you
will use the number \texttt{1} to represent the end of a sentence.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}9}]:} \PY{c+c1}{\PYZsh{} UNQ\PYZus{}C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)}
        \PY{c+c1}{\PYZsh{} GRADED FUNCTION: line\PYZus{}to\PYZus{}tensor}
        \PY{k}{def} \PY{n+nf}{line\PYZus{}to\PYZus{}tensor}\PY{p}{(}\PY{n}{line}\PY{p}{,} \PY{n}{EOS\PYZus{}int}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Turns a line of text into a tensor}
        
        \PY{l+s+sd}{    Args:}
        \PY{l+s+sd}{        line (str): A single line of text.}
        \PY{l+s+sd}{        EOS\PYZus{}int (int, optional): End\PYZhy{}of\PYZhy{}sentence integer. Defaults to 1.}
        
        \PY{l+s+sd}{    Returns:}
        \PY{l+s+sd}{        list: a list of integers (unicode values) for the characters in the `line`.}
        \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
            
            \PY{c+c1}{\PYZsh{} Initialize the tensor as an empty list}
            \PY{n}{tensor} \PY{o}{=} \PY{p}{[}\PY{p}{]}
            \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} START CODE HERE (Replace instances of \PYZsq{}None\PYZsq{} with your code) \PYZsh{}\PYZsh{}\PYZsh{}}
            \PY{c+c1}{\PYZsh{} for each character:}
            \PY{k}{for} \PY{n}{c} \PY{o+ow}{in} \PY{n}{line}\PY{p}{:}
                
                \PY{c+c1}{\PYZsh{} convert to unicode int}
                \PY{n}{c\PYZus{}int} \PY{o}{=} \PY{n+nb}{ord}\PY{p}{(}\PY{n}{c}\PY{p}{)}
                
                \PY{c+c1}{\PYZsh{} append the unicode integer to the tensor list}
                \PY{n}{tensor}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{c\PYZus{}int}\PY{p}{)}
            
            \PY{c+c1}{\PYZsh{} include the end\PYZhy{}of\PYZhy{}sentence integer}
            \PY{n}{tensor}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{EOS\PYZus{}int}\PY{p}{)}
            \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} END CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}
        
            \PY{k}{return} \PY{n}{tensor}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{c+c1}{\PYZsh{} Testing your output}
         \PY{n}{line\PYZus{}to\PYZus{}tensor}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{abc xyz}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}10}]:} [97, 98, 99, 32, 120, 121, 122, 1]
\end{Verbatim}
            
    \hypertarget{expected-output}{%
\subparagraph{Expected Output}\label{expected-output}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{[}\DecValTok{97}\NormalTok{, }\DecValTok{98}\NormalTok{, }\DecValTok{99}\NormalTok{, }\DecValTok{32}\NormalTok{, }\DecValTok{120}\NormalTok{, }\DecValTok{121}\NormalTok{, }\DecValTok{122}\NormalTok{, }\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

    \#\#\# 1.3 Batch generator

Most of the time in Natural Language Processing, and AI in general we
use batches when training our data sets. Here, you will build a data
generator that takes in a text and returns a batch of text lines (lines
are sentences). - The generator converts text lines (sentences) into
numpy arrays of integers padded by zeros so that all arrays have the
same length, which is the length of the longest sentence in the entire
data set.

Once you create the generator, you can iterate on it like this:

\begin{verbatim}
next(data_generator)
\end{verbatim}

This generator returns the data in a format that you could directly use
in your model when computing the feed-forward of your algorithm. This
iterator returns a batch of lines and per token mask. The batch is a
tuple of three parts: inputs, targets, mask. The inputs and targets are
identical. The second column will be used to evaluate your predictions.
Mask is 1 for non-padding tokens.

\#\#\# Exercise 02 \textbf{Instructions:} Implement the data generator
below. Here are some things you will need.

\begin{itemize}
\tightlist
\item
  While True loop: this will yield one batch at a time.
\item
  if index \textgreater{}= num\_lines, set index to 0.
\item
  The generator should return shuffled batches of data. To achieve this
  without modifying the actual lines a list containing the indexes of
  \texttt{data\_lines} is created. This list can be shuffled and used to
  get random batches everytime the index is reset.
\item
  if len(line) \textless{} max\_length append line to cur\_batch.

  \begin{itemize}
  \tightlist
  \item
    Note that a line that has length equal to max\_length should not be
    appended to the batch.
  \item
    This is because when converting the characters into a tensor of
    integers, an additional end of sentence token id will be added.\\
  \item
    So if max\_length is 5, and a line has 4 characters, the tensor
    representing those 4 characters plus the end of sentence character
    will be of length 5, which is the max length.
  \end{itemize}
\item
  if len(cur\_batch) == batch\_size, go over every line, convert it to
  an int and store it.
\end{itemize}

\textbf{Remember that when calling np you are really calling
trax.fastmath.numpy which is trax's version of numpy that is compatible
with JAX. As a result of this, where you used to encounter the type
numpy.ndarray now you will find the type
jax.interpreters.xla.DeviceArray.}

    Hints

Use the line\_to\_tensor function above inside a list comprehension in
order to pad lines with zeros.

Keep in mind that the length of the tensor is always 1 + the length of
the original line of characters. Keep this in mind when setting the
padding of zeros.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}12}]:} \PY{c+c1}{\PYZsh{} UNQ\PYZus{}C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)}
         \PY{c+c1}{\PYZsh{} GRADED FUNCTION: data\PYZus{}generator}
         \PY{k}{def} \PY{n+nf}{data\PYZus{}generator}\PY{p}{(}\PY{n}{batch\PYZus{}size}\PY{p}{,} \PY{n}{max\PYZus{}length}\PY{p}{,} \PY{n}{data\PYZus{}lines}\PY{p}{,} \PY{n}{line\PYZus{}to\PYZus{}tensor}\PY{o}{=}\PY{n}{line\PYZus{}to\PYZus{}tensor}\PY{p}{,} \PY{n}{shuffle}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Generator function that yields batches of data}
         
         \PY{l+s+sd}{    Args:}
         \PY{l+s+sd}{        batch\PYZus{}size (int): number of examples (in this case, sentences) per batch.}
         \PY{l+s+sd}{        max\PYZus{}length (int): maximum length of the output tensor.}
         \PY{l+s+sd}{        NOTE: max\PYZus{}length includes the end\PYZhy{}of\PYZhy{}sentence character that will be added}
         \PY{l+s+sd}{                to the tensor.  }
         \PY{l+s+sd}{                Keep in mind that the length of the tensor is always 1 + the length}
         \PY{l+s+sd}{                of the original line of characters.}
         \PY{l+s+sd}{        data\PYZus{}lines (list): list of the sentences to group into batches.}
         \PY{l+s+sd}{        line\PYZus{}to\PYZus{}tensor (function, optional): function that converts line to tensor. Defaults to line\PYZus{}to\PYZus{}tensor.}
         \PY{l+s+sd}{        shuffle (bool, optional): True if the generator should generate random batches of data. Defaults to True.}
         
         \PY{l+s+sd}{    Yields:}
         \PY{l+s+sd}{        tuple: two copies of the batch (jax.interpreters.xla.DeviceArray) and mask (jax.interpreters.xla.DeviceArray).}
         \PY{l+s+sd}{        NOTE: jax.interpreters.xla.DeviceArray is trax\PYZsq{}s version of numpy.ndarray}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{c+c1}{\PYZsh{} initialize the index that points to the current position in the lines index array}
             \PY{n}{index} \PY{o}{=} \PY{l+m+mi}{0}
             
             \PY{c+c1}{\PYZsh{} initialize the list that will contain the current batch}
             \PY{n}{cur\PYZus{}batch} \PY{o}{=} \PY{p}{[}\PY{p}{]}
             
             \PY{c+c1}{\PYZsh{} count the number of lines in data\PYZus{}lines}
             \PY{n}{num\PYZus{}lines} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{data\PYZus{}lines}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{} create an array with the indexes of data\PYZus{}lines that can be shuffled}
             \PY{n}{lines\PYZus{}index} \PY{o}{=} \PY{p}{[}\PY{o}{*}\PY{n+nb}{range}\PY{p}{(}\PY{n}{num\PYZus{}lines}\PY{p}{)}\PY{p}{]}
             
             \PY{c+c1}{\PYZsh{} shuffle line indexes if shuffle is set to True}
             \PY{k}{if} \PY{n}{shuffle}\PY{p}{:}
                 \PY{n}{rnd}\PY{o}{.}\PY{n}{shuffle}\PY{p}{(}\PY{n}{lines\PYZus{}index}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} START CODE HERE (Replace instances of \PYZsq{}None\PYZsq{} with your code) \PYZsh{}\PYZsh{}\PYZsh{}}
             \PY{k}{while} \PY{k+kc}{True}\PY{p}{:}
                 
                 \PY{c+c1}{\PYZsh{} if the index is greater or equal than to the number of lines in data\PYZus{}lines}
                 \PY{k}{if} \PY{n}{index} \PY{o}{\PYZgt{}}\PY{o}{=} \PY{n}{num\PYZus{}lines}\PY{p}{:}
                     \PY{c+c1}{\PYZsh{} then reset the index to 0}
                     \PY{n}{index} \PY{o}{=} \PY{l+m+mi}{0}
                     \PY{c+c1}{\PYZsh{} shuffle line indexes if shuffle is set to True}
                     \PY{k}{if} \PY{n}{shuffle}\PY{p}{:}
                         \PY{n}{rnd}\PY{o}{.}\PY{n}{shuffle}\PY{p}{(}\PY{n}{lines\PYZus{}index}\PY{p}{)}
                     
                 \PY{c+c1}{\PYZsh{} get a line at the `lines\PYZus{}index[index]` position in data\PYZus{}lines}
                 \PY{n}{line} \PY{o}{=} \PY{n}{data\PYZus{}lines}\PY{p}{[}\PY{n}{lines\PYZus{}index}\PY{p}{[}\PY{n}{index}\PY{p}{]}\PY{p}{]}
                 
                 \PY{c+c1}{\PYZsh{} if the length of the line is less than max\PYZus{}length}
                 \PY{k}{if} \PY{n+nb}{len}\PY{p}{(}\PY{n}{line}\PY{p}{)} \PY{o}{\PYZlt{}} \PY{n}{max\PYZus{}length}\PY{p}{:}
                     \PY{c+c1}{\PYZsh{} append the line to the current batch}
                     \PY{n}{cur\PYZus{}batch}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{line}\PY{p}{)}
                     
                 \PY{c+c1}{\PYZsh{} increment the index by one}
                 \PY{n}{index} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
                 
                 \PY{c+c1}{\PYZsh{} if the current batch is now equal to the desired batch size}
                 \PY{k}{if} \PY{n+nb}{len}\PY{p}{(}\PY{n}{cur\PYZus{}batch}\PY{p}{)} \PY{o}{==} \PY{n}{batch\PYZus{}size}\PY{p}{:}
                     
                     \PY{n}{batch} \PY{o}{=} \PY{p}{[}\PY{p}{]}
                     \PY{n}{mask} \PY{o}{=} \PY{p}{[}\PY{p}{]}
                     
                     \PY{c+c1}{\PYZsh{} go through each line (li) in cur\PYZus{}batch}
                     \PY{k}{for} \PY{n}{li} \PY{o+ow}{in} \PY{n}{cur\PYZus{}batch}\PY{p}{:}
                         \PY{c+c1}{\PYZsh{} convert the line (li) to a tensor of integers}
                         \PY{n}{tensor} \PY{o}{=} \PY{n}{line\PYZus{}to\PYZus{}tensor}\PY{p}{(}\PY{n}{li}\PY{p}{)}
                         
                         \PY{c+c1}{\PYZsh{} Create a list of zeros to represent the padding}
                         \PY{c+c1}{\PYZsh{} so that the tensor plus padding will have length `max\PYZus{}length`}
                         \PY{n}{pad} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{*} \PY{p}{(}\PY{n}{max\PYZus{}length} \PY{o}{\PYZhy{}} \PY{n+nb}{len}\PY{p}{(}\PY{n}{tensor}\PY{p}{)}\PY{p}{)}
                         
                         \PY{c+c1}{\PYZsh{} combine the tensor plus pad}
                         \PY{n}{tensor\PYZus{}pad} \PY{o}{=} \PY{n}{tensor} \PY{o}{+} \PY{n}{pad}
                         
                         \PY{c+c1}{\PYZsh{} append the padded tensor to the batch}
                         \PY{n}{batch}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{tensor\PYZus{}pad}\PY{p}{)}
         
                         \PY{c+c1}{\PYZsh{} A mask for  tensor\PYZus{}pad is 1 wherever tensor\PYZus{}pad is not}
                         \PY{c+c1}{\PYZsh{} 0 and 0 wherever tensor\PYZus{}pad is 0, i.e. if tensor\PYZus{}pad is}
                         \PY{c+c1}{\PYZsh{} [1, 2, 3, 0, 0, 0] then example\PYZus{}mask should be}
                         \PY{c+c1}{\PYZsh{} [1, 1, 1, 0, 0, 0]}
                         \PY{c+c1}{\PYZsh{} Hint: Use a list comprehension for this}
                         \PY{n}{example\PYZus{}mask} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{0} \PY{k}{if} \PY{n}{t} \PY{o}{==} \PY{l+m+mi}{0} \PY{k}{else} \PY{l+m+mi}{1} \PY{k}{for} \PY{n}{t} \PY{o+ow}{in} \PY{n}{tensor\PYZus{}pad}\PY{p}{]}
                         \PY{n}{mask}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{example\PYZus{}mask}\PY{p}{)}
                        
                     \PY{c+c1}{\PYZsh{} convert the batch (data type list) to a trax\PYZsq{}s numpy array}
                     \PY{n}{batch\PYZus{}np\PYZus{}arr} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{batch}\PY{p}{)}
                     \PY{n}{mask\PYZus{}np\PYZus{}arr} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{mask}\PY{p}{)}
                     
                     \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} END CODE HERE \PYZsh{}\PYZsh{}}
                     
                     \PY{c+c1}{\PYZsh{} Yield two copies of the batch and mask.}
                     \PY{k}{yield} \PY{n}{batch\PYZus{}np\PYZus{}arr}\PY{p}{,} \PY{n}{batch\PYZus{}np\PYZus{}arr}\PY{p}{,} \PY{n}{mask\PYZus{}np\PYZus{}arr}
                     
                     \PY{c+c1}{\PYZsh{} reset the current batch to an empty list}
                     \PY{n}{cur\PYZus{}batch} \PY{o}{=} \PY{p}{[}\PY{p}{]}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} \PY{c+c1}{\PYZsh{} Try out your data generator}
         \PY{n}{tmp\PYZus{}lines} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{12345678901}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{c+c1}{\PYZsh{}length 11}
                      \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{123456789}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{c+c1}{\PYZsh{} length 9}
                      \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{234567890}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{c+c1}{\PYZsh{} length 9}
                      \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{345678901}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{c+c1}{\PYZsh{} length 9}
         
         \PY{c+c1}{\PYZsh{} Get a batch size of 2, max length 10}
         \PY{n}{tmp\PYZus{}data\PYZus{}gen} \PY{o}{=} \PY{n}{data\PYZus{}generator}\PY{p}{(}\PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} 
                                       \PY{n}{max\PYZus{}length}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} 
                                       \PY{n}{data\PYZus{}lines}\PY{o}{=}\PY{n}{tmp\PYZus{}lines}\PY{p}{,}
                                       \PY{n}{shuffle}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} get one batch}
         \PY{n}{tmp\PYZus{}batch} \PY{o}{=} \PY{n+nb}{next}\PY{p}{(}\PY{n}{tmp\PYZus{}data\PYZus{}gen}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} view the batch}
         \PY{n}{tmp\PYZus{}batch}
\end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}14}]:} (DeviceArray([[49, 50, 51, 52, 53, 54, 55, 56, 57,  1],
                       [50, 51, 52, 53, 54, 55, 56, 57, 48,  1]], dtype=int32),
          DeviceArray([[49, 50, 51, 52, 53, 54, 55, 56, 57,  1],
                       [50, 51, 52, 53, 54, 55, 56, 57, 48,  1]], dtype=int32),
          DeviceArray([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
                       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32))
\end{Verbatim}
            
    \hypertarget{expected-output}{%
\subparagraph{Expected output}\label{expected-output}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(DeviceArray([[}\DecValTok{49}\NormalTok{,}\AttributeTok{ }\DecValTok{50}\NormalTok{,}\AttributeTok{ }\DecValTok{51}\NormalTok{,}\AttributeTok{ }\DecValTok{52}\NormalTok{,}\AttributeTok{ }\DecValTok{53}\NormalTok{,}\AttributeTok{ }\DecValTok{54}\NormalTok{,}\AttributeTok{ }\DecValTok{55}\NormalTok{,}\AttributeTok{ }\DecValTok{56}\NormalTok{,}\AttributeTok{ }\DecValTok{57}\NormalTok{,}\AttributeTok{  }\DecValTok{1}\NormalTok{],}
\AttributeTok{              }\NormalTok{[}\DecValTok{50}\NormalTok{,}\AttributeTok{ }\DecValTok{51}\NormalTok{,}\AttributeTok{ }\DecValTok{52}\NormalTok{,}\AttributeTok{ }\DecValTok{53}\NormalTok{,}\AttributeTok{ }\DecValTok{54}\NormalTok{,}\AttributeTok{ }\DecValTok{55}\NormalTok{,}\AttributeTok{ }\DecValTok{56}\NormalTok{,}\AttributeTok{ }\DecValTok{57}\NormalTok{,}\AttributeTok{ }\DecValTok{48}\NormalTok{,}\AttributeTok{  }\DecValTok{1}\NormalTok{]], dtype=int32),}
\NormalTok{ DeviceArray([[}\DecValTok{49}\NormalTok{,}\AttributeTok{ }\DecValTok{50}\NormalTok{,}\AttributeTok{ }\DecValTok{51}\NormalTok{,}\AttributeTok{ }\DecValTok{52}\NormalTok{,}\AttributeTok{ }\DecValTok{53}\NormalTok{,}\AttributeTok{ }\DecValTok{54}\NormalTok{,}\AttributeTok{ }\DecValTok{55}\NormalTok{,}\AttributeTok{ }\DecValTok{56}\NormalTok{,}\AttributeTok{ }\DecValTok{57}\NormalTok{,}\AttributeTok{  }\DecValTok{1}\NormalTok{],}
\AttributeTok{              }\NormalTok{[}\DecValTok{50}\NormalTok{,}\AttributeTok{ }\DecValTok{51}\NormalTok{,}\AttributeTok{ }\DecValTok{52}\NormalTok{,}\AttributeTok{ }\DecValTok{53}\NormalTok{,}\AttributeTok{ }\DecValTok{54}\NormalTok{,}\AttributeTok{ }\DecValTok{55}\NormalTok{,}\AttributeTok{ }\DecValTok{56}\NormalTok{,}\AttributeTok{ }\DecValTok{57}\NormalTok{,}\AttributeTok{ }\DecValTok{48}\NormalTok{,}\AttributeTok{  }\DecValTok{1}\NormalTok{]], dtype=int32),}
\NormalTok{ DeviceArray([[}\DecValTok{1}\NormalTok{,}\AttributeTok{ }\DecValTok{1}\NormalTok{,}\AttributeTok{ }\DecValTok{1}\NormalTok{,}\AttributeTok{ }\DecValTok{1}\NormalTok{,}\AttributeTok{ }\DecValTok{1}\NormalTok{,}\AttributeTok{ }\DecValTok{1}\NormalTok{,}\AttributeTok{ }\DecValTok{1}\NormalTok{,}\AttributeTok{ }\DecValTok{1}\NormalTok{,}\AttributeTok{ }\DecValTok{1}\NormalTok{,}\AttributeTok{ }\DecValTok{1}\NormalTok{],}
\AttributeTok{              }\NormalTok{[}\DecValTok{1}\NormalTok{,}\AttributeTok{ }\DecValTok{1}\NormalTok{,}\AttributeTok{ }\DecValTok{1}\NormalTok{,}\AttributeTok{ }\DecValTok{1}\NormalTok{,}\AttributeTok{ }\DecValTok{1}\NormalTok{,}\AttributeTok{ }\DecValTok{1}\NormalTok{,}\AttributeTok{ }\DecValTok{1}\NormalTok{,}\AttributeTok{ }\DecValTok{1}\NormalTok{,}\AttributeTok{ }\DecValTok{1}\NormalTok{,}\AttributeTok{ }\DecValTok{1}\NormalTok{]], dtype=int32))}
\end{Highlighting}
\end{Shaded}

    Now that you have your generator, you can just call them and they will
return tensors which correspond to your lines in Shakespeare. The first
column and the second column are identical. Now you can go ahead and
start building your neural network.

    \#\#\# 1.4 Repeating Batch generator

The way the iterator is currently defined, it will keep providing
batches forever.

Although it is not needed, we want to show you the
\texttt{itertools.cycle} function which is really useful when the
generator eventually stops

Notice that it is expected to use this function within the training
function further below

Usually we want to cycle over the dataset multiple times during training
(i.e.~train for multiple \emph{epochs}).

For small datasets we can use
\href{https://docs.python.org/3.8/library/itertools.html\#itertools.cycle}{\texttt{itertools.cycle}}
to achieve this easily.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}15}]:} \PY{k+kn}{import} \PY{n+nn}{itertools}
         
         \PY{n}{infinite\PYZus{}data\PYZus{}generator} \PY{o}{=} \PY{n}{itertools}\PY{o}{.}\PY{n}{cycle}\PY{p}{(}
             \PY{n}{data\PYZus{}generator}\PY{p}{(}\PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{max\PYZus{}length}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{data\PYZus{}lines}\PY{o}{=}\PY{n}{tmp\PYZus{}lines}\PY{p}{)}\PY{p}{)}
\end{Verbatim}

    You can see that we can get more than the 5 lines in tmp\_lines using
this.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}16}]:} \PY{n}{ten\PYZus{}lines} \PY{o}{=} \PY{p}{[}\PY{n+nb}{next}\PY{p}{(}\PY{n}{infinite\PYZus{}data\PYZus{}generator}\PY{p}{)} \PY{k}{for} \PY{n}{\PYZus{}} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{]}
         \PY{n+nb}{print}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{ten\PYZus{}lines}\PY{p}{)}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
10

    \end{Verbatim}

    \hypertarget{part-2-defining-the-gru-model}{%
\section{Part 2: Defining the GRU
model}\label{part-2-defining-the-gru-model}}

Now that you have the input and output tensors, you will go ahead and
initialize your model. You will be implementing the \texttt{GRULM},
gated recurrent unit model. To implement this model, you will be using
google's \texttt{trax} package. Instead of making you implement the
\texttt{GRU} from scratch, we will give you the necessary methods from a
build in package. You can use the following packages when constructing
the model:

\begin{itemize}
\tightlist
\item
  \texttt{tl.Serial}: Combinator that applies layers serially (by
  function composition).
  \href{https://trax-ml.readthedocs.io/en/latest/trax.layers.html\#trax.layers.combinators.Serial}{docs}
  /
  \href{https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/combinators.py\#L26}{source
  code}

  \begin{itemize}
  \tightlist
  \item
    You can pass in the layers as arguments to \texttt{Serial},
    separated by commas.
  \item
    For example:
    \texttt{tl.Serial(tl.Embeddings(...),\ tl.Mean(...),\ tl.Dense(...),\ tl.LogSoftmax(...))}
  \end{itemize}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\begin{itemize}
\tightlist
\item
  \texttt{tl.ShiftRight}: Allows the model to go right in the feed
  forward.
  \href{https://trax-ml.readthedocs.io/en/latest/trax.layers.html\#trax.layers.attention.ShiftRight}{docs}
  /
  \href{https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/attention.py\#L297}{source
  code}

  \begin{itemize}
  \tightlist
  \item
    \texttt{ShiftRight(n\_shifts=1,\ mode=\textquotesingle{}train\textquotesingle{})}
    layer to shift the tensor to the right n\_shift times
  \item
    Here in the exercise you only need to specify the mode and not worry
    about n\_shifts
  \end{itemize}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\begin{itemize}
\tightlist
\item
  \texttt{tl.Embedding}: Initializes the embedding. In this case it is
  the size of the vocabulary by the dimension of the model.
  \href{https://trax-ml.readthedocs.io/en/latest/trax.layers.html\#trax.layers.core.Embedding}{docs}
  /
  \href{https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/core.py\#L113}{source
  code}

  \begin{itemize}
  \tightlist
  \item
    \texttt{tl.Embedding(vocab\_size,\ d\_feature)}.
  \item
    \texttt{vocab\_size} is the number of unique words in the given
    vocabulary.
  \item
    \texttt{d\_feature} is the number of elements in the word embedding
    (some choices for a word embedding size range from 150 to 300, for
    example). \_\_\_
  \end{itemize}
\item
  \texttt{tl.GRU}: \texttt{Trax} GRU layer.
  \href{https://trax-ml.readthedocs.io/en/latest/trax.layers.html\#trax.layers.rnn.GRU}{docs}
  /
  \href{https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/rnn.py\#L143}{source
  code}

  \begin{itemize}
  \tightlist
  \item
    \texttt{GRU(n\_units)} Builds a traditional GRU of n\_cells with
    dense internal transformations.
  \item
    \texttt{GRU} paper: https://arxiv.org/abs/1412.3555 \_\_\_
  \end{itemize}
\item
  \texttt{tl.Dense}: A dense layer.
  \href{https://trax-ml.readthedocs.io/en/latest/trax.layers.html\#trax.layers.core.Dense}{docs}
  /
  \href{https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/core.py\#L28}{source
  code}

  \begin{itemize}
  \tightlist
  \item
    \texttt{tl.Dense(n\_units)}: The parameter \texttt{n\_units} is the
    number of units chosen for this dense layer. \_\_\_
  \end{itemize}
\item
  \texttt{tl.LogSoftmax}: Log of the output probabilities.
  \href{https://trax-ml.readthedocs.io/en/latest/trax.layers.html\#trax.layers.core.LogSoftmax}{docs}
  /
  \href{https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/core.py\#L242}{source
  code}

  \begin{itemize}
  \tightlist
  \item
    Here, you don't need to set any parameters for
    \texttt{LogSoftMax()}. \_\_\_
  \end{itemize}
\end{itemize}

\#\#\# Exercise 03 \textbf{Instructions:} Implement the \texttt{GRULM}
class below. You should be using all the methods explained above.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}17}]:} \PY{c+c1}{\PYZsh{} UNQ\PYZus{}C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)}
         \PY{c+c1}{\PYZsh{} GRADED FUNCTION: GRULM}
         \PY{k}{def} \PY{n+nf}{GRULM}\PY{p}{(}\PY{n}{vocab\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{256}\PY{p}{,} \PY{n}{d\PYZus{}model}\PY{o}{=}\PY{l+m+mi}{512}\PY{p}{,} \PY{n}{n\PYZus{}layers}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{mode}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Returns a GRU language model.}
         
         \PY{l+s+sd}{    Args:}
         \PY{l+s+sd}{        vocab\PYZus{}size (int, optional): Size of the vocabulary. Defaults to 256.}
         \PY{l+s+sd}{        d\PYZus{}model (int, optional): Depth of embedding (n\PYZus{}units in the GRU cell). Defaults to 512.}
         \PY{l+s+sd}{        n\PYZus{}layers (int, optional): Number of GRU layers. Defaults to 2.}
         \PY{l+s+sd}{        mode (str, optional): \PYZsq{}train\PYZsq{}, \PYZsq{}eval\PYZsq{} or \PYZsq{}predict\PYZsq{}, predict mode is for fast inference. Defaults to \PYZdq{}train\PYZdq{}.}
         
         \PY{l+s+sd}{    Returns:}
         \PY{l+s+sd}{        trax.layers.combinators.Serial: A GRU language model as a layer that maps from a tensor of tokens to activations over a vocab set.}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} START CODE HERE (Replace instances of \PYZsq{}None\PYZsq{} with your code) \PYZsh{}\PYZsh{}\PYZsh{}}
             \PY{n}{model} \PY{o}{=} \PY{n}{tl}\PY{o}{.}\PY{n}{Serial}\PY{p}{(}
               \PY{n}{tl}\PY{o}{.}\PY{n}{ShiftRight}\PY{p}{(}\PY{n}{mode}\PY{o}{=}\PY{n}{mode}\PY{p}{)}\PY{p}{,} \PY{c+c1}{\PYZsh{} Stack the ShiftRight layer}
               \PY{n}{tl}\PY{o}{.}\PY{n}{Embedding}\PY{p}{(}\PY{n}{vocab\PYZus{}size}\PY{o}{=}\PY{n}{vocab\PYZus{}size}\PY{p}{,} \PY{n}{d\PYZus{}feature}\PY{o}{=}\PY{n}{d\PYZus{}model}\PY{p}{)}\PY{p}{,} \PY{c+c1}{\PYZsh{} Stack the embedding layer}
               \PY{p}{[}\PY{n}{tl}\PY{o}{.}\PY{n}{GRU}\PY{p}{(}\PY{n}{n\PYZus{}units}\PY{o}{=}\PY{n}{d\PYZus{}model}\PY{p}{)} \PY{k}{for} \PY{n}{\PYZus{}} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{n\PYZus{}layers}\PY{p}{)}\PY{p}{]}\PY{p}{,} \PY{c+c1}{\PYZsh{} Stack GRU layers of d\PYZus{}model units keeping n\PYZus{}layer parameter in mind (use list comprehension syntax)}
               \PY{n}{tl}\PY{o}{.}\PY{n}{Dense}\PY{p}{(}\PY{n}{n\PYZus{}units}\PY{o}{=}\PY{n}{vocab\PYZus{}size}\PY{p}{)}\PY{p}{,} \PY{c+c1}{\PYZsh{} Dense layer}
               \PY{n}{tl}\PY{o}{.}\PY{n}{LogSoftmax}\PY{p}{(}\PY{p}{)} \PY{c+c1}{\PYZsh{} Log Softmax}
             \PY{p}{)}
             \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} END CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}
             \PY{k}{return} \PY{n}{model}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}18}]:} \PY{c+c1}{\PYZsh{} testing your model}
         \PY{n}{model} \PY{o}{=} \PY{n}{GRULM}\PY{p}{(}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{model}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Serial[
  ShiftRight(1)
  Embedding\_256\_512
  GRU\_512
  GRU\_512
  Dense\_256
  LogSoftmax
]

    \end{Verbatim}

    \hypertarget{expected-output}{%
\subparagraph{Expected output}\label{expected-output}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Serial[}
\NormalTok{  ShiftRight(}\DecValTok{1}\NormalTok{)}
\NormalTok{  Embeddin}\VariableTok{g_256_512}
\NormalTok{  GRU_512}
\NormalTok{  GRU_512}
\NormalTok{  Dense_256}
\NormalTok{  LogSoftmax}
\NormalTok{]}
\end{Highlighting}
\end{Shaded}

    \# Part 3: Training

Now you are going to train your model. As usual, you have to define the
cost function, the optimizer, and decide whether you will be training it
on a \texttt{gpu} or \texttt{cpu}. You also have to feed in a built
model. Before, going into the training, we re-introduce the
\texttt{TrainTask} and \texttt{EvalTask} abstractions from the last
week's assignment.

To train a model on a task, Trax defines an abstraction
\texttt{trax.supervised.training.TrainTask} which packages the train
data, loss and optimizer (among other things) together into an object.

Similarly to evaluate a model, Trax defines an abstraction
\texttt{trax.supervised.training.EvalTask} which packages the eval data
and metrics (among other things) into another object.

The final piece tying things together is the
\texttt{trax.supervised.training.Loop} abstraction that is a very simple
and flexible way to put everything together and train the model, all the
while evaluating it and saving checkpoints. Using \texttt{training.Loop}
will save you a lot of code compared to always writing the training loop
by hand, like you did in courses 1 and 2. More importantly, you are less
likely to have a bug in that code that would ruin your training.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}19}]:} \PY{n}{batch\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{32}
         \PY{n}{max\PYZus{}length} \PY{o}{=} \PY{l+m+mi}{64}
\end{Verbatim}

    An \texttt{epoch} is traditionally defined as one pass through the
dataset.

Since the dataset was divided in \texttt{batches} you need several
\texttt{steps} (gradient evaluations) in order to complete an
\texttt{epoch}. So, one \texttt{epoch} corresponds to the number of
examples in a \texttt{batch} times the number of \texttt{steps}. In
short, in each \texttt{epoch} you go over all the dataset.

The \texttt{max\_length} variable defines the maximum length of lines to
be used in training our data, lines longer that that length are
discarded.

Below is a function and results that indicate how many lines conform to
our criteria of maximum length of a sentence in the entire dataset and
how many \texttt{steps} are required in order to cover the entire
dataset which in turn corresponds to an \texttt{epoch}.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}20}]:} \PY{k}{def} \PY{n+nf}{n\PYZus{}used\PYZus{}lines}\PY{p}{(}\PY{n}{lines}\PY{p}{,} \PY{n}{max\PYZus{}length}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}}
         \PY{l+s+sd}{    Args: }
         \PY{l+s+sd}{    lines: all lines of text an array of lines}
         \PY{l+s+sd}{    max\PYZus{}length \PYZhy{} max\PYZus{}length of a line in order to be considered an int}
         \PY{l+s+sd}{    Return:}
         \PY{l+s+sd}{    n\PYZus{}lines \PYZhy{}number of efective examples}
         \PY{l+s+sd}{    \PYZsq{}\PYZsq{}\PYZsq{}}
         
             \PY{n}{n\PYZus{}lines} \PY{o}{=} \PY{l+m+mi}{0}
             \PY{k}{for} \PY{n}{l} \PY{o+ow}{in} \PY{n}{lines}\PY{p}{:}
                 \PY{k}{if} \PY{n+nb}{len}\PY{p}{(}\PY{n}{l}\PY{p}{)} \PY{o}{\PYZlt{}}\PY{o}{=} \PY{n}{max\PYZus{}length}\PY{p}{:}
                     \PY{n}{n\PYZus{}lines} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
             \PY{k}{return} \PY{n}{n\PYZus{}lines}
         
         \PY{n}{num\PYZus{}used\PYZus{}lines} \PY{o}{=} \PY{n}{n\PYZus{}used\PYZus{}lines}\PY{p}{(}\PY{n}{lines}\PY{p}{,} \PY{l+m+mi}{32}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Number of used lines from the dataset:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{num\PYZus{}used\PYZus{}lines}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Batch size (a power of 2):}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n+nb}{int}\PY{p}{(}\PY{n}{batch\PYZus{}size}\PY{p}{)}\PY{p}{)}
         \PY{n}{steps\PYZus{}per\PYZus{}epoch} \PY{o}{=} \PY{n+nb}{int}\PY{p}{(}\PY{n}{num\PYZus{}used\PYZus{}lines}\PY{o}{/}\PY{n}{batch\PYZus{}size}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Number of steps to cover one epoch:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{steps\PYZus{}per\PYZus{}epoch}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Number of used lines from the dataset: 25881
Batch size (a power of 2): 32
Number of steps to cover one epoch: 808

    \end{Verbatim}

    \textbf{Expected output:}

Number of used lines from the dataset: 25881

Batch size (a power of 2): 32

Number of steps to cover one epoch: 808

    \#\#\# 3.1 Training the model

You will now write a function that takes in your model and trains it. To
train your model you have to decide how many times you want to iterate
over the entire data set.

\#\#\# Exercise 04

\textbf{Instructions:} Implement the \texttt{train\_model} program below
to train the neural network above. Here is a list of things you should
do:

\begin{itemize}
\tightlist
\item
  Create a \texttt{trax.supervised.trainer.TrainTask} object, this
  encapsulates the aspects of the dataset and the problem at hand:

  \begin{itemize}
  \tightlist
  \item
    labeled\_data = the labeled data that we want to \emph{train} on.
  \item
    loss\_fn =
    \href{https://trax-ml.readthedocs.io/en/latest/trax.layers.html?highlight=CrossEntropyLoss\#trax.layers.metrics.CrossEntropyLoss}{tl.CrossEntropyLoss()}
  \item
    optimizer =
    \href{https://trax-ml.readthedocs.io/en/latest/trax.optimizers.html?highlight=Adam\#trax.optimizers.adam.Adam}{trax.optimizers.Adam()}
    with learning rate = 0.0005
  \end{itemize}
\item
  Create a \texttt{trax.supervised.trainer.EvalTask} object, this
  encapsulates aspects of evaluating the model:

  \begin{itemize}
  \tightlist
  \item
    labeled\_data = the labeled data that we want to \emph{evaluate} on.
  \item
    metrics =
    \href{https://trax-ml.readthedocs.io/en/latest/trax.layers.html\#trax.layers.metrics.CrossEntropyLoss}{tl.CrossEntropyLoss()}
    and
    \href{https://trax-ml.readthedocs.io/en/latest/trax.layers.html\#trax.layers.metrics.Accuracy}{tl.Accuracy()}
  \item
    How frequently we want to evaluate and checkpoint the model.
  \end{itemize}
\item
  Create a \texttt{trax.supervised.trainer.Loop} object, this
  encapsulates the following:

  \begin{itemize}
  \tightlist
  \item
    The previously created \texttt{TrainTask} and \texttt{EvalTask}
    objects.
  \item
    the training model = Section \ref{ex03}
  \item
    optionally the evaluation model, if different from the training
    model. NOTE: in presence of Dropout etc we usually want the
    evaluation model to behave slightly differently than the training
    model.
  \end{itemize}
\end{itemize}

You will be using a cross entropy loss, with Adam optimizer. Please read
the \href{https://trax-ml.readthedocs.io/en/latest/index.html}{trax}
documentation to get a full understanding. Make sure you use the number
of steps provided as a parameter to train for the desired number of
steps.

\textbf{NOTE:} Don't forget to wrap the data generator in
\texttt{itertools.cycle} to iterate on it for multiple epochs.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}21}]:} \PY{k+kn}{from} \PY{n+nn}{trax}\PY{n+nn}{.}\PY{n+nn}{supervised} \PY{k}{import} \PY{n}{training}
         
         \PY{c+c1}{\PYZsh{} UNQ\PYZus{}C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)}
         \PY{c+c1}{\PYZsh{} GRADED FUNCTION: train\PYZus{}model}
         \PY{k}{def} \PY{n+nf}{train\PYZus{}model}\PY{p}{(}\PY{n}{model}\PY{p}{,} \PY{n}{data\PYZus{}generator}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{32}\PY{p}{,} \PY{n}{max\PYZus{}length}\PY{o}{=}\PY{l+m+mi}{64}\PY{p}{,} \PY{n}{lines}\PY{o}{=}\PY{n}{lines}\PY{p}{,} \PY{n}{eval\PYZus{}lines}\PY{o}{=}\PY{n}{eval\PYZus{}lines}\PY{p}{,} \PY{n}{n\PYZus{}steps}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{output\PYZus{}dir}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{model/}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{:} 
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Function that trains the model}
         
         \PY{l+s+sd}{    Args:}
         \PY{l+s+sd}{        model (trax.layers.combinators.Serial): GRU model.}
         \PY{l+s+sd}{        data\PYZus{}generator (function): Data generator function.}
         \PY{l+s+sd}{        batch\PYZus{}size (int, optional): Number of lines per batch. Defaults to 32.}
         \PY{l+s+sd}{        max\PYZus{}length (int, optional): Maximum length allowed for a line to be processed. Defaults to 64.}
         \PY{l+s+sd}{        lines (list, optional): List of lines to use for training. Defaults to lines.}
         \PY{l+s+sd}{        eval\PYZus{}lines (list, optional): List of lines to use for evaluation. Defaults to eval\PYZus{}lines.}
         \PY{l+s+sd}{        n\PYZus{}steps (int, optional): Number of steps to train. Defaults to 1.}
         \PY{l+s+sd}{        output\PYZus{}dir (str, optional): Relative path of directory to save model. Defaults to \PYZdq{}model/\PYZdq{}.}
         
         \PY{l+s+sd}{    Returns:}
         \PY{l+s+sd}{        trax.supervised.training.Loop: Training loop for the model.}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             
             \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} START CODE HERE (Replace instances of \PYZsq{}None\PYZsq{} with your code) \PYZsh{}\PYZsh{}\PYZsh{}}
             \PY{n}{bare\PYZus{}train\PYZus{}generator} \PY{o}{=} \PY{n}{data\PYZus{}generator}\PY{p}{(}\PY{n}{batch\PYZus{}size}\PY{p}{,} \PY{n}{max\PYZus{}length}\PY{p}{,} \PY{n}{data\PYZus{}lines}\PY{o}{=}\PY{n}{lines}\PY{p}{)}
             \PY{n}{infinite\PYZus{}train\PYZus{}generator} \PY{o}{=} \PY{n}{itertools}\PY{o}{.}\PY{n}{cycle}\PY{p}{(}\PY{n}{bare\PYZus{}train\PYZus{}generator}\PY{p}{)}
             
             \PY{n}{bare\PYZus{}eval\PYZus{}generator} \PY{o}{=} \PY{n}{data\PYZus{}generator}\PY{p}{(}\PY{n}{batch\PYZus{}size}\PY{p}{,} \PY{n}{max\PYZus{}length}\PY{p}{,} \PY{n}{data\PYZus{}lines}\PY{o}{=}\PY{n}{eval\PYZus{}lines}\PY{p}{)}
             \PY{n}{infinite\PYZus{}eval\PYZus{}generator} \PY{o}{=} \PY{n}{itertools}\PY{o}{.}\PY{n}{cycle}\PY{p}{(}\PY{n}{bare\PYZus{}eval\PYZus{}generator}\PY{p}{)}
            
             \PY{n}{train\PYZus{}task} \PY{o}{=} \PY{n}{training}\PY{o}{.}\PY{n}{TrainTask}\PY{p}{(}
                 \PY{n}{labeled\PYZus{}data}\PY{o}{=}\PY{n}{infinite\PYZus{}train\PYZus{}generator}\PY{p}{,} \PY{c+c1}{\PYZsh{} Use infinite train data generator}
                 \PY{n}{loss\PYZus{}layer}\PY{o}{=}\PY{n}{tl}\PY{o}{.}\PY{n}{CrossEntropyLoss}\PY{p}{(}\PY{p}{)}\PY{p}{,}   \PY{c+c1}{\PYZsh{} Don\PYZsq{}t forget to instantiate this object}
                 \PY{n}{optimizer}\PY{o}{=}\PY{n}{trax}\PY{o}{.}\PY{n}{optimizers}\PY{o}{.}\PY{n}{Adam}\PY{p}{(}\PY{l+m+mf}{0.0005}\PY{p}{)}     \PY{c+c1}{\PYZsh{} Don\PYZsq{}t forget to add the learning rate parameter}
             \PY{p}{)}
         
             \PY{n}{eval\PYZus{}task} \PY{o}{=} \PY{n}{training}\PY{o}{.}\PY{n}{EvalTask}\PY{p}{(}
                 \PY{n}{labeled\PYZus{}data}\PY{o}{=}\PY{n}{infinite\PYZus{}eval\PYZus{}generator}\PY{p}{,}    \PY{c+c1}{\PYZsh{} Use infinite eval data generator}
                 \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{n}{tl}\PY{o}{.}\PY{n}{CrossEntropyLoss}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{tl}\PY{o}{.}\PY{n}{Accuracy}\PY{p}{(}\PY{p}{)}\PY{p}{]}\PY{p}{,} \PY{c+c1}{\PYZsh{} Don\PYZsq{}t forget to instantiate these objects}
                 \PY{n}{n\PYZus{}eval\PYZus{}batches}\PY{o}{=}\PY{l+m+mi}{3}      \PY{c+c1}{\PYZsh{} For better evaluation accuracy in reasonable time}
             \PY{p}{)}
             
             \PY{n}{training\PYZus{}loop} \PY{o}{=} \PY{n}{training}\PY{o}{.}\PY{n}{Loop}\PY{p}{(}\PY{n}{model}\PY{p}{,}
                                           \PY{n}{train\PYZus{}task}\PY{p}{,}
                                           \PY{n}{eval\PYZus{}task}\PY{o}{=}\PY{n}{eval\PYZus{}task}\PY{p}{,}
                                           \PY{n}{output\PYZus{}dir}\PY{o}{=}\PY{n}{output\PYZus{}dir}\PY{p}{)}
         
             \PY{n}{training\PYZus{}loop}\PY{o}{.}\PY{n}{run}\PY{p}{(}\PY{n}{n\PYZus{}steps}\PY{o}{=}\PY{n}{n\PYZus{}steps}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} END CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}
             
             \PY{c+c1}{\PYZsh{} We return this because it contains a handle to the model, which has the weights etc.}
             \PY{k}{return} \PY{n}{training\PYZus{}loop}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}22}]:} \PY{c+c1}{\PYZsh{} Train the model 1 step and keep the `trax.supervised.training.Loop` object.}
         \PY{n}{training\PYZus{}loop} \PY{o}{=} \PY{n}{train\PYZus{}model}\PY{p}{(}\PY{n}{GRULM}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{data\PYZus{}generator}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Step      1: train CrossEntropyLoss |  5.54486227
Step      1: eval  CrossEntropyLoss |  5.48863840
Step      1: eval          Accuracy |  0.17598992

    \end{Verbatim}

    The model was only trained for 1 step due to the constraints of this
environment. Even on a GPU accelerated environment it will take many
hours for it to achieve a good level of accuracy. For the rest of the
assignment you will be using a pretrained model but now you should
understand how the training can be done using Trax.

    \# Part 4: Evaluation\\
\#\#\# 4.1 Evaluating using the deep nets

Now that you have learned how to train a model, you will learn how to
evaluate it. To evaluate language models, we usually use perplexity
which is a measure of how well a probability model predicts a sample.
Note that perplexity is defined as:

\[P(W) = \sqrt[N]{\prod_{i=1}^{N} \frac{1}{P(w_i| w_1,...,w_{n-1})}}\]

As an implementation hack, you would usually take the log of that
formula (to enable us to use the log probabilities we get as output of
our \texttt{RNN}, convert exponents to products, and products into sums
which makes computations less complicated and computationally more
efficient). You should also take care of the padding, since you do not
want to include the padding when calculating the perplexity (because we
do not want to have a perplexity measure artificially good).

\[log P(W) = {log\big(\sqrt[N]{\prod_{i=1}^{N} \frac{1}{P(w_i| w_1,...,w_{n-1})}}\big)}\]

\[ = {log\big({\prod_{i=1}^{N} \frac{1}{P(w_i| w_1,...,w_{n-1})}}\big)^{\frac{1}{N}}}\]

\[ = {log\big({\prod_{i=1}^{N}{P(w_i| w_1,...,w_{n-1})}}\big)^{-\frac{1}{N}}} \]
\[ = -\frac{1}{N}{log\big({\prod_{i=1}^{N}{P(w_i| w_1,...,w_{n-1})}}\big)} \]
\[ = -\frac{1}{N}{\big({\sum_{i=1}^{N}{logP(w_i| w_1,...,w_{n-1})}}\big)} \]

\#\#\# Exercise 05 \textbf{Instructions:} Write a program that will help
evaluate your model. Implementation hack: your program takes in preds
and target. Preds is a tensor of log probabilities. You can use
\href{https://github.com/google/trax/blob/22765bb18608d376d8cd660f9865760e4ff489cd/trax/layers/metrics.py\#L154}{\texttt{tl.one\_hot}}
to transform the target into the same dimension. You then multiply them
and sum.

You also have to create a mask to only get the non-padded probabilities.
Good luck!

    Hints

To convert the target into the same dimension as the predictions tensor
use tl.one.hot with target and preds.shape{[}-1{]}.

You will also need the np.equal function in order to unpad the data and
properly compute perplexity.

Keep in mind while implementing the formula above that wi represents a
letter from our 256 letter alphabet.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}23}]:} \PY{c+c1}{\PYZsh{} UNQ\PYZus{}C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)}
         \PY{c+c1}{\PYZsh{} GRADED FUNCTION: test\PYZus{}model}
         \PY{k}{def} \PY{n+nf}{test\PYZus{}model}\PY{p}{(}\PY{n}{preds}\PY{p}{,} \PY{n}{target}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Function to test the model.}
         
         \PY{l+s+sd}{    Args:}
         \PY{l+s+sd}{        preds (jax.interpreters.xla.DeviceArray): Predictions of a list of batches of tensors corresponding to lines of text.}
         \PY{l+s+sd}{        target (jax.interpreters.xla.DeviceArray): Actual list of batches of tensors corresponding to lines of text.}
         
         \PY{l+s+sd}{    Returns:}
         \PY{l+s+sd}{        float: log\PYZus{}perplexity of the model.}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} START CODE HERE (Replace instances of \PYZsq{}None\PYZsq{} with your code) \PYZsh{}\PYZsh{}\PYZsh{}}
             \PY{n}{total\PYZus{}log\PYZus{}ppx} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{preds} \PY{o}{*} \PY{n}{tl}\PY{o}{.}\PY{n}{one\PYZus{}hot}\PY{p}{(}\PY{n}{target}\PY{p}{,} \PY{n}{preds}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{,}\PY{n}{axis}\PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)} \PY{c+c1}{\PYZsh{} HINT: tl.one\PYZus{}hot() should replace one of the Nones}
         
             \PY{n}{non\PYZus{}pad} \PY{o}{=} \PY{l+m+mf}{1.0} \PY{o}{\PYZhy{}} \PY{n}{np}\PY{o}{.}\PY{n}{equal}\PY{p}{(}\PY{n}{target}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{)}          \PY{c+c1}{\PYZsh{} You should check if the target equals 0}
             \PY{n}{ppx} \PY{o}{=} \PY{n}{total\PYZus{}log\PYZus{}ppx} \PY{o}{*} \PY{n}{non\PYZus{}pad}                       \PY{c+c1}{\PYZsh{} Get rid of the padding}
         
             \PY{n}{log\PYZus{}ppx} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{ppx}\PY{p}{)} \PY{o}{/} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{non\PYZus{}pad}\PY{p}{)}
             \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} END CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}
             
             \PY{k}{return} \PY{o}{\PYZhy{}}\PY{n}{log\PYZus{}ppx}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}24}]:} \PY{c+c1}{\PYZsh{} UNQ\PYZus{}C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)}
         \PY{c+c1}{\PYZsh{} Testing }
         \PY{n}{model} \PY{o}{=} \PY{n}{GRULM}\PY{p}{(}\PY{p}{)}
         \PY{n}{model}\PY{o}{.}\PY{n}{init\PYZus{}from\PYZus{}file}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{model.pkl.gz}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{batch} \PY{o}{=} \PY{n+nb}{next}\PY{p}{(}\PY{n}{data\PYZus{}generator}\PY{p}{(}\PY{n}{batch\PYZus{}size}\PY{p}{,} \PY{n}{max\PYZus{}length}\PY{p}{,} \PY{n}{lines}\PY{p}{,} \PY{n}{shuffle}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{p}{)}
         \PY{n}{preds} \PY{o}{=} \PY{n}{model}\PY{p}{(}\PY{n}{batch}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
         \PY{n}{log\PYZus{}ppx} \PY{o}{=} \PY{n}{test\PYZus{}model}\PY{p}{(}\PY{n}{preds}\PY{p}{,} \PY{n}{batch}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{The log perplexity and perplexity of your model are respectively}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{log\PYZus{}ppx}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{n}{log\PYZus{}ppx}\PY{p}{)}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
The log perplexity and perplexity of your model are respectively 1.9785146 7.2319922

    \end{Verbatim}

    \textbf{Expected Output:} The log perplexity and perplexity of your
model are respectively around 1.9 and 7.2.

    \# Part 5: Generating the language with your own model

We will now use your own language model to generate new sentences for
that we need to make draws from a Gumble distribution.

    The Gumbel Probability Density Function (PDF) is defined as:

\[ f(z) = {1\over{\beta}}e^{(-z+e^{(-z)})} \]

where: \[ z = {(x - \mu)\over{\beta}}\]

The maximum value, which is what we choose as the prediction in the last
step of a Recursive Neural Network \texttt{RNN} we are using for text
generation, in a sample of a random variable following an exponential
distribution approaches the Gumbel distribution when the sample
increases asymptotically. For that reason, the Gumbel distribution is
used to sample from a categorical distribution.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}25}]:} \PY{c+c1}{\PYZsh{} Run this cell to generate some news sentence}
         \PY{k}{def} \PY{n+nf}{gumbel\PYZus{}sample}\PY{p}{(}\PY{n}{log\PYZus{}probs}\PY{p}{,} \PY{n}{temperature}\PY{o}{=}\PY{l+m+mf}{1.0}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Gumbel sampling from a categorical distribution.\PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{n}{u} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{uniform}\PY{p}{(}\PY{n}{low}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}6}\PY{p}{,} \PY{n}{high}\PY{o}{=}\PY{l+m+mf}{1.0} \PY{o}{\PYZhy{}} \PY{l+m+mf}{1e\PYZhy{}6}\PY{p}{,} \PY{n}{size}\PY{o}{=}\PY{n}{log\PYZus{}probs}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
             \PY{n}{g} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{n}{np}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{n}{np}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{n}{u}\PY{p}{)}\PY{p}{)}
             \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{log\PYZus{}probs} \PY{o}{+} \PY{n}{g} \PY{o}{*} \PY{n}{temperature}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
         
         \PY{k}{def} \PY{n+nf}{predict}\PY{p}{(}\PY{n}{num\PYZus{}chars}\PY{p}{,} \PY{n}{prefix}\PY{p}{)}\PY{p}{:}
             \PY{n}{inp} \PY{o}{=} \PY{p}{[}\PY{n+nb}{ord}\PY{p}{(}\PY{n}{c}\PY{p}{)} \PY{k}{for} \PY{n}{c} \PY{o+ow}{in} \PY{n}{prefix}\PY{p}{]}
             \PY{n}{result} \PY{o}{=} \PY{p}{[}\PY{n}{c} \PY{k}{for} \PY{n}{c} \PY{o+ow}{in} \PY{n}{prefix}\PY{p}{]}
             \PY{n}{max\PYZus{}len} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{prefix}\PY{p}{)} \PY{o}{+} \PY{n}{num\PYZus{}chars}
             \PY{k}{for} \PY{n}{\PYZus{}} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{num\PYZus{}chars}\PY{p}{)}\PY{p}{:}
                 \PY{n}{cur\PYZus{}inp} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{inp} \PY{o}{+} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{*} \PY{p}{(}\PY{n}{max\PYZus{}len} \PY{o}{\PYZhy{}} \PY{n+nb}{len}\PY{p}{(}\PY{n}{inp}\PY{p}{)}\PY{p}{)}\PY{p}{)}
                 \PY{n}{outp} \PY{o}{=} \PY{n}{model}\PY{p}{(}\PY{n}{cur\PYZus{}inp}\PY{p}{[}\PY{k+kc}{None}\PY{p}{,} \PY{p}{:}\PY{p}{]}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Add batch dim.}
                 \PY{n}{next\PYZus{}char} \PY{o}{=} \PY{n}{gumbel\PYZus{}sample}\PY{p}{(}\PY{n}{outp}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{inp}\PY{p}{)}\PY{p}{]}\PY{p}{)}
                 \PY{n}{inp} \PY{o}{+}\PY{o}{=} \PY{p}{[}\PY{n+nb}{int}\PY{p}{(}\PY{n}{next\PYZus{}char}\PY{p}{)}\PY{p}{]}
                
                 \PY{k}{if} \PY{n}{inp}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{==} \PY{l+m+mi}{1}\PY{p}{:}
                     \PY{k}{break}  \PY{c+c1}{\PYZsh{} EOS}
                 \PY{n}{result}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n+nb}{chr}\PY{p}{(}\PY{n+nb}{int}\PY{p}{(}\PY{n}{next\PYZus{}char}\PY{p}{)}\PY{p}{)}\PY{p}{)}
             
             \PY{k}{return} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n}{result}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{predict}\PY{p}{(}\PY{l+m+mi}{32}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
And in the shapes of heaven, he 

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}26}]:} \PY{n+nb}{print}\PY{p}{(}\PY{n}{predict}\PY{p}{(}\PY{l+m+mi}{32}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{predict}\PY{p}{(}\PY{l+m+mi}{32}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{predict}\PY{p}{(}\PY{l+m+mi}{32}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
MARK ANTONY	To go, good sir.
Even with a countenance, exempt 
I'll leave him to; so 'twere so 

    \end{Verbatim}

    In the generated text above, you can see that the model generates text
that makes sense capturing dependencies between words and without any
input. A simple n-gram model would have not been able to capture all of
that in one sentence.

    \#\#\# { On statistical methods }

Using a statistical method like the one you implemented in course 2 will
not give you results that are as good. Your model will not be able to
encode information seen previously in the data set and as a result, the
perplexity will increase. Remember from course 2 that the higher the
perplexity, the worse your model is. Furthermore, statistical ngram
models take up too much space and memory. As a result, it will be
inefficient and too slow. Conversely, with deepnets, you can get a
better perplexity. Note, learning about n-gram language models is still
important and allows you to better understand deepnets.


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
